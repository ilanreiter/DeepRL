{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "\n",
    "In this chapter, we'll try to apply the same theory to problems of much greater complexity: arcade games from the Atari 2600 platform, which are the de-facto benchmark of the RL research community. To deal with this new and more challenging goal, we'll talk about problems with the Value iteration method and introduce its variation, called Q-learning. In particular, we'll look at the application of Q-learning to so-called \"grid world\" environments, which is called tabular Q-learning, and then we'll discuss Q-learning in conjunction with neural networks. This combination has the name DQN. At the end of the chapter, we'll reimplement a DQN algorithm from the famous paper, Playing Atari with Deep Reinforcement Learning by V. Mnih and others, published in 2013, which started a new era in RL development.\n",
    "\n",
    "## Real-life value iteration \n",
    "The improvements we got in the FrozenLake environment by switching from Cross-Entropy to the Value iteration method are quite encouraging, so it's tempting to apply the value iteration method to more challenging problems. However, let's first look at the assumptions and limitations that our Value iteration method has.\n",
    "We will start with a quick recap of the method. The Value iteration method on every step does a loop on all states, and for every state, it performs an update of its value with a Bellman approximation. The variation of the same method for Q-values (values for actions) is almost the same, but we approximate and store values for every state and action. So, what's wrong with this process?\n",
    "The first obvious problem is the count of environment states and our ability to iterate over them. In the Value iteration, we assume that we know all states in our environment in advance, can iterate over them and can store value approximation associated with the state. It's definitely true for the simple \"grid world\" environment of FrozenLake, but what about other tasks? First, let's try to understand how scalable the Value iteration approach is, or, in other words, how many states we can easily iterate over in every loop. Even a moderate-sized computer can keep several billion float values in memory (8.5 billion in 32 GB of RAM), so the memory required for value tables doesn't look like a huge constraint. Iteration over billions of states and actions will be more memory intensive, but not an insurmountable problem.\n",
    "Nowadays, we have multicore systems that are mostly idle. The real problem is the number of samples required to get good approximations for state transition dynamics. Imagine that you have some environment with, say, a billion states (this corresponds approximately to a FrozenLake of size 31600 × 31600). To calculate even a rough approximation for every state of this environment, we'll need hundreds of billions of transitions evenly distributed over our states, which is not practical.\n",
    "To give you an example of an environment with a much larger number of potential states, let's consider the Atari 2600 game console again. This was very popular in the 1980s and many arcade-style games were available for it. The Atari console is archaic by today's gaming standards, but its games give an excellent set of RL problems that humans can master fairly quickly, but still are challenging for computers. Not surprisingly, this platform (using an emulator, of course) is a very popular benchmark among RL researches.\n",
    "Let's calculate the state space for the Atari platform. The resolution of the screen is 210 x 160 pixels, and every pixel has one of 128 colors. So, every frame of the screen has 210 × 160 = 33600 pixels and the total amount of different screens possible is $128^{33600}$ which is slightly more than $10^{70802}$. If we decide to just enumerate all possible states\n",
    "of Atari once, it will take billions of billions of years even for the fastest supercomputer. Also, 99(.9)% of this job will be a waste of time, as most of the combinations will never be shown during even long gameplay, so we'll never have samples of those states. However, the value iteration method wants to iterate over them just in case.\n",
    "Another problem with the value iteration approach is that it limits us to discrete action spaces. Indeed, both Q(s, a) and V(s) approximations assume that our actions are a mutually exclusive discrete set, which is not true for continuous control problems where actions can represent continuous variables, such as the angle of a steering wheel, the force on an actuator, or the temperature of a heater. This issue is much more challenging than the first, and we'll talk about it in the last part of the book, in chapters dedicated to continuous action space problems. For now, let's assume that we have a discrete count of actions and this count is not very large (orders of tens). How should we handle the state space size issue?\n",
    "\n",
    "# Tabular Q-learning\n",
    "First of all, do we really need to iterate over every state in the state space? We have an environment that can be used as a source of real-life samples of states. If some state in the state space is not shown to us by the environment, why should we care about its value? We can use states obtained from the environment to update values of states, which can save us lots of work.\n",
    "This modification of the Value iteration method is known as Q-learning, as mentioned earlier, and for cases with explicit state-to-value mappings, has the following steps:\n",
    "\n",
    "1. Start with an empty table, mapping states to values of actions.\n",
    "2. By interacting with the environment, obtain the **tuple** $s, a, r, s′$ ``(state, action, reward, and the new state)``. In this step, we need to decide which action to take, and there is no single proper way to make this decision. We discussed this problem as **exploration versus exploitation** and will talk a lot about this.\n",
    "3. Update the $Q(s, a)$ value using the Bellman approximation: $Q(s,a) \\leftarrow r + \\gamma \\max_{a' \\in A} Q_{s',a'}$\n",
    "4. Repeat from step 2.\n",
    "\n",
    "As in Value iteration, the end condition could be some threshold of the update or we can perform test episodes to estimate the expected reward from the policy. Another thing to note here is how to update the Q-values. As we take samples from the environment, it's generally a bad idea to just assign new values on top of existing values, as training can become unstable. What is usually done in practice is to update the $Q(s, a)$ with approximations using a **\"blending\"** technique, which is just averaging between old and new values of $Q$ using learning rate $\\alpha$ with a value from 0 to 1:\n",
    "\n",
    "$Q(s,a) \\leftarrow (1 - \\alpha) Q_{s,a} + \\alpha (r + \\gamma \\max_{a' \\in A} Q_{s',a'})$\n",
    "\n",
    "This allows values of $Q$ to converge smoothly, even if our environment is noisy.\n",
    "The final version of the algorithm is here:\n",
    "\n",
    "1. Start with an empty table for $Q(s,a)$\n",
    "2. Obtain the **tuple** ($s, a, r, s′)$ ``(state, action, reward, and the new state)`` from the environment. \n",
    "3. Make a Bellman update:\n",
    "$Q(s,a) \\leftarrow (1 - \\alpha) Q_{s,a} + \\alpha (r + \\gamma \\max_{a' \\in A} Q_{s',a'})$\n",
    "4. Check convergence conditions. If not met, repeat from step 2.\n",
    "\n",
    "As mentioned earlier, this method is called **tabular Q-learning**, as we keep a **table of states with their Q-values**. Let's try it on our FrozenLake environment. The whole example code is in Chapter06/01_frozenlake_q_learning.py.\n",
    "You may have noticed that this version used more iterations to solve the problem compared to the value iteration method from the previous chapter. The reason for that is that we're **no longer using the experience obtained during testing**. (In Chapter05/02_frozenlake_q_iteration.py, periodical tests cause an update of Q-table statistics. **Here we don't touch Q-values during the test**, which cause more iterations before the environment gets solved.) Overall, the total amount of samples required from the environment is almost the same. The reward chart in TensorBoard also shows good training dynamics, which are very similar to the value iteration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050 env. steps per iteration =  196\n",
      "Best reward updated 0.050 -> 0.100 env. steps per iteration =  223\n",
      "Best reward updated 0.100 -> 0.150 env. steps per iteration =  141\n",
      "Best reward updated 0.150 -> 0.200 env. steps per iteration =  429\n",
      "Best reward updated 0.200 -> 0.300 env. steps per iteration =  419\n",
      "Best reward updated 0.300 -> 0.350 env. steps per iteration =  448\n",
      "Best reward updated 0.350 -> 0.450 env. steps per iteration =  641\n",
      "Best reward updated 0.450 -> 0.600 env. steps per iteration =  659\n",
      "Best reward updated 0.600 -> 0.650 env. steps per iteration =  529\n",
      "Best reward updated 0.650 -> 0.700 env. steps per iteration =  508\n",
      "Best reward updated 0.700 -> 0.800 env. steps per iteration =  673\n",
      "Best reward updated 0.800 -> 0.900 env. steps per iteration =  885\n",
      "Solved in 6539 iterations!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"\n",
    "GAMMA = 0.9 #Discount factor\n",
    "ALPHA = 0.2 #Learning rate\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    #Initialize the agent\n",
    "    def __init__(self): \n",
    "        self.env = gym.make(ENV_NAME) #reate a new envrionment \n",
    "        self.state = self.env.reset() #reset the env.\n",
    "        self.values = collections.defaultdict(float) #initiate an empty values table\n",
    "\n",
    "    #methos to execute a single step sample of the envrionment\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample() #Obtain a single sampled action from the environment \n",
    "        old_state = self.state #safe the prev. state\n",
    "        new_state, reward, is_done, _ = self.env.step(action) #Perform the sampled action on the env\n",
    "        self.state = self.env.reset() if is_done else new_state #Store the new obtained state or reste if done\n",
    "        return (old_state, action, reward, new_state) #return a tuple (s,a,r,s')\n",
    "\n",
    "    #Method to select the best action in a given state\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n): #iterate over all possible actions of the environment \n",
    "            action_value = self.values[(state, action)] #Get the current action value (Q-Value) from the values table\n",
    "            if best_value is None or best_value < action_value: #calculate the best value and he best action\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action #return the best value and the best action\n",
    "    \n",
    "    #Method to update the value table \n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_val = r + GAMMA * best_v #caluclate the new value with Bellman equation\n",
    "        old_val = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA #Update the value with wights based on the learning rate\n",
    "\n",
    "    #Method to play a sinlge episode\n",
    "    #We don't update the tables while runing the episode\n",
    "    def play_episode(self, env):\n",
    "        state = env.reset() #Create a new env. (not using the environment used to upadate the value table)\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state) #get the best action for the current state\n",
    "            new_state, reward, is_done, _ = env.step(action) #mae a step in the env with the best action\n",
    "            total_reward += reward #increment total reward with the new reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state #keep the new state s the new state\n",
    "            steps += 1\n",
    "        return total_reward, steps\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent() #Create a new agent object\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        s, a, r, next_s = agent.sample_env() #samle the environmet\n",
    "        agent.value_update(s, a, r, next_s) #update the value table\n",
    "\n",
    "        total_reward = 0.0\n",
    "        total_steps = 0\n",
    "        for _ in range(TEST_EPISODES): #Test on the neviroment using ghe new tables\n",
    "            reward, steps = agent.play_episode(test_env)\n",
    "            total_reward += reward\n",
    "            total_steps += steps\n",
    "        total_reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if total_reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, total_reward), 'env. steps per iteration = ', total_steps)\n",
    "            best_reward = total_reward\n",
    "        if total_reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward= 0.581  Av. steps per episode =  41.202\n"
     ]
    }
   ],
   "source": [
    "TEST_EPISODES = 1000\n",
    "env = gym.make(ENV_NAME) #Create a new environment\n",
    "total_reward = 0\n",
    "total_steps = 0\n",
    "for i in range(TEST_EPISODES):\n",
    "    rewards, steps = agent.play_episode(env)\n",
    "    #print('Episode: ',i,' Reward=', reward)\n",
    "    total_reward += rewards\n",
    "    total_steps += steps\n",
    "print('Average reward=', total_reward/TEST_EPISODES, ' Av. steps per episode = ', total_steps/TEST_EPISODES)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning\n",
    "\n",
    "For enviroment with continues states, we can use **a nonlinear representation that maps both state and action onto a value**. In machine learning this is called a **\"regression problem\"**. The concrete way to represent and train such a representation can vary, but, as you may have already guessed from this section's title, using a deep neural network is one of the most popular options, especially when dealing with observations represented as screen images. With this in mind, let's make modifications to the Q-learning algorithm:\n",
    "We do a modifcation to the Tabluar Q-Learninng Algorithem above to accomudate very large state,action space and to use DNN:\n",
    "\n",
    "1. Initialize $Q(s, a)$ with some initial approximation \n",
    "2. By interacting with the environment, obtain the tuple $(s, a, r, s′)$ \n",
    "3. Calculate loss: $L =(Q_{s,a} − r)^2$ if episode has ended or $L =(Q_{s,a} − (r + \\gamma \\max_{a' \\in A} Q_{s',a'}))^2$ otherwise\n",
    "4. Update $Q(s, a)$ using the stochastic gradient descent (SGD) algorithm, by minimizing the loss with respect to the model parameters\n",
    "5. Repeat from step 2 until converged\n",
    "\n",
    "### Interaction with the environment\n",
    "\n",
    "First of all, we need to interact with the environment somehow to receive data to train on. In simple environments, such as FrozenLake, we can act randomly, but is this the best strategy to use? Imagine the game of Pong. What's the probability of winning a single point by randomly moving the paddle? It's not zero but it's extremely small, which just means that we'll need to wait for a very long time for such a rare situation. As an alternative, we can use our Q function approximation as a source of behavior (as we did before in the value iteration method, when we remembered our experience during testing).\n",
    "\n",
    "If our representation of Q is good, then the experience that we get from the environment will show the agent relevant data to train on. However, we're in trouble when our approximation is not perfect (at the beginning of the training, for example). In such a case, our agent can be stuck with bad actions for some states without ever trying to behave differently. This **exploration versus exploitation dilemma** was mentioned briefly in Chapter 1, What is Reinforcement Learning?. On the one hand, our agent needs to explore the environment to build a complete picture of transitions and action outcomes. On the other hand, we should use interaction with the environment efficiently: we shouldn't waste time by randomly trying actions we've already tried and have learned their outcomes. As you can see, random behavior is better at the beginning of the training when our Q approximation is bad, as it gives us more uniformly distributed information about the environment states. As our training progresses, random behavior becomes inefficient and we want to fall back to our Q approximation to decide how to act.\n",
    "\n",
    "A method which performs such a mix of two extreme behaviors is known as an **epsilon-greedy method**, which just means switching between random and $Q$ policy using the probability hyperparameter $\\epsilon$. By varying $\\epsilon$ we can select the ratio of random actions. The usual practice is to start with $\\epsilon = 1.0$ (100% random actions) and slowly decrease it to some small value such as 5% or 2% of random actions. Using an epsilon-greedy method helps both to explore the environment in the beginning and to stick to good policy at the end of the training. There are other solutions to the **\"exploration versus exploitation\"** problem, and we'll discuss some of them in part three of the book. This problem is **one of the fundamental open questions in RL and an active area of research, which is not even close to being resolved completely**.\n",
    "\n",
    "### SGD optimization\n",
    "The core of our Q-learning procedure is borrowed from the supervised learning. Indeed, we are trying to approximate a complex, nonlinear function $Q(s, a)$ with a neural network. To do this, we calculate targets for this function using the Bellman equation and then pretend that we have a supervised learning problem at hand. That's okay, but one of the fundamental requirements for SGD optimization is that the training data is **independent and identically distributed (i.i.d)**.\n",
    "\n",
    "In our case, data that we're going to use for the SGD update doesn't fulfill these criteria:\n",
    "1. Our samples are not independent. Even if we accumulate a large batch of data samples, they all will be very close to each other, as they belong to the same episode.\n",
    "2. Distribution of our training data won't be identical to samples provided by the optimal policy that we want to learn. Data that we have is a result of some other policy (our current policy, random, or both in the case of $\\epsilon$-greedy), but we don't want to learn how to play randomly: we want an optimal policy with the best reward.\n",
    "\n",
    "To deal with this nuisance, we usually need to use a **large buffer of our past experience and sample training data from it, instead of using our latest experience**. This method is called **replay buffer**. The simplest implementation is a buffer of **fixed size**, with new data added to the end of the buffer so that it pushes the oldest experience out of it. \n",
    "\n",
    "**Replay buffer** allows us to train on more-or-less independent data, but data will still be fresh enough to train on samples generated by our recent policy.\n",
    "\n",
    "### Correlation between steps\n",
    "Another practical issue with the default training procedure is also related to the lack of\n",
    "i.i.d in our data, but in a slightly different manner. The Bellman equation provides us with the value of $Q(s, a)$ via $Q(s′, a′)$ (which has the name of **bootstrapping**). However, both states $s$ and $s′$ have only one step between them. This makes them very similar and it's really hard for neural networks to distinguish between them. When we perform an update of our network's parameters, to make $Q(s, a)$ closer to the desired result, we indirectly can alter the value produced for $Q(s′, a′)$ and other states nearby. This can make our training really unstable, like chasing our own tail: when we update $Q$ for state $s$, then on subsequent states we discover that $Q(s′, a′)$ becomes worse, but attempts to update it can spoil our $Q(s, a)$ approximation, and so on.\n",
    "\n",
    "To make training more stable, there is a **trick, called target network**, when we keep a copy of our network and use it for the $Q(s′, a′)$ value in the Bellman equation. This network is synchronized with our main network only periodically, for example, once in $N$ steps (where $N$ is usually quite a large hyperparameter, such as 1k or 10k training iterations).\n",
    "\n",
    "### The Markov property\n",
    "Our RL methods use MDP formalism as their basis, which **assumes that the environment obeys the Markov property**: observation from the environment is all that we need to act optimally (in other words, our observations allow us to distinguish states from one another). As we've seen on the preceding Pong's screenshot, one single image from the Atari game is not enough to capture all important information (using only one image we have no idea about the speed and direction of objects, like the ball and our opponent's paddle). This obviously violates the Markov property and moves our single-frame Pong environment into the area of **partially observable MDPs (POMDP)**. A POMDP is basically MDP without the Markov property and they are very important in practice. For example, for most card games where you don't see your opponents' cards, game observations are POMDPs, because current observation (your cards and cards on the table) could correspond to different cards in your opponents' hands.\n",
    "We'll not discuss POMPDs in detail in this book, so, for now, we'll use a small technique to push our environment back into the MDP domain. **The solution is maintaining several observations from the past and using them as a state**. In the case of Atari games, we usually stack $k$ subsequent frames together and use them as the observation at every state. This **allows our agent to deduct the dynamics of the current state**, for instance, to get the speed of the ball and its direction. The usual \"classical\" **number of $k$ for Atari is four**. Of course, it's just a hack, as there can be longer dependencies in the environment, but for most of the games it works well.\n",
    "\n",
    "## The final form of DQN (Deep Q-Learning) training\n",
    "There are many more tips and tricks that researchers have discovered to make **DQN** training more stable and efficient, and we'll cover the best of them in the next chapter. However, **$\\epsilon$-greedy**, **replay buffer**, and **target network** form the basis that allows DeepMind to successfully train a DQN on a set of 49 Atari games and demonstrate the efficiency of this approach applied to complicated environments.\n",
    "\n",
    "The original paper (without target network) was published at the end of 2013 ([Playing Atari with Deep Reinforcement Learning 1312.5602v1, Mnih and others.)](https://arxiv.org/pdf/1312.5602.pdf), and they used seven games for testing. Later, at the beginning of 2015, a revised version of the article, with 49 different games, was published in Nature ([Human-Level Control Through Deep Reinforcement Learning doi:10.1038/nature14236, Mnih and others.](https://www.nature.com/articles/nature14236))\n",
    "\n",
    "The algorithm for DQN from the preceding papers has the following steps:\n",
    "1. Initialize parameters for $Q(s, a)$ and $\\hat Q (s, a)$ with random weights, **$\\epsilon\\leftarrow 1.0$**, and **empty replay buffer**\n",
    "2. With probability $\\epsilon$, select a random action $a$, otherwise $a = \\arg \\max_a Q_{s,a}$\n",
    "3. Execute action $a$ in an **emulator** and observe reward $r$ and the next state $s′$ \n",
    "4. Store transition $(s, a, r, s′)$ in the **replay buffer**\n",
    "5. Sample a random **minibatch** of transitions from the **replay buffer**\n",
    "6. For every transition in the buffer, calculate target $y = r$ if the episode has ended at this step or $y = r + \\gamma \\max_{a' \\in A} \\hat Q_{s',a'}$ otherwise\n",
    "7. Calculate loss: $L = (Q_{s,a} − y)^2$\n",
    "8. Update $Q(s, a)$ using the **SGD algorithm** by minimizing the loss in respect to model parameters\n",
    "9. Every $N$ steps copy weights from $Q$ to $\\hat Q_t$\n",
    "10. Repeat from step 2 until converged\n",
    "\n",
    "\n",
    "## DQN on FrozenLake\n",
    "### DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#Class for setting the Deep Q-Learning Network\n",
    "#For the FrozenLake game the w use as simple fully connected single hidden layer dnn\n",
    "class DQN(nn.Module):\n",
    "    #Method to intialize the DNN\n",
    "    def __init__(self, obs_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    #method to perform a single forward operation\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0559, -0.0556,  0.0841,  0.0249],\n",
      "        [-0.1455, -0.0652,  0.0771, -0.1603],\n",
      "        [-0.1200, -0.1061,  0.0172,  0.0289],\n",
      "        [-0.0559, -0.0556,  0.0841,  0.0249],\n",
      "        [-0.1455, -0.0652,  0.0771, -0.1603],\n",
      "        [-0.1455, -0.0652,  0.0771, -0.1603],\n",
      "        [-0.1200, -0.1061,  0.0172,  0.0289],\n",
      "        [-0.1200, -0.1061,  0.0172,  0.0289]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 0, 1, 1, 0, 3, 0, 0]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0841],\n",
      "        [-0.1455],\n",
      "        [-0.1061],\n",
      "        [-0.0556],\n",
      "        [-0.1455],\n",
      "        [-0.1603],\n",
      "        [-0.1200],\n",
      "        [-0.1200]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0841, -0.1455, -0.1061, -0.0556, -0.1455, -0.1603, -0.1200, -0.1200],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1439, -0.0643,  0.0780, -0.1588],\n",
      "        [-0.0544, -0.0544,  0.0859,  0.0260],\n",
      "        [-0.1198, -0.0677,  0.0968, -0.0176],\n",
      "        [-0.1439, -0.0643,  0.0780, -0.1588],\n",
      "        [-0.1182, -0.1048,  0.0183,  0.0299],\n",
      "        [-0.0159, -0.1158, -0.0158,  0.1140],\n",
      "        [-0.1182, -0.1048,  0.0183,  0.0299],\n",
      "        [-0.0159, -0.1158, -0.0158,  0.1140]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 3, 0, 3, 1, 1, 2]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1439],\n",
      "        [ 0.0859],\n",
      "        [-0.0176],\n",
      "        [-0.1439],\n",
      "        [ 0.0299],\n",
      "        [-0.1158],\n",
      "        [-0.1048],\n",
      "        [-0.0158]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1439,  0.0859, -0.0176, -0.1439,  0.0299, -0.1158, -0.1048, -0.0158],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1423, -0.0633,  0.0790, -0.1576],\n",
      "        [-0.1168, -0.1035,  0.0194,  0.0311],\n",
      "        [-0.1423, -0.0633,  0.0790, -0.1576],\n",
      "        [-0.0149, -0.1146, -0.0144,  0.1150],\n",
      "        [-0.1168, -0.1035,  0.0194,  0.0311],\n",
      "        [-0.1423, -0.0633,  0.0790, -0.1576],\n",
      "        [-0.1187, -0.0667,  0.0979, -0.0163],\n",
      "        [-0.1423, -0.0633,  0.0790, -0.1576]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 1, 2, 1, 2, 0, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1423],\n",
      "        [ 0.0311],\n",
      "        [-0.0633],\n",
      "        [-0.0144],\n",
      "        [-0.1035],\n",
      "        [ 0.0790],\n",
      "        [-0.1187],\n",
      "        [-0.1423]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1423,  0.0311, -0.0633, -0.0144, -0.1035,  0.0790, -0.1187, -0.1423],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1408, -0.0623,  0.0803, -0.1565],\n",
      "        [-0.0882, -0.0285,  0.0892, -0.0464],\n",
      "        [-0.0882, -0.0285,  0.0892, -0.0464],\n",
      "        [-0.1408, -0.0623,  0.0803, -0.1565],\n",
      "        [-0.1156, -0.1022,  0.0206,  0.0323],\n",
      "        [-0.1408, -0.0623,  0.0803, -0.1565],\n",
      "        [-0.1408, -0.0623,  0.0803, -0.1565],\n",
      "        [-0.0140, -0.1135, -0.0128,  0.1159]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 1, 0, 3, 1, 0, 0, 2]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0803],\n",
      "        [-0.0285],\n",
      "        [-0.0882],\n",
      "        [-0.1565],\n",
      "        [-0.1022],\n",
      "        [-0.1408],\n",
      "        [-0.1408],\n",
      "        [-0.0128]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0803, -0.0285, -0.0882, -0.1565, -0.1022, -0.1408, -0.1408, -0.0128],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0871, -0.0277,  0.0902, -0.0456],\n",
      "        [-0.1145, -0.1009,  0.0217,  0.0334],\n",
      "        [-0.1393, -0.0613,  0.0815, -0.1554],\n",
      "        [-0.1393, -0.0613,  0.0815, -0.1554],\n",
      "        [-0.1393, -0.0613,  0.0815, -0.1554],\n",
      "        [-0.1393, -0.0613,  0.0815, -0.1554],\n",
      "        [-0.1167, -0.0648,  0.1003, -0.0141],\n",
      "        [-0.1393, -0.0613,  0.0815, -0.1554]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 1, 0, 1, 0, 3, 3, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0456],\n",
      "        [-0.1009],\n",
      "        [-0.1393],\n",
      "        [-0.0613],\n",
      "        [-0.1393],\n",
      "        [-0.1554],\n",
      "        [-0.0141],\n",
      "        [-0.1393]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0456, -0.1009, -0.1393, -0.0613, -0.1393, -0.1554, -0.0141, -0.1393],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1378, -0.0603,  0.0825, -0.1541],\n",
      "        [-0.1133, -0.0996,  0.0227,  0.0346],\n",
      "        [-0.1133, -0.0996,  0.0227,  0.0346],\n",
      "        [-0.1378, -0.0603,  0.0825, -0.1541],\n",
      "        [-0.1133, -0.0996,  0.0227,  0.0346],\n",
      "        [-0.1378, -0.0603,  0.0825, -0.1541],\n",
      "        [-0.0123, -0.1116, -0.0098,  0.1179],\n",
      "        [-0.1157, -0.0639,  0.1013, -0.0129]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 1, 3, 0, 2, 3, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1378],\n",
      "        [ 0.0227],\n",
      "        [-0.0996],\n",
      "        [-0.1541],\n",
      "        [-0.1133],\n",
      "        [ 0.0825],\n",
      "        [ 0.1179],\n",
      "        [-0.1157]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1378,  0.0227, -0.0996, -0.1541, -0.1133,  0.0825,  0.1179, -0.1157],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1364, -0.0593,  0.0836, -0.1528],\n",
      "        [-0.1364, -0.0593,  0.0836, -0.1528],\n",
      "        [-0.1364, -0.0593,  0.0836, -0.1528],\n",
      "        [-0.1364, -0.0593,  0.0836, -0.1528],\n",
      "        [-0.1122, -0.0985,  0.0239,  0.0358],\n",
      "        [-0.1364, -0.0593,  0.0836, -0.1528],\n",
      "        [-0.1122, -0.0985,  0.0239,  0.0358],\n",
      "        [-0.1122, -0.0985,  0.0239,  0.0358]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 3, 2, 1, 0, 2, 2]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1364],\n",
      "        [-0.0593],\n",
      "        [-0.1528],\n",
      "        [ 0.0836],\n",
      "        [-0.0985],\n",
      "        [-0.1364],\n",
      "        [ 0.0239],\n",
      "        [ 0.0239]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1364, -0.0593, -0.1528,  0.0836, -0.0985, -0.1364,  0.0239,  0.0239],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1350, -0.0583,  0.0847, -0.1515],\n",
      "        [-0.0842, -0.0253,  0.0925, -0.0427],\n",
      "        [-0.1112, -0.0972,  0.0249,  0.0369],\n",
      "        [-0.1135, -0.0621,  0.1032, -0.0105],\n",
      "        [-0.1350, -0.0583,  0.0847, -0.1515],\n",
      "        [-0.0842, -0.0253,  0.0925, -0.0427],\n",
      "        [-0.1112, -0.0972,  0.0249,  0.0369],\n",
      "        [-0.1112, -0.0972,  0.0249,  0.0369]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 3, 1, 1, 1, 3, 1, 1]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0847],\n",
      "        [-0.0427],\n",
      "        [-0.0972],\n",
      "        [-0.0621],\n",
      "        [-0.0583],\n",
      "        [-0.0427],\n",
      "        [-0.0972],\n",
      "        [-0.0972]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0847, -0.0427, -0.0972, -0.0621, -0.0583, -0.0427, -0.0972, -0.0972],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0834, -0.0245,  0.0932, -0.0416],\n",
      "        [-0.1337, -0.0572,  0.0857, -0.1502],\n",
      "        [-0.1337, -0.0572,  0.0857, -0.1502],\n",
      "        [-0.1125, -0.0611,  0.1041, -0.0093],\n",
      "        [-0.1337, -0.0572,  0.0857, -0.1502],\n",
      "        [-0.1337, -0.0572,  0.0857, -0.1502],\n",
      "        [-0.0096, -0.1089, -0.0064,  0.1212],\n",
      "        [-0.0834, -0.0245,  0.0932, -0.0416]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 0, 1, 3, 2, 2, 3]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0245],\n",
      "        [-0.1337],\n",
      "        [-0.1337],\n",
      "        [-0.0611],\n",
      "        [-0.1502],\n",
      "        [ 0.0857],\n",
      "        [-0.0064],\n",
      "        [-0.0416]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0245, -0.1337, -0.1337, -0.0611, -0.1502,  0.0857, -0.0064, -0.0416],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0455, -0.0453,  0.0976,  0.0355],\n",
      "        [-0.1324, -0.0561,  0.0867, -0.1489],\n",
      "        [-0.0827, -0.0235,  0.0938, -0.0404],\n",
      "        [-0.0088, -0.1079, -0.0052,  0.1223],\n",
      "        [-0.1093, -0.0946,  0.0268,  0.0392],\n",
      "        [-0.1093, -0.0946,  0.0268,  0.0392],\n",
      "        [-0.0827, -0.0235,  0.0938, -0.0404],\n",
      "        [-0.1324, -0.0561,  0.0867, -0.1489]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 1, 1, 3, 1, 1, 3, 2]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0976],\n",
      "        [-0.0561],\n",
      "        [-0.0235],\n",
      "        [ 0.1223],\n",
      "        [-0.0946],\n",
      "        [-0.0946],\n",
      "        [-0.0404],\n",
      "        [ 0.0867]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0976, -0.0561, -0.0235,  0.1223, -0.0946, -0.0946, -0.0404,  0.0867],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0446, -0.0441,  0.0990,  0.0367],\n",
      "        [-0.1085, -0.0933,  0.0278,  0.0404],\n",
      "        [-0.1313, -0.0550,  0.0879, -0.1476],\n",
      "        [-0.1313, -0.0550,  0.0879, -0.1476],\n",
      "        [-0.1085, -0.0933,  0.0278,  0.0404],\n",
      "        [-0.1313, -0.0550,  0.0879, -0.1476],\n",
      "        [-0.0446, -0.0441,  0.0990,  0.0367],\n",
      "        [-0.0820, -0.0225,  0.0946, -0.0392]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 3, 0, 2, 3, 0, 1, 1]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0367],\n",
      "        [ 0.0404],\n",
      "        [-0.1313],\n",
      "        [ 0.0879],\n",
      "        [ 0.0404],\n",
      "        [-0.1313],\n",
      "        [-0.0441],\n",
      "        [-0.0225]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0367,  0.0404, -0.1313,  0.0879,  0.0404, -0.1313, -0.0441, -0.0225],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1076, -0.0919,  0.0288,  0.0417],\n",
      "        [-0.1076, -0.0919,  0.0288,  0.0417],\n",
      "        [-0.1076, -0.0919,  0.0288,  0.0417],\n",
      "        [-0.1098, -0.0581,  0.1069, -0.0057],\n",
      "        [-0.1076, -0.0919,  0.0288,  0.0417],\n",
      "        [-0.1301, -0.0539,  0.0891, -0.1463],\n",
      "        [-0.1301, -0.0539,  0.0891, -0.1463],\n",
      "        [-0.1301, -0.0539,  0.0891, -0.1463]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 3, 1, 2, 3, 1, 1]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1076],\n",
      "        [-0.1076],\n",
      "        [ 0.0417],\n",
      "        [-0.0581],\n",
      "        [ 0.0288],\n",
      "        [-0.1463],\n",
      "        [-0.0539],\n",
      "        [-0.0539]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1076, -0.1076,  0.0417, -0.0581,  0.0288, -0.1463, -0.0539, -0.0539],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(states_v) tensor([[-0.1290, -0.0527,  0.0901, -0.1450],\n",
      "        [-0.1290, -0.0527,  0.0901, -0.1450],\n",
      "        [-0.0427, -0.0417,  0.1014,  0.0396],\n",
      "        [-0.1290, -0.0527,  0.0901, -0.1450],\n",
      "        [-0.1066, -0.0906,  0.0297,  0.0430],\n",
      "        [-0.1089, -0.0571,  0.1077, -0.0045],\n",
      "        [-0.1066, -0.0906,  0.0297,  0.0430],\n",
      "        [-0.1290, -0.0527,  0.0901, -0.1450]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 3, 0, 2, 1, 3, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1290],\n",
      "        [ 0.0901],\n",
      "        [ 0.0396],\n",
      "        [-0.1290],\n",
      "        [ 0.0297],\n",
      "        [-0.0571],\n",
      "        [ 0.0430],\n",
      "        [-0.1450]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1290,  0.0901,  0.0396, -0.1290,  0.0297, -0.0571,  0.0430, -0.1450],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1279, -0.0517,  0.0912, -0.1436],\n",
      "        [-0.1279, -0.0517,  0.0912, -0.1436],\n",
      "        [-0.1279, -0.0517,  0.0912, -0.1436],\n",
      "        [-0.1279, -0.0517,  0.0912, -0.1436],\n",
      "        [-0.1056, -0.0895,  0.0307,  0.0444],\n",
      "        [-0.1279, -0.0517,  0.0912, -0.1436],\n",
      "        [-0.0058, -0.1039, -0.0009,  0.1272],\n",
      "        [-0.0798, -0.0192,  0.0966, -0.0357]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 0, 1, 1, 2, 2, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0517],\n",
      "        [-0.1279],\n",
      "        [-0.1279],\n",
      "        [-0.0517],\n",
      "        [-0.0895],\n",
      "        [ 0.0912],\n",
      "        [-0.0009],\n",
      "        [-0.0192]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0517, -0.1279, -0.1279, -0.0517, -0.0895,  0.0912, -0.0009, -0.0192],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1072, -0.0551,  0.1095, -0.0022],\n",
      "        [-0.1267, -0.0505,  0.0923, -0.1424],\n",
      "        [-0.1267, -0.0505,  0.0923, -0.1424],\n",
      "        [-0.1267, -0.0505,  0.0923, -0.1424],\n",
      "        [-0.1047, -0.0883,  0.0317,  0.0456],\n",
      "        [-0.1267, -0.0505,  0.0923, -0.1424],\n",
      "        [-0.1267, -0.0505,  0.0923, -0.1424],\n",
      "        [-0.1267, -0.0505,  0.0923, -0.1424]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 3, 1, 3, 1, 2, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1072],\n",
      "        [-0.1267],\n",
      "        [-0.1424],\n",
      "        [-0.0505],\n",
      "        [ 0.0456],\n",
      "        [-0.0505],\n",
      "        [ 0.0923],\n",
      "        [-0.1267]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1072, -0.1267, -0.1424, -0.0505,  0.0456, -0.0505,  0.0923, -0.1267],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1037, -0.0871,  0.0328,  0.0468],\n",
      "        [-0.1037, -0.0871,  0.0328,  0.0468],\n",
      "        [-0.1037, -0.0871,  0.0328,  0.0468],\n",
      "        [-0.1037, -0.0871,  0.0328,  0.0468],\n",
      "        [-0.0399, -0.0383,  0.1050,  0.0438],\n",
      "        [-0.1063, -0.0541,  0.1105, -0.0011],\n",
      "        [-0.0784, -0.0171,  0.0981, -0.0337],\n",
      "        [-0.1255, -0.0493,  0.0935, -0.1412]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 3, 0, 3, 3, 0, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1037],\n",
      "        [-0.0871],\n",
      "        [ 0.0468],\n",
      "        [-0.1037],\n",
      "        [ 0.0438],\n",
      "        [-0.0011],\n",
      "        [-0.0784],\n",
      "        [-0.1255]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1037, -0.0871,  0.0468, -0.1037,  0.0438, -0.0011, -0.0784, -0.1255],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1242, -0.0481,  0.0946, -0.1399],\n",
      "        [-0.1242, -0.0481,  0.0946, -0.1399],\n",
      "        [-0.1242, -0.0481,  0.0946, -0.1399],\n",
      "        [-0.1242, -0.0481,  0.0946, -0.1399],\n",
      "        [-0.1026, -0.0858,  0.0336,  0.0481],\n",
      "        [-0.1242, -0.0481,  0.0946, -0.1399],\n",
      "        [-0.0034, -0.1008,  0.0023,  0.1305],\n",
      "        [-0.1242, -0.0481,  0.0946, -0.1399]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 3, 3, 0, 0, 1, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1242],\n",
      "        [ 0.0946],\n",
      "        [-0.1399],\n",
      "        [-0.1399],\n",
      "        [-0.1026],\n",
      "        [-0.1242],\n",
      "        [-0.1008],\n",
      "        [-0.1242]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1242,  0.0946, -0.1399, -0.1399, -0.1026, -0.1242, -0.1008, -0.1242],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1229, -0.0469,  0.0957, -0.1386],\n",
      "        [-0.1229, -0.0469,  0.0957, -0.1386],\n",
      "        [-0.1229, -0.0469,  0.0957, -0.1386],\n",
      "        [-0.1229, -0.0469,  0.0957, -0.1386],\n",
      "        [-0.0768, -0.0150,  0.0994, -0.0317],\n",
      "        [-0.1013, -0.0846,  0.0345,  0.0493],\n",
      "        [-0.1229, -0.0469,  0.0957, -0.1386],\n",
      "        [-0.1043, -0.0520,  0.1121,  0.0013]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 0, 0, 2, 1, 0, 2, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1386],\n",
      "        [-0.1229],\n",
      "        [-0.1229],\n",
      "        [ 0.0957],\n",
      "        [-0.0150],\n",
      "        [-0.1013],\n",
      "        [ 0.0957],\n",
      "        [-0.1043]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1386, -0.1229, -0.1229,  0.0957, -0.0150, -0.1013,  0.0957, -0.1043],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1215, -0.0459,  0.0969, -0.1372],\n",
      "        [-0.0999, -0.0834,  0.0354,  0.0505],\n",
      "        [-0.1215, -0.0459,  0.0969, -0.1372],\n",
      "        [-0.1215, -0.0459,  0.0969, -0.1372],\n",
      "        [-0.1215, -0.0459,  0.0969, -0.1372],\n",
      "        [-0.0999, -0.0834,  0.0354,  0.0505],\n",
      "        [-0.0013, -0.0986,  0.0042,  0.1327],\n",
      "        [-0.0363, -0.0348,  0.1081,  0.0481]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 0, 0, 2, 3, 2, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1215],\n",
      "        [-0.0999],\n",
      "        [-0.1215],\n",
      "        [-0.1215],\n",
      "        [ 0.0969],\n",
      "        [ 0.0505],\n",
      "        [ 0.0042],\n",
      "        [ 0.0481]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1215, -0.0999, -0.1215, -0.1215,  0.0969,  0.0505,  0.0042,  0.0481],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1199, -0.0449,  0.0980, -0.1360],\n",
      "        [-0.0349, -0.0338,  0.1092,  0.0495],\n",
      "        [-0.1199, -0.0449,  0.0980, -0.1360],\n",
      "        [-0.1199, -0.0449,  0.0980, -0.1360],\n",
      "        [-0.1199, -0.0449,  0.0980, -0.1360],\n",
      "        [-0.1199, -0.0449,  0.0980, -0.1360],\n",
      "        [-0.1199, -0.0449,  0.0980, -0.1360],\n",
      "        [-0.0985, -0.0824,  0.0363,  0.0517]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 3, 0, 1, 2, 2, 3, 1]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0980],\n",
      "        [ 0.0495],\n",
      "        [-0.1199],\n",
      "        [-0.0449],\n",
      "        [ 0.0980],\n",
      "        [ 0.0980],\n",
      "        [-0.1360],\n",
      "        [-0.0824]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0980,  0.0495, -0.1199, -0.0449,  0.0980,  0.0980, -0.1360, -0.0824],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0971, -0.0812,  0.0372,  0.0528],\n",
      "        [-0.0971, -0.0812,  0.0372,  0.0528],\n",
      "        [-0.1185, -0.0439,  0.0993, -0.1347],\n",
      "        [-0.1185, -0.0439,  0.0993, -0.1347],\n",
      "        [-0.1185, -0.0439,  0.0993, -0.1347],\n",
      "        [-0.1006, -0.0491,  0.1148,  0.0047],\n",
      "        [-0.0971, -0.0812,  0.0372,  0.0528],\n",
      "        [ 0.0008, -0.0966,  0.0064,  0.1347]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 3, 3, 2, 3, 3, 1, 3]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0812],\n",
      "        [ 0.0528],\n",
      "        [-0.1347],\n",
      "        [ 0.0993],\n",
      "        [-0.1347],\n",
      "        [ 0.0047],\n",
      "        [-0.0812],\n",
      "        [ 0.1347]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0812,  0.0528, -0.1347,  0.0993, -0.1347,  0.0047, -0.0812,  0.1347],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1172, -0.0429,  0.1006, -0.1332],\n",
      "        [-0.0960, -0.0799,  0.0381,  0.0542],\n",
      "        [-0.1172, -0.0429,  0.1006, -0.1332],\n",
      "        [-0.0960, -0.0799,  0.0381,  0.0542],\n",
      "        [-0.1172, -0.0429,  0.1006, -0.1332],\n",
      "        [-0.1172, -0.0429,  0.1006, -0.1332],\n",
      "        [-0.0995, -0.0481,  0.1157,  0.0060],\n",
      "        [-0.0960, -0.0799,  0.0381,  0.0542]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 0, 3, 0, 3, 0, 2]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1172],\n",
      "        [-0.0799],\n",
      "        [-0.1172],\n",
      "        [ 0.0542],\n",
      "        [-0.1172],\n",
      "        [-0.1332],\n",
      "        [-0.0995],\n",
      "        [ 0.0381]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1172, -0.0799, -0.1172,  0.0542, -0.1172, -0.1332, -0.0995,  0.0381],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0948, -0.0786,  0.0392,  0.0554],\n",
      "        [-0.1158, -0.0420,  0.1019, -0.1318],\n",
      "        [-0.1158, -0.0420,  0.1019, -0.1318],\n",
      "        [-0.1158, -0.0420,  0.1019, -0.1318],\n",
      "        [-0.1158, -0.0420,  0.1019, -0.1318],\n",
      "        [-0.1158, -0.0420,  0.1019, -0.1318],\n",
      "        [-0.0948, -0.0786,  0.0392,  0.0554],\n",
      "        [-0.0983, -0.0471,  0.1166,  0.0072]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 0, 0, 0, 0, 1, 1]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0948],\n",
      "        [-0.1158],\n",
      "        [-0.1158],\n",
      "        [-0.1158],\n",
      "        [-0.1158],\n",
      "        [-0.1158],\n",
      "        [-0.0786],\n",
      "        [-0.0471]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0948, -0.1158, -0.1158, -0.1158, -0.1158, -0.1158, -0.0786, -0.0471],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1143, -0.0410,  0.1029, -0.1305],\n",
      "        [-0.1143, -0.0410,  0.1029, -0.1305],\n",
      "        [-0.1143, -0.0410,  0.1029, -0.1305],\n",
      "        [-0.0970, -0.0459,  0.1173,  0.0083],\n",
      "        [-0.1143, -0.0410,  0.1029, -0.1305],\n",
      "        [-0.1143, -0.0410,  0.1029, -0.1305],\n",
      "        [-0.1143, -0.0410,  0.1029, -0.1305],\n",
      "        [-0.0970, -0.0459,  0.1173,  0.0083]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 3, 1, 2, 3, 2, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1143],\n",
      "        [-0.1143],\n",
      "        [-0.1305],\n",
      "        [-0.0459],\n",
      "        [ 0.1029],\n",
      "        [-0.1305],\n",
      "        [ 0.1029],\n",
      "        [-0.0970]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1143, -0.1143, -0.1305, -0.0459,  0.1029, -0.1305,  0.1029, -0.0970],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0050, -0.0924,  0.0103,  0.1393],\n",
      "        [-0.1127, -0.0401,  0.1041, -0.1292],\n",
      "        [-0.1127, -0.0401,  0.1041, -0.1292],\n",
      "        [-0.0921, -0.0760,  0.0410,  0.0577],\n",
      "        [-0.1127, -0.0401,  0.1041, -0.1292],\n",
      "        [-0.0286, -0.0282,  0.1144,  0.0569],\n",
      "        [-0.1127, -0.0401,  0.1041, -0.1292],\n",
      "        [-0.1127, -0.0401,  0.1041, -0.1292]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 1, 1, 1, 0, 1, 3, 1]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0103],\n",
      "        [-0.0401],\n",
      "        [-0.0401],\n",
      "        [-0.0760],\n",
      "        [-0.1127],\n",
      "        [-0.0282],\n",
      "        [-0.1292],\n",
      "        [-0.0401]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0103, -0.0401, -0.0401, -0.0760, -0.1127, -0.0282, -0.1292, -0.0401],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1112, -0.0390,  0.1053, -0.1279],\n",
      "        [-0.1112, -0.0390,  0.1053, -0.1279],\n",
      "        [-0.0944, -0.0436,  0.1190,  0.0105],\n",
      "        [-0.1112, -0.0390,  0.1053, -0.1279],\n",
      "        [-0.1112, -0.0390,  0.1053, -0.1279],\n",
      "        [-0.1112, -0.0390,  0.1053, -0.1279],\n",
      "        [-0.0909, -0.0746,  0.0419,  0.0588],\n",
      "        [-0.0909, -0.0746,  0.0419,  0.0588]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 3, 2, 0, 0, 2, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0390],\n",
      "        [-0.1112],\n",
      "        [ 0.0105],\n",
      "        [ 0.1053],\n",
      "        [-0.1112],\n",
      "        [-0.1112],\n",
      "        [ 0.0419],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-0.0746]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0390, -0.1112,  0.0105,  0.1053, -0.1112, -0.1112,  0.0419, -0.0746],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1097, -0.0379,  0.1065, -0.1267],\n",
      "        [-0.1097, -0.0379,  0.1065, -0.1267],\n",
      "        [-0.1097, -0.0379,  0.1065, -0.1267],\n",
      "        [-0.0896, -0.0732,  0.0429,  0.0598],\n",
      "        [-0.0062, -0.1053,  0.2710, -0.0182],\n",
      "        [-0.1097, -0.0379,  0.1065, -0.1267],\n",
      "        [-0.1097, -0.0379,  0.1065, -0.1267],\n",
      "        [-0.1097, -0.0379,  0.1065, -0.1267]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 2, 3, 3, 0, 2, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1097],\n",
      "        [-0.1267],\n",
      "        [ 0.1065],\n",
      "        [ 0.0598],\n",
      "        [-0.0182],\n",
      "        [-0.1097],\n",
      "        [ 0.1065],\n",
      "        [-0.1267]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1097, -0.1267,  0.1065,  0.0598, -0.0182, -0.1097,  0.1065, -0.1267],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0885, -0.0718,  0.0438,  0.0608],\n",
      "        [-0.1082, -0.0369,  0.1077, -0.1254],\n",
      "        [-0.0885, -0.0718,  0.0438,  0.0608],\n",
      "        [-0.1082, -0.0369,  0.1077, -0.1254],\n",
      "        [-0.1082, -0.0369,  0.1077, -0.1254],\n",
      "        [-0.1082, -0.0369,  0.1077, -0.1254],\n",
      "        [-0.0885, -0.0718,  0.0438,  0.0608],\n",
      "        [-0.1082, -0.0369,  0.1077, -0.1254]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 1, 3, 0, 0, 3, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0718],\n",
      "        [-0.1082],\n",
      "        [-0.0718],\n",
      "        [-0.1254],\n",
      "        [-0.1082],\n",
      "        [-0.1082],\n",
      "        [ 0.0608],\n",
      "        [-0.0369]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0718, -0.1082, -0.0718, -0.1254, -0.1082, -0.1082,  0.0608, -0.0369],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1066, -0.0359,  0.1088, -0.1241],\n",
      "        [-0.0873, -0.0704,  0.0446,  0.0619],\n",
      "        [-0.0907, -0.0402,  0.1215,  0.0137],\n",
      "        [-0.1066, -0.0359,  0.1088, -0.1241],\n",
      "        [-0.1066, -0.0359,  0.1088, -0.1241],\n",
      "        [-0.1066, -0.0359,  0.1088, -0.1241],\n",
      "        [-0.0873, -0.0704,  0.0446,  0.0619],\n",
      "        [-0.1066, -0.0359,  0.1088, -0.1241]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 0, 0, 2, 0, 2, 1]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1066],\n",
      "        [-0.0704],\n",
      "        [-0.0907],\n",
      "        [-0.1066],\n",
      "        [ 0.1088],\n",
      "        [-0.1066],\n",
      "        [ 0.0446],\n",
      "        [-0.0359]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1066, -0.0704, -0.0907, -0.1066,  0.1088, -0.1066,  0.0446, -0.0359],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0861, -0.0691,  0.0454,  0.0629],\n",
      "        [-0.1050, -0.0348,  0.1099, -0.1229],\n",
      "        [-0.1050, -0.0348,  0.1099, -0.1229],\n",
      "        [-0.0861, -0.0691,  0.0454,  0.0629],\n",
      "        [-0.1050, -0.0348,  0.1099, -0.1229],\n",
      "        [-0.0893, -0.0391,  0.1223,  0.0146],\n",
      "        [-0.0861, -0.0691,  0.0454,  0.0629],\n",
      "        [-0.0861, -0.0691,  0.0454,  0.0629]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 1, 3, 2, 2, 3, 1, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0629],\n",
      "        [-0.0348],\n",
      "        [-0.1229],\n",
      "        [ 0.0454],\n",
      "        [ 0.1099],\n",
      "        [ 0.0146],\n",
      "        [-0.0691],\n",
      "        [-0.0861]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0629, -0.0348, -0.1229,  0.0454,  0.1099,  0.0146, -0.0691, -0.0861],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1035, -0.0337,  0.1109, -0.1216],\n",
      "        [-0.0850, -0.0677,  0.0461,  0.0640],\n",
      "        [-0.0850, -0.0677,  0.0461,  0.0640],\n",
      "        [-0.1035, -0.0337,  0.1109, -0.1216],\n",
      "        [-0.0850, -0.0677,  0.0461,  0.0640],\n",
      "        [-0.0850, -0.0677,  0.0461,  0.0640],\n",
      "        [-0.0850, -0.0677,  0.0461,  0.0640],\n",
      "        [-0.1035, -0.0337,  0.1109, -0.1216]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 0, 2, 2, 3, 1, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1035],\n",
      "        [-0.0677],\n",
      "        [-0.0850],\n",
      "        [ 0.1109],\n",
      "        [ 0.0461],\n",
      "        [ 0.0640],\n",
      "        [-0.0677],\n",
      "        [-0.1035]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1035, -0.0677, -0.0850,  0.1109,  0.0461,  0.0640, -0.0677, -0.1035],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1020, -0.0326,  0.1120, -0.1205],\n",
      "        [-0.0642, -0.0020,  0.1089, -0.0198],\n",
      "        [-0.1020, -0.0326,  0.1120, -0.1205],\n",
      "        [-0.0642, -0.0020,  0.1089, -0.0198],\n",
      "        [-0.1020, -0.0326,  0.1120, -0.1205],\n",
      "        [ 0.0124, -0.0851,  0.0169,  0.1459],\n",
      "        [-0.1020, -0.0326,  0.1120, -0.1205],\n",
      "        [-0.0837, -0.0664,  0.0471,  0.0650]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 1, 1, 0, 1, 0, 0, 0]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0326],\n",
      "        [-0.0020],\n",
      "        [-0.0326],\n",
      "        [-0.0642],\n",
      "        [-0.0326],\n",
      "        [ 0.0124],\n",
      "        [-0.1020],\n",
      "        [-0.0837]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0326, -0.0020, -0.0326, -0.0642, -0.0326,  0.0124, -0.1020, -0.0837],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.1005, -0.0314,  0.1130, -0.1195],\n",
      "        [-0.0853, -0.0358,  0.1244,  0.0176],\n",
      "        [-0.0853, -0.0358,  0.1244,  0.0176],\n",
      "        [ 0.0138, -0.0840,  0.0176,  0.1467],\n",
      "        [-0.0823, -0.0651,  0.0479,  0.0659],\n",
      "        [-0.0823, -0.0651,  0.0479,  0.0659],\n",
      "        [-0.1005, -0.0314,  0.1130, -0.1195],\n",
      "        [-0.1005, -0.0314,  0.1130, -0.1195]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 1, 0, 1, 1, 0, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1005],\n",
      "        [ 0.0176],\n",
      "        [-0.0358],\n",
      "        [ 0.0138],\n",
      "        [-0.0651],\n",
      "        [-0.0651],\n",
      "        [-0.1005],\n",
      "        [-0.1005]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1005,  0.0176, -0.0358,  0.0138, -0.0651, -0.0651, -0.1005, -0.1005],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0988, -0.0302,  0.1139, -0.1185],\n",
      "        [-0.0838, -0.0347,  0.1251,  0.0185],\n",
      "        [-0.0988, -0.0302,  0.1139, -0.1185],\n",
      "        [-0.0988, -0.0302,  0.1139, -0.1185],\n",
      "        [-0.0808, -0.0637,  0.0487,  0.0667],\n",
      "        [-0.0621, -0.0003,  0.1100, -0.0186],\n",
      "        [-0.0988, -0.0302,  0.1139, -0.1185],\n",
      "        [-0.0988, -0.0302,  0.1139, -0.1185]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 3, 2, 0, 0, 0, 0, 0]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1139],\n",
      "        [ 0.0185],\n",
      "        [ 0.1139],\n",
      "        [-0.0988],\n",
      "        [-0.0808],\n",
      "        [-0.0621],\n",
      "        [-0.0988],\n",
      "        [-0.0988]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1139,  0.0185,  0.1139, -0.0988, -0.0808, -0.0621, -0.0988, -0.0988],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0971, -0.0291,  0.1149, -0.1176],\n",
      "        [-0.0823, -0.0337,  0.1257,  0.0195],\n",
      "        [-0.0608,  0.0005,  0.1105, -0.0180],\n",
      "        [-0.0793, -0.0624,  0.0494,  0.0676],\n",
      "        [-0.0971, -0.0291,  0.1149, -0.1176],\n",
      "        [-0.0971, -0.0291,  0.1149, -0.1176],\n",
      "        [-0.0793, -0.0624,  0.0494,  0.0676],\n",
      "        [-0.0793, -0.0624,  0.0494,  0.0676]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 0, 3, 0, 3, 0, 1]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0971],\n",
      "        [-0.0337],\n",
      "        [-0.0608],\n",
      "        [ 0.0676],\n",
      "        [-0.0971],\n",
      "        [-0.1176],\n",
      "        [-0.0793],\n",
      "        [-0.0624]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0971, -0.0337, -0.0608,  0.0676, -0.0971, -0.1176, -0.0793, -0.0624],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0954, -0.0281,  0.1158, -0.1166],\n",
      "        [-0.0954, -0.0281,  0.1158, -0.1166],\n",
      "        [ 0.0057, -0.0950,  0.2784, -0.0099],\n",
      "        [-0.0954, -0.0281,  0.1158, -0.1166],\n",
      "        [-0.0954, -0.0281,  0.1158, -0.1166],\n",
      "        [-0.0777, -0.0611,  0.0501,  0.0685],\n",
      "        [-0.0954, -0.0281,  0.1158, -0.1166],\n",
      "        [-0.0954, -0.0281,  0.1158, -0.1166]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 3, 0, 1, 2, 2, 0, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.1166],\n",
      "        [-0.1166],\n",
      "        [ 0.0057],\n",
      "        [-0.0281],\n",
      "        [ 0.1158],\n",
      "        [ 0.0501],\n",
      "        [-0.0954],\n",
      "        [-0.0954]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.1166, -0.1166,  0.0057, -0.0281,  0.1158,  0.0501, -0.0954, -0.0954],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0199, -0.0801,  0.0202,  0.1498],\n",
      "        [-0.0581,  0.0021,  0.1114, -0.0167],\n",
      "        [-0.0936, -0.0271,  0.1167, -0.1154],\n",
      "        [ 0.0199, -0.0801,  0.0202,  0.1498],\n",
      "        [-0.0936, -0.0271,  0.1167, -0.1154],\n",
      "        [-0.0761, -0.0599,  0.0508,  0.0695],\n",
      "        [-0.0793, -0.0318,  0.1269,  0.0216],\n",
      "        [-0.0936, -0.0271,  0.1167, -0.1154]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 1, 0, 3, 0, 0, 1]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0199],\n",
      "        [ 0.0021],\n",
      "        [-0.0271],\n",
      "        [ 0.0199],\n",
      "        [-0.1154],\n",
      "        [-0.0761],\n",
      "        [-0.0793],\n",
      "        [-0.0271]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0199,  0.0021, -0.0271,  0.0199, -0.1154, -0.0761, -0.0793, -0.0271],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0919, -0.0259,  0.1175, -0.1143],\n",
      "        [-0.0919, -0.0259,  0.1175, -0.1143],\n",
      "        [-0.0919, -0.0259,  0.1175, -0.1143],\n",
      "        [-0.0919, -0.0259,  0.1175, -0.1143],\n",
      "        [-0.0919, -0.0259,  0.1175, -0.1143],\n",
      "        [ 0.0216, -0.0791,  0.0207,  0.1507],\n",
      "        [-0.0745, -0.0587,  0.0514,  0.0705],\n",
      "        [-0.0146, -0.0312,  0.0840,  0.0530]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 2, 2, 3, 0, 0, 1, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0259],\n",
      "        [ 0.1175],\n",
      "        [ 0.1175],\n",
      "        [-0.1143],\n",
      "        [-0.0919],\n",
      "        [ 0.0216],\n",
      "        [-0.0587],\n",
      "        [-0.0312]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0259,  0.1175,  0.1175, -0.1143, -0.0919,  0.0216, -0.0587, -0.0312],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0728, -0.0574,  0.0520,  0.0714],\n",
      "        [-0.0902, -0.0247,  0.1185, -0.1132],\n",
      "        [-0.0902, -0.0247,  0.1185, -0.1132],\n",
      "        [-0.0728, -0.0574,  0.0520,  0.0714],\n",
      "        [-0.0902, -0.0247,  0.1185, -0.1132],\n",
      "        [-0.0728, -0.0574,  0.0520,  0.0714],\n",
      "        [-0.0728, -0.0574,  0.0520,  0.0714],\n",
      "        [-0.0902, -0.0247,  0.1185, -0.1132]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 1, 0, 1, 1, 0, 2]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0728],\n",
      "        [ 0.1185],\n",
      "        [-0.0247],\n",
      "        [-0.0728],\n",
      "        [-0.0247],\n",
      "        [-0.0574],\n",
      "        [-0.0728],\n",
      "        [ 0.1185]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0728,  0.1185, -0.0247, -0.0728, -0.0247, -0.0574, -0.0728,  0.1185],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0711, -0.0560,  0.0527,  0.0722],\n",
      "        [-0.0886, -0.0233,  0.1195, -0.1121],\n",
      "        [-0.0886, -0.0233,  0.1195, -0.1121],\n",
      "        [-0.0711, -0.0560,  0.0527,  0.0722],\n",
      "        [-0.0711, -0.0560,  0.0527,  0.0722],\n",
      "        [-0.0711, -0.0560,  0.0527,  0.0722],\n",
      "        [-0.0886, -0.0233,  0.1195, -0.1121],\n",
      "        [-0.0711, -0.0560,  0.0527,  0.0722]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 2, 0, 1, 2, 0, 3, 0]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0560],\n",
      "        [ 0.1195],\n",
      "        [-0.0886],\n",
      "        [-0.0560],\n",
      "        [ 0.0527],\n",
      "        [-0.0711],\n",
      "        [-0.1121],\n",
      "        [-0.0711]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0560,  0.1195, -0.0886, -0.0560,  0.0527, -0.0711, -0.1121, -0.0711],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0693, -0.0547,  0.0533,  0.0730],\n",
      "        [-0.0100, -0.0269,  0.0862,  0.0554],\n",
      "        [-0.0871, -0.0220,  0.1206, -0.1111],\n",
      "        [-0.0693, -0.0547,  0.0533,  0.0730],\n",
      "        [-0.0526,  0.0062,  0.1132, -0.0142],\n",
      "        [-0.0871, -0.0220,  0.1206, -0.1111],\n",
      "        [ 0.0268, -0.0755,  0.0224,  0.1529],\n",
      "        [-0.0693, -0.0547,  0.0533,  0.0730]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 1, 0, 1, 0, 0, 0, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0730],\n",
      "        [-0.0269],\n",
      "        [-0.0871],\n",
      "        [-0.0547],\n",
      "        [-0.0526],\n",
      "        [-0.0871],\n",
      "        [ 0.0268],\n",
      "        [-0.0693]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0730, -0.0269, -0.0871, -0.0547, -0.0526, -0.0871,  0.0268, -0.0693],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0675, -0.0533,  0.0539,  0.0739],\n",
      "        [-0.0855, -0.0207,  0.1216, -0.1101],\n",
      "        [-0.0855, -0.0207,  0.1216, -0.1101],\n",
      "        [ 0.0442, -0.0238,  0.2218, -0.0119],\n",
      "        [-0.0855, -0.0207,  0.1216, -0.1101],\n",
      "        [-0.0675, -0.0533,  0.0539,  0.0739],\n",
      "        [-0.0855, -0.0207,  0.1216, -0.1101],\n",
      "        [-0.0711, -0.0260,  0.1297,  0.0261]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 0, 0, 1, 1, 0, 2]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0675],\n",
      "        [-0.0207],\n",
      "        [-0.0855],\n",
      "        [ 0.0442],\n",
      "        [-0.0207],\n",
      "        [-0.0533],\n",
      "        [-0.0855],\n",
      "        [ 0.1297]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0675, -0.0207, -0.0855,  0.0442, -0.0207, -0.0533, -0.0855,  0.1297],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0838, -0.0193,  0.1224, -0.1091],\n",
      "        [-0.0838, -0.0193,  0.1224, -0.1091],\n",
      "        [-0.0838, -0.0193,  0.1224, -0.1091],\n",
      "        [-0.0838, -0.0193,  0.1224, -0.1091],\n",
      "        [-0.0656, -0.0517,  0.0544,  0.0748],\n",
      "        [-0.0838, -0.0193,  0.1224, -0.1091],\n",
      "        [-0.0694, -0.0247,  0.1302,  0.0269],\n",
      "        [-0.0838, -0.0193,  0.1224, -0.1091]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 1, 2, 0, 1, 3, 1, 2]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1224],\n",
      "        [-0.0193],\n",
      "        [ 0.1224],\n",
      "        [-0.0838],\n",
      "        [-0.0517],\n",
      "        [-0.1091],\n",
      "        [-0.0247],\n",
      "        [ 0.1224]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1224, -0.0193,  0.1224, -0.0838, -0.0517, -0.1091, -0.0247,  0.1224],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0639, -0.0501,  0.0550,  0.0756],\n",
      "        [-0.0823, -0.0177,  0.1235, -0.1082],\n",
      "        [-0.0483,  0.0097,  0.1144, -0.0125],\n",
      "        [ 0.0479, -0.0210,  0.2229, -0.0104],\n",
      "        [ 0.0321, -0.0716,  0.0238,  0.1549],\n",
      "        [-0.0823, -0.0177,  0.1235, -0.1082],\n",
      "        [-0.0639, -0.0501,  0.0550,  0.0756],\n",
      "        [-0.0639, -0.0501,  0.0550,  0.0756]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 1, 0, 0, 2, 1, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0639],\n",
      "        [ 0.1235],\n",
      "        [ 0.0097],\n",
      "        [ 0.0479],\n",
      "        [ 0.0321],\n",
      "        [ 0.1235],\n",
      "        [-0.0501],\n",
      "        [-0.0639]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0639,  0.1235,  0.0097,  0.0479,  0.0321,  0.1235, -0.0501, -0.0639],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0663, -0.0217,  0.1315,  0.0283],\n",
      "        [-0.0807, -0.0163,  0.1247, -0.1073],\n",
      "        [ 0.0339, -0.0702,  0.0244,  0.1555],\n",
      "        [-0.0807, -0.0163,  0.1247, -0.1073],\n",
      "        [-0.0621, -0.0485,  0.0557,  0.0764],\n",
      "        [-0.0807, -0.0163,  0.1247, -0.1073],\n",
      "        [-0.0621, -0.0485,  0.0557,  0.0764],\n",
      "        [-0.0621, -0.0485,  0.0557,  0.0764]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 0, 3, 1, 3, 0, 3]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0217],\n",
      "        [-0.0807],\n",
      "        [ 0.0339],\n",
      "        [-0.1073],\n",
      "        [-0.0485],\n",
      "        [-0.1073],\n",
      "        [-0.0621],\n",
      "        [ 0.0764]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0217, -0.0807,  0.0339, -0.1073, -0.0485, -0.1073, -0.0621,  0.0764],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0791, -0.0149,  0.1258, -0.1063],\n",
      "        [-0.0020, -0.0187,  0.0898,  0.0588],\n",
      "        [-0.0791, -0.0149,  0.1258, -0.1063],\n",
      "        [-0.0791, -0.0149,  0.1258, -0.1063],\n",
      "        [-0.0791, -0.0149,  0.1258, -0.1063],\n",
      "        [-0.0791, -0.0149,  0.1258, -0.1063],\n",
      "        [-0.0791, -0.0149,  0.1258, -0.1063],\n",
      "        [-0.0791, -0.0149,  0.1258, -0.1063]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 0, 3, 1, 0, 0, 2]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0791],\n",
      "        [-0.0187],\n",
      "        [-0.0791],\n",
      "        [-0.1063],\n",
      "        [-0.0149],\n",
      "        [-0.0791],\n",
      "        [-0.0791],\n",
      "        [ 0.1258]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0791, -0.0187, -0.0791, -0.1063, -0.0149, -0.0791, -0.0791,  0.1258],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0630, -0.0187,  0.1327,  0.0298],\n",
      "        [-0.0774, -0.0135,  0.1269, -0.1052],\n",
      "        [-0.0004, -0.0170,  0.0905,  0.0595],\n",
      "        [-0.0586, -0.0453,  0.0569,  0.0779],\n",
      "        [-0.0586, -0.0453,  0.0569,  0.0779],\n",
      "        [-0.0774, -0.0135,  0.1269, -0.1052],\n",
      "        [-0.0630, -0.0187,  0.1327,  0.0298],\n",
      "        [-0.0774, -0.0135,  0.1269, -0.1052]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 1, 1, 0, 0, 3, 2]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0630],\n",
      "        [-0.1052],\n",
      "        [-0.0170],\n",
      "        [-0.0453],\n",
      "        [-0.0586],\n",
      "        [-0.0774],\n",
      "        [ 0.0298],\n",
      "        [ 0.1269]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0630, -0.1052, -0.0170, -0.0453, -0.0586, -0.0774,  0.0298,  0.1269],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0614, -0.0172,  0.1333,  0.0307],\n",
      "        [-0.0614, -0.0172,  0.1333,  0.0307],\n",
      "        [-0.0758, -0.0122,  0.1279, -0.1041],\n",
      "        [-0.0568, -0.0438,  0.0575,  0.0788],\n",
      "        [ 0.0555, -0.0152,  0.2251, -0.0076],\n",
      "        [-0.0568, -0.0438,  0.0575,  0.0788],\n",
      "        [-0.0758, -0.0122,  0.1279, -0.1041],\n",
      "        [-0.0568, -0.0438,  0.0575,  0.0788]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 3, 1, 0, 1, 0, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0614],\n",
      "        [-0.0172],\n",
      "        [-0.1041],\n",
      "        [-0.0438],\n",
      "        [ 0.0555],\n",
      "        [-0.0438],\n",
      "        [-0.0758],\n",
      "        [ 0.0788]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0614, -0.0172, -0.1041, -0.0438,  0.0555, -0.0438, -0.0758,  0.0788],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0742, -0.0108,  0.1289, -0.1029],\n",
      "        [-0.0742, -0.0108,  0.1289, -0.1029],\n",
      "        [-0.0597, -0.0156,  0.1337,  0.0316],\n",
      "        [-0.0742, -0.0108,  0.1289, -0.1029],\n",
      "        [-0.0742, -0.0108,  0.1289, -0.1029],\n",
      "        [-0.0597, -0.0156,  0.1337,  0.0316],\n",
      "        [-0.0742, -0.0108,  0.1289, -0.1029],\n",
      "        [-0.0597, -0.0156,  0.1337,  0.0316]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 0, 0, 3, 3, 3, 2]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0108],\n",
      "        [-0.0742],\n",
      "        [-0.0597],\n",
      "        [-0.0742],\n",
      "        [-0.1029],\n",
      "        [ 0.0316],\n",
      "        [-0.1029],\n",
      "        [ 0.1337]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0108, -0.0742, -0.0597, -0.0742, -0.1029,  0.0316, -0.1029,  0.1337],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0725, -0.0095,  0.1297, -0.1016],\n",
      "        [-0.0534, -0.0406,  0.0584,  0.0805],\n",
      "        [-0.0534, -0.0406,  0.0584,  0.0805],\n",
      "        [-0.0400,  0.0169,  0.1169, -0.0090],\n",
      "        [-0.0579, -0.0141,  0.1342,  0.0326],\n",
      "        [-0.0725, -0.0095,  0.1297, -0.1016],\n",
      "        [-0.0534, -0.0406,  0.0584,  0.0805],\n",
      "        [-0.0725, -0.0095,  0.1297, -0.1016]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 1, 2, 2, 0, 3, 0, 0]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0095],\n",
      "        [-0.0406],\n",
      "        [ 0.0584],\n",
      "        [ 0.1169],\n",
      "        [-0.0579],\n",
      "        [-0.1016],\n",
      "        [-0.0534],\n",
      "        [-0.0725]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0095, -0.0406,  0.0584,  0.1169, -0.0579, -0.1016, -0.0534, -0.0725],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0709, -0.0082,  0.1306, -0.1003],\n",
      "        [-0.0709, -0.0082,  0.1306, -0.1003],\n",
      "        [-0.0709, -0.0082,  0.1306, -0.1003],\n",
      "        [-0.0561, -0.0127,  0.1347,  0.0336],\n",
      "        [-0.0709, -0.0082,  0.1306, -0.1003],\n",
      "        [-0.0517, -0.0391,  0.0589,  0.0814],\n",
      "        [ 0.0329, -0.0742,  0.2889,  0.0026],\n",
      "        [-0.0709, -0.0082,  0.1306, -0.1003]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 2, 1, 2, 0, 1, 3, 2]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1306],\n",
      "        [ 0.1306],\n",
      "        [-0.0082],\n",
      "        [ 0.1347],\n",
      "        [-0.0709],\n",
      "        [-0.0391],\n",
      "        [ 0.0026],\n",
      "        [ 0.1306]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1306,  0.1306, -0.0082,  0.1347, -0.0709, -0.0391,  0.0026,  0.1306],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0502, -0.0376,  0.0596,  0.0822],\n",
      "        [-0.0694, -0.0068,  0.1317, -0.0991],\n",
      "        [-0.0544, -0.0112,  0.1354,  0.0345],\n",
      "        [-0.0502, -0.0376,  0.0596,  0.0822],\n",
      "        [-0.0694, -0.0068,  0.1317, -0.0991],\n",
      "        [-0.0694, -0.0068,  0.1317, -0.0991],\n",
      "        [-0.0544, -0.0112,  0.1354,  0.0345],\n",
      "        [-0.0502, -0.0376,  0.0596,  0.0822]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 1, 1, 0, 0, 3, 3, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0376],\n",
      "        [-0.0068],\n",
      "        [-0.0112],\n",
      "        [-0.0502],\n",
      "        [-0.0694],\n",
      "        [-0.0991],\n",
      "        [ 0.0345],\n",
      "        [-0.0376]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0376, -0.0068, -0.0112, -0.0502, -0.0694, -0.0991,  0.0345, -0.0376],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0679, -0.0054,  0.1327, -0.0979],\n",
      "        [-0.0679, -0.0054,  0.1327, -0.0979],\n",
      "        [-0.0679, -0.0054,  0.1327, -0.0979],\n",
      "        [-0.0487, -0.0358,  0.0603,  0.0831],\n",
      "        [-0.0487, -0.0358,  0.0603,  0.0831],\n",
      "        [-0.0679, -0.0054,  0.1327, -0.0979],\n",
      "        [-0.0679, -0.0054,  0.1327, -0.0979],\n",
      "        [-0.0487, -0.0358,  0.0603,  0.0831]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 3, 2, 0, 3, 2, 0, 0]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1327],\n",
      "        [-0.0979],\n",
      "        [ 0.1327],\n",
      "        [-0.0487],\n",
      "        [ 0.0831],\n",
      "        [ 0.1327],\n",
      "        [-0.0679],\n",
      "        [-0.0487]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1327, -0.0979,  0.1327, -0.0487,  0.0831,  0.1327, -0.0679, -0.0487],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0665, -0.0041,  0.1340, -0.0966],\n",
      "        [-0.0472, -0.0343,  0.0610,  0.0840],\n",
      "        [-0.0472, -0.0343,  0.0610,  0.0840],\n",
      "        [ 0.0375, -0.0699,  0.2913,  0.0052],\n",
      "        [-0.0472, -0.0343,  0.0610,  0.0840],\n",
      "        [-0.0665, -0.0041,  0.1340, -0.0966],\n",
      "        [-0.0514, -0.0082,  0.1366,  0.0366],\n",
      "        [-0.0665, -0.0041,  0.1340, -0.0966]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 2, 0, 3, 0, 0, 3, 1]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0966],\n",
      "        [ 0.0610],\n",
      "        [-0.0472],\n",
      "        [ 0.0052],\n",
      "        [-0.0472],\n",
      "        [-0.0665],\n",
      "        [ 0.0366],\n",
      "        [-0.0041]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0966,  0.0610, -0.0472,  0.0052, -0.0472, -0.0665,  0.0366, -0.0041],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0651, -0.0028,  0.1351, -0.0953],\n",
      "        [-0.0651, -0.0028,  0.1351, -0.0953],\n",
      "        [-0.0456, -0.0329,  0.0616,  0.0850],\n",
      "        [-0.0456, -0.0329,  0.0616,  0.0850],\n",
      "        [ 0.0502, -0.0569,  0.0297,  0.1632],\n",
      "        [-0.0456, -0.0329,  0.0616,  0.0850],\n",
      "        [-0.0651, -0.0028,  0.1351, -0.0953],\n",
      "        [-0.0651, -0.0028,  0.1351, -0.0953]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 3, 3, 0, 0, 3, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0651],\n",
      "        [-0.0651],\n",
      "        [ 0.0850],\n",
      "        [ 0.0850],\n",
      "        [ 0.0502],\n",
      "        [-0.0456],\n",
      "        [-0.0953],\n",
      "        [-0.0953]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0651, -0.0651,  0.0850,  0.0850,  0.0502, -0.0456, -0.0953, -0.0953],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(states_v) tensor([[-0.0636, -0.0016,  0.1361, -0.0938],\n",
      "        [ 0.0405, -0.0673,  0.2928,  0.0073],\n",
      "        [-0.0636, -0.0016,  0.1361, -0.0938],\n",
      "        [-0.0440, -0.0316,  0.0621,  0.0861],\n",
      "        [-0.0636, -0.0016,  0.1361, -0.0938],\n",
      "        [ 0.0518, -0.0558,  0.0302,  0.1642],\n",
      "        [-0.0440, -0.0316,  0.0621,  0.0861],\n",
      "        [-0.0440, -0.0316,  0.0621,  0.0861]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 3, 3, 1, 3, 0, 3, 3]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0016],\n",
      "        [ 0.0073],\n",
      "        [-0.0938],\n",
      "        [-0.0316],\n",
      "        [-0.0938],\n",
      "        [ 0.0518],\n",
      "        [ 0.0861],\n",
      "        [ 0.0861]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0016,  0.0073, -0.0938, -0.0316, -0.0938,  0.0518,  0.0861,  0.0861],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0622, -0.0004,  0.1370, -0.0922],\n",
      "        [-0.0622, -0.0004,  0.1370, -0.0922],\n",
      "        [-0.0622, -0.0004,  0.1370, -0.0922],\n",
      "        [-0.0320,  0.0241,  0.1217, -0.0041],\n",
      "        [-0.0469, -0.0044,  0.1383,  0.0403],\n",
      "        [-0.0425, -0.0303,  0.0626,  0.0876],\n",
      "        [-0.0425, -0.0303,  0.0626,  0.0876],\n",
      "        [-0.0622, -0.0004,  0.1370, -0.0922]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 0, 1, 0, 1, 1, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0622],\n",
      "        [-0.0622],\n",
      "        [-0.0622],\n",
      "        [ 0.0241],\n",
      "        [-0.0469],\n",
      "        [-0.0303],\n",
      "        [-0.0303],\n",
      "        [-0.0622]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0622, -0.0622, -0.0622,  0.0241, -0.0469, -0.0303, -0.0303, -0.0622],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0410, -0.0290,  0.0630,  0.0889],\n",
      "        [-0.0454, -0.0032,  0.1387,  0.0416],\n",
      "        [-0.0606,  0.0007,  0.1377, -0.0907],\n",
      "        [ 0.0549, -0.0536,  0.0310,  0.1664],\n",
      "        [-0.0410, -0.0290,  0.0630,  0.0889],\n",
      "        [-0.0454, -0.0032,  0.1387,  0.0416],\n",
      "        [-0.0410, -0.0290,  0.0630,  0.0889],\n",
      "        [-0.0410, -0.0290,  0.0630,  0.0889]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 3, 0, 0, 1, 0, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0410],\n",
      "        [-0.0454],\n",
      "        [-0.0907],\n",
      "        [ 0.0549],\n",
      "        [-0.0410],\n",
      "        [-0.0032],\n",
      "        [-0.0410],\n",
      "        [ 0.0889]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0410, -0.0454, -0.0907,  0.0549, -0.0410, -0.0032, -0.0410,  0.0889],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0590,  0.0018,  0.1384, -0.0892],\n",
      "        [-0.0438, -0.0020,  0.1391,  0.0428],\n",
      "        [-0.0393, -0.0278,  0.0634,  0.0902],\n",
      "        [-0.0590,  0.0018,  0.1384, -0.0892],\n",
      "        [-0.0393, -0.0278,  0.0634,  0.0902],\n",
      "        [-0.0590,  0.0018,  0.1384, -0.0892],\n",
      "        [-0.0590,  0.0018,  0.1384, -0.0892],\n",
      "        [-0.0590,  0.0018,  0.1384, -0.0892]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 2, 2, 1, 3, 0, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0590],\n",
      "        [ 0.0428],\n",
      "        [ 0.0634],\n",
      "        [ 0.1384],\n",
      "        [-0.0278],\n",
      "        [-0.0892],\n",
      "        [-0.0590],\n",
      "        [-0.0892]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0590,  0.0428,  0.0634,  0.1384, -0.0278, -0.0892, -0.0590, -0.0892],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0575,  0.0028,  0.1392, -0.0876],\n",
      "        [-0.0575,  0.0028,  0.1392, -0.0876],\n",
      "        [-0.0575,  0.0028,  0.1392, -0.0876],\n",
      "        [-0.0377, -0.0266,  0.0641,  0.0916],\n",
      "        [-0.0377, -0.0266,  0.0641,  0.0916],\n",
      "        [-0.0286,  0.0272,  0.1231, -0.0015],\n",
      "        [ 0.0184,  0.0013,  0.0995,  0.0721],\n",
      "        [ 0.0581, -0.0515,  0.0318,  0.1686]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 0, 0, 1, 2, 2, 1, 0]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1392],\n",
      "        [-0.0575],\n",
      "        [-0.0575],\n",
      "        [-0.0266],\n",
      "        [ 0.0641],\n",
      "        [ 0.1231],\n",
      "        [ 0.0013],\n",
      "        [ 0.0581]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1392, -0.0575, -0.0575, -0.0266,  0.0641,  0.1231,  0.0013,  0.0581],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0560,  0.0039,  0.1402, -0.0861],\n",
      "        [-0.0362, -0.0253,  0.0649,  0.0928],\n",
      "        [-0.0362, -0.0253,  0.0649,  0.0928],\n",
      "        [-0.0560,  0.0039,  0.1402, -0.0861],\n",
      "        [-0.0560,  0.0039,  0.1402, -0.0861],\n",
      "        [-0.0362, -0.0253,  0.0649,  0.0928],\n",
      "        [-0.0560,  0.0039,  0.1402, -0.0861],\n",
      "        [-0.0275,  0.0282,  0.1240, -0.0007]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 3, 0, 3, 2, 2, 3, 0]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1402],\n",
      "        [ 0.0928],\n",
      "        [-0.0362],\n",
      "        [-0.0861],\n",
      "        [ 0.1402],\n",
      "        [ 0.0649],\n",
      "        [-0.0861],\n",
      "        [-0.0275]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1402,  0.0928, -0.0362, -0.0861,  0.1402,  0.0649, -0.0861, -0.0275],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0545,  0.0048,  0.1414, -0.0845],\n",
      "        [-0.0393,  0.0016,  0.1411,  0.0466],\n",
      "        [-0.0545,  0.0048,  0.1414, -0.0845],\n",
      "        [-0.0265,  0.0291,  0.1249,  0.0002],\n",
      "        [-0.0347, -0.0241,  0.0657,  0.0942],\n",
      "        [-0.0545,  0.0048,  0.1414, -0.0845],\n",
      "        [ 0.0495, -0.0603,  0.2969,  0.0147],\n",
      "        [-0.0545,  0.0048,  0.1414, -0.0845]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 0, 1, 1, 3, 0, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0545],\n",
      "        [-0.0393],\n",
      "        [-0.0545],\n",
      "        [ 0.0291],\n",
      "        [-0.0241],\n",
      "        [-0.0845],\n",
      "        [ 0.0495],\n",
      "        [-0.0845]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0545, -0.0393, -0.0545,  0.0291, -0.0241, -0.0845,  0.0495, -0.0845],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0628, -0.0483,  0.0338,  0.1718],\n",
      "        [-0.0332, -0.0227,  0.0665,  0.0955],\n",
      "        [-0.0531,  0.0058,  0.1424, -0.0829],\n",
      "        [-0.0531,  0.0058,  0.1424, -0.0829],\n",
      "        [-0.0254,  0.0302,  0.1257,  0.0011],\n",
      "        [-0.0531,  0.0058,  0.1424, -0.0829],\n",
      "        [-0.0531,  0.0058,  0.1424, -0.0829],\n",
      "        [-0.0332, -0.0227,  0.0665,  0.0955]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 1, 0, 1, 2, 2, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0628],\n",
      "        [-0.0227],\n",
      "        [ 0.0058],\n",
      "        [-0.0531],\n",
      "        [ 0.0302],\n",
      "        [ 0.1424],\n",
      "        [ 0.1424],\n",
      "        [-0.0332]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0628, -0.0227,  0.0058, -0.0531,  0.0302,  0.1424,  0.1424, -0.0332],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0516,  0.0069,  0.1435, -0.0814],\n",
      "        [-0.0516,  0.0069,  0.1435, -0.0814],\n",
      "        [-0.0516,  0.0069,  0.1435, -0.0814],\n",
      "        [-0.0516,  0.0069,  0.1435, -0.0814],\n",
      "        [-0.0365,  0.0040,  0.1425,  0.0490],\n",
      "        [-0.0516,  0.0069,  0.1435, -0.0814],\n",
      "        [ 0.0807,  0.0047,  0.2349,  0.0086],\n",
      "        [-0.0516,  0.0069,  0.1435, -0.0814]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 0, 1, 0, 3, 0, 0, 0]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1435],\n",
      "        [-0.0516],\n",
      "        [ 0.0069],\n",
      "        [-0.0516],\n",
      "        [ 0.0490],\n",
      "        [-0.0516],\n",
      "        [ 0.0807],\n",
      "        [-0.0516]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1435, -0.0516,  0.0069, -0.0516,  0.0490, -0.0516,  0.0807, -0.0516],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0499,  0.0080,  0.1447, -0.0800],\n",
      "        [-0.0301, -0.0199,  0.0681,  0.0979],\n",
      "        [-0.0499,  0.0080,  0.1447, -0.0800],\n",
      "        [-0.0499,  0.0080,  0.1447, -0.0800],\n",
      "        [-0.0499,  0.0080,  0.1447, -0.0800],\n",
      "        [-0.0499,  0.0080,  0.1447, -0.0800],\n",
      "        [ 0.0824,  0.0059,  0.2356,  0.0096],\n",
      "        [-0.0349,  0.0052,  0.1432,  0.0502]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 0, 1, 0, 0, 1, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0080],\n",
      "        [-0.0301],\n",
      "        [-0.0499],\n",
      "        [ 0.0080],\n",
      "        [-0.0499],\n",
      "        [-0.0499],\n",
      "        [ 0.0059],\n",
      "        [ 0.0052]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0080, -0.0301, -0.0499,  0.0080, -0.0499, -0.0499,  0.0059,  0.0052],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0220,  0.0339,  0.1280,  0.0035],\n",
      "        [-0.0334,  0.0066,  0.1439,  0.0513],\n",
      "        [-0.0334,  0.0066,  0.1439,  0.0513],\n",
      "        [-0.0482,  0.0093,  0.1457, -0.0787],\n",
      "        [-0.0285, -0.0185,  0.0688,  0.0989],\n",
      "        [-0.0285, -0.0185,  0.0688,  0.0989],\n",
      "        [-0.0285, -0.0185,  0.0688,  0.0989],\n",
      "        [-0.0285, -0.0185,  0.0688,  0.0989]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 1, 2, 2, 0, 0, 2, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0339],\n",
      "        [ 0.0066],\n",
      "        [ 0.1439],\n",
      "        [ 0.1457],\n",
      "        [-0.0285],\n",
      "        [-0.0285],\n",
      "        [ 0.0688],\n",
      "        [-0.0185]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0339,  0.0066,  0.1439,  0.1457, -0.0285, -0.0285,  0.0688, -0.0185],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0320,  0.0080,  0.1445,  0.0523],\n",
      "        [-0.0466,  0.0105,  0.1467, -0.0776],\n",
      "        [-0.0208,  0.0350,  0.1287,  0.0041],\n",
      "        [-0.0269, -0.0173,  0.0695,  0.0999],\n",
      "        [-0.0466,  0.0105,  0.1467, -0.0776],\n",
      "        [-0.0466,  0.0105,  0.1467, -0.0776],\n",
      "        [-0.0269, -0.0173,  0.0695,  0.0999],\n",
      "        [ 0.0582, -0.0539,  0.3012,  0.0202]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 1, 0, 1, 0, 3, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0320],\n",
      "        [ 0.1467],\n",
      "        [ 0.0350],\n",
      "        [-0.0269],\n",
      "        [ 0.0105],\n",
      "        [-0.0466],\n",
      "        [ 0.0999],\n",
      "        [ 0.0582]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0320,  0.1467,  0.0350, -0.0269,  0.0105, -0.0466,  0.0999,  0.0582],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0450,  0.0117,  0.1478, -0.0765],\n",
      "        [-0.0252, -0.0160,  0.0701,  0.1008],\n",
      "        [-0.0252, -0.0160,  0.0701,  0.1008],\n",
      "        [-0.0306,  0.0094,  0.1452,  0.0532],\n",
      "        [-0.0450,  0.0117,  0.1478, -0.0765],\n",
      "        [-0.0252, -0.0160,  0.0701,  0.1008],\n",
      "        [-0.0450,  0.0117,  0.1478, -0.0765],\n",
      "        [ 0.0297,  0.0113,  0.1064,  0.0803]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 1, 2, 0, 2, 0, 3, 1]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1478],\n",
      "        [-0.0160],\n",
      "        [ 0.0701],\n",
      "        [-0.0306],\n",
      "        [ 0.1478],\n",
      "        [-0.0252],\n",
      "        [-0.0765],\n",
      "        [ 0.0113]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1478, -0.0160,  0.0701, -0.0306,  0.1478, -0.0252, -0.0765,  0.0113],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0237, -0.0147,  0.0709,  0.1018],\n",
      "        [-0.0237, -0.0147,  0.0709,  0.1018],\n",
      "        [-0.0237, -0.0147,  0.0709,  0.1018],\n",
      "        [-0.0436,  0.0130,  0.1490, -0.0754],\n",
      "        [-0.0436,  0.0130,  0.1490, -0.0754],\n",
      "        [ 0.0618, -0.0511,  0.3029,  0.0220],\n",
      "        [-0.0292,  0.0109,  0.1459,  0.0542],\n",
      "        [ 0.0722, -0.0410,  0.0375,  0.1771]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 1, 0, 3, 3, 3, 3, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1018],\n",
      "        [-0.0147],\n",
      "        [-0.0237],\n",
      "        [-0.0754],\n",
      "        [-0.0754],\n",
      "        [ 0.0220],\n",
      "        [ 0.0542],\n",
      "        [ 0.0722]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1018, -0.0147, -0.0237, -0.0754, -0.0754,  0.0220,  0.0542,  0.0722],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0221, -0.0133,  0.0716,  0.1029],\n",
      "        [ 0.0324,  0.0143,  0.1080,  0.0821],\n",
      "        [-0.0422,  0.0142,  0.1502, -0.0741],\n",
      "        [-0.0422,  0.0142,  0.1502, -0.0741],\n",
      "        [-0.0221, -0.0133,  0.0716,  0.1029],\n",
      "        [-0.0422,  0.0142,  0.1502, -0.0741],\n",
      "        [-0.0422,  0.0142,  0.1502, -0.0741],\n",
      "        [-0.0175,  0.0389,  0.1307,  0.0062]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 1, 2, 3, 0, 2, 1, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1029],\n",
      "        [ 0.0143],\n",
      "        [ 0.1502],\n",
      "        [-0.0741],\n",
      "        [-0.0221],\n",
      "        [ 0.1502],\n",
      "        [ 0.0142],\n",
      "        [-0.0175]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1029,  0.0143,  0.1502, -0.0741, -0.0221,  0.1502,  0.0142, -0.0175],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0267,  0.0137,  0.1473,  0.0565],\n",
      "        [-0.0206, -0.0119,  0.0723,  0.1042],\n",
      "        [-0.0410,  0.0155,  0.1514, -0.0728],\n",
      "        [-0.0206, -0.0119,  0.0723,  0.1042],\n",
      "        [-0.0206, -0.0119,  0.0723,  0.1042],\n",
      "        [-0.0410,  0.0155,  0.1514, -0.0728],\n",
      "        [-0.0267,  0.0137,  0.1473,  0.0565],\n",
      "        [-0.0206, -0.0119,  0.0723,  0.1042]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 3, 2, 3, 2, 3, 3, 3]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1473],\n",
      "        [ 0.1042],\n",
      "        [ 0.1514],\n",
      "        [ 0.1042],\n",
      "        [ 0.0723],\n",
      "        [-0.0728],\n",
      "        [ 0.0565],\n",
      "        [ 0.1042]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1473,  0.1042,  0.1514,  0.1042,  0.0723, -0.0728,  0.0565,  0.1042],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0348,  0.0175,  0.1098,  0.0844],\n",
      "        [-0.0398,  0.0168,  0.1528, -0.0713],\n",
      "        [-0.0192, -0.0106,  0.0730,  0.1057],\n",
      "        [-0.0398,  0.0168,  0.1528, -0.0713],\n",
      "        [-0.0398,  0.0168,  0.1528, -0.0713],\n",
      "        [-0.0256,  0.0151,  0.1481,  0.0579],\n",
      "        [-0.0192, -0.0106,  0.0730,  0.1057],\n",
      "        [-0.0398,  0.0168,  0.1528, -0.0713]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 1, 3, 2, 0, 0, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0175],\n",
      "        [-0.0398],\n",
      "        [-0.0106],\n",
      "        [-0.0713],\n",
      "        [ 0.1528],\n",
      "        [-0.0256],\n",
      "        [-0.0192],\n",
      "        [ 0.0168]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0175, -0.0398, -0.0106, -0.0713,  0.1528, -0.0256, -0.0192,  0.0168],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0245,  0.0164,  0.1488,  0.0593],\n",
      "        [ 0.0776, -0.0359,  0.0401,  0.1814],\n",
      "        [ 0.0359,  0.0192,  0.1107,  0.0856],\n",
      "        [ 0.0776, -0.0359,  0.0401,  0.1814],\n",
      "        [-0.0387,  0.0181,  0.1541, -0.0698],\n",
      "        [-0.0387,  0.0181,  0.1541, -0.0698],\n",
      "        [-0.0178, -0.0093,  0.0738,  0.1072],\n",
      "        [-0.0387,  0.0181,  0.1541, -0.0698]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 0, 1, 0, 2, 0, 1, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0593],\n",
      "        [ 0.0776],\n",
      "        [ 0.0192],\n",
      "        [ 0.0776],\n",
      "        [ 0.1541],\n",
      "        [-0.0387],\n",
      "        [-0.0093],\n",
      "        [-0.0387]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0593,  0.0776,  0.0192,  0.0776,  0.1541, -0.0387, -0.0093, -0.0387],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0164, -0.0079,  0.0745,  0.1086],\n",
      "        [-0.0374,  0.0194,  0.1554, -0.0684],\n",
      "        [-0.0374,  0.0194,  0.1554, -0.0684],\n",
      "        [-0.0374,  0.0194,  0.1554, -0.0684],\n",
      "        [-0.0374,  0.0194,  0.1554, -0.0684],\n",
      "        [-0.0374,  0.0194,  0.1554, -0.0684],\n",
      "        [-0.0374,  0.0194,  0.1554, -0.0684],\n",
      "        [-0.0164, -0.0079,  0.0745,  0.1086]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 2, 0, 1, 0, 0, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0164],\n",
      "        [-0.0374],\n",
      "        [ 0.1554],\n",
      "        [-0.0374],\n",
      "        [ 0.0194],\n",
      "        [-0.0374],\n",
      "        [-0.0374],\n",
      "        [-0.0164]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0164, -0.0374,  0.1554, -0.0374,  0.0194, -0.0374, -0.0374, -0.0164],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0219,  0.0192,  0.1503,  0.0619],\n",
      "        [ 0.0806, -0.0333,  0.0415,  0.1836],\n",
      "        [ 0.0806, -0.0333,  0.0415,  0.1836],\n",
      "        [-0.0359,  0.0206,  0.1568, -0.0672],\n",
      "        [-0.0359,  0.0206,  0.1568, -0.0672],\n",
      "        [-0.0359,  0.0206,  0.1568, -0.0672],\n",
      "        [-0.0359,  0.0206,  0.1568, -0.0672],\n",
      "        [-0.0359,  0.0206,  0.1568, -0.0672]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 0, 0, 0, 1, 0, 0, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0219],\n",
      "        [ 0.0806],\n",
      "        [ 0.0806],\n",
      "        [-0.0359],\n",
      "        [ 0.0206],\n",
      "        [-0.0359],\n",
      "        [-0.0359],\n",
      "        [-0.0672]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0219,  0.0806,  0.0806, -0.0359,  0.0206, -0.0359, -0.0359, -0.0672],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0131, -0.0054,  0.0759,  0.1111],\n",
      "        [-0.0343,  0.0219,  0.1580, -0.0659],\n",
      "        [-0.0131, -0.0054,  0.0759,  0.1111],\n",
      "        [-0.0343,  0.0219,  0.1580, -0.0659],\n",
      "        [-0.0131, -0.0054,  0.0759,  0.1111],\n",
      "        [-0.0343,  0.0219,  0.1580, -0.0659],\n",
      "        [-0.0343,  0.0219,  0.1580, -0.0659],\n",
      "        [-0.0343,  0.0219,  0.1580, -0.0659]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 0, 2, 0, 1, 1, 2, 2]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1111],\n",
      "        [-0.0343],\n",
      "        [ 0.0759],\n",
      "        [-0.0343],\n",
      "        [-0.0054],\n",
      "        [ 0.0219],\n",
      "        [ 0.1580],\n",
      "        [ 0.1580]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1111, -0.0343,  0.0759, -0.0343, -0.0054,  0.0219,  0.1580,  0.1580],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0415,  0.0259,  0.1144,  0.0897],\n",
      "        [-0.0115, -0.0042,  0.0769,  0.1121],\n",
      "        [-0.0115, -0.0042,  0.0769,  0.1121],\n",
      "        [-0.0115, -0.0042,  0.0769,  0.1121],\n",
      "        [-0.0115, -0.0042,  0.0769,  0.1121],\n",
      "        [-0.0326,  0.0231,  0.1594, -0.0648],\n",
      "        [-0.0326,  0.0231,  0.1594, -0.0648],\n",
      "        [-0.0115, -0.0042,  0.0769,  0.1121]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 3, 0, 1, 0, 2, 3, 2]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0259],\n",
      "        [ 0.1121],\n",
      "        [-0.0115],\n",
      "        [-0.0042],\n",
      "        [-0.0115],\n",
      "        [ 0.1594],\n",
      "        [-0.0648],\n",
      "        [ 0.0769]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0259,  0.1121, -0.0115, -0.0042, -0.0115,  0.1594, -0.0648,  0.0769],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0311,  0.0244,  0.1608, -0.0636],\n",
      "        [-0.0099, -0.0030,  0.0781,  0.1133],\n",
      "        [-0.0311,  0.0244,  0.1608, -0.0636],\n",
      "        [-0.0311,  0.0244,  0.1608, -0.0636],\n",
      "        [-0.0311,  0.0244,  0.1608, -0.0636],\n",
      "        [-0.0099, -0.0030,  0.0781,  0.1133],\n",
      "        [-0.0311,  0.0244,  0.1608, -0.0636],\n",
      "        [ 0.1009,  0.0226,  0.2455,  0.0230]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 2, 0, 0, 3, 3, 0, 1]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1608],\n",
      "        [ 0.0781],\n",
      "        [-0.0311],\n",
      "        [-0.0311],\n",
      "        [-0.0636],\n",
      "        [ 0.1133],\n",
      "        [-0.0311],\n",
      "        [ 0.0226]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1608,  0.0781, -0.0311, -0.0311, -0.0636,  0.1133, -0.0311,  0.0226],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0164,  0.0240,  0.1539,  0.0664],\n",
      "        [-0.0083, -0.0020,  0.0795,  0.1145],\n",
      "        [-0.0295,  0.0255,  0.1624, -0.0623],\n",
      "        [-0.0295,  0.0255,  0.1624, -0.0623],\n",
      "        [-0.0295,  0.0255,  0.1624, -0.0623],\n",
      "        [-0.0295,  0.0255,  0.1624, -0.0623],\n",
      "        [-0.0295,  0.0255,  0.1624, -0.0623],\n",
      "        [-0.0164,  0.0240,  0.1539,  0.0664]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 1, 2, 2, 0, 2, 1, 3]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0240],\n",
      "        [-0.0020],\n",
      "        [ 0.1624],\n",
      "        [ 0.1624],\n",
      "        [-0.0295],\n",
      "        [ 0.1624],\n",
      "        [ 0.0255],\n",
      "        [ 0.0664]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0240, -0.0020,  0.1624,  0.1624, -0.0295,  0.1624,  0.0255,  0.0664],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0069, -0.0009,  0.0810,  0.1156],\n",
      "        [-0.0281,  0.0267,  0.1642, -0.0612],\n",
      "        [-0.0281,  0.0267,  0.1642, -0.0612],\n",
      "        [-0.0281,  0.0267,  0.1642, -0.0612],\n",
      "        [-0.0281,  0.0267,  0.1642, -0.0612],\n",
      "        [-0.0281,  0.0267,  0.1642, -0.0612],\n",
      "        [-0.0152,  0.0251,  0.1551,  0.0676],\n",
      "        [-0.0281,  0.0267,  0.1642, -0.0612]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 3, 1, 2, 2, 3, 0]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0009],\n",
      "        [-0.0281],\n",
      "        [-0.0612],\n",
      "        [ 0.0267],\n",
      "        [ 0.1642],\n",
      "        [ 0.1642],\n",
      "        [ 0.0676],\n",
      "        [-0.0281]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0009, -0.0281, -0.0612,  0.0267,  0.1642,  0.1642,  0.0676, -0.0281],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0267,  0.0278,  0.1660, -0.0599],\n",
      "        [-0.0267,  0.0278,  0.1660, -0.0599],\n",
      "        [-0.0267,  0.0278,  0.1660, -0.0599],\n",
      "        [-0.0267,  0.0278,  0.1660, -0.0599],\n",
      "        [-0.0267,  0.0278,  0.1660, -0.0599],\n",
      "        [-0.0056,  0.0002,  0.0825,  0.1168],\n",
      "        [-0.0267,  0.0278,  0.1660, -0.0599],\n",
      "        [-0.0267,  0.0278,  0.1660, -0.0599]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 1, 0, 0, 3, 2, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0267],\n",
      "        [-0.0599],\n",
      "        [ 0.0278],\n",
      "        [-0.0267],\n",
      "        [-0.0267],\n",
      "        [ 0.1168],\n",
      "        [ 0.1660],\n",
      "        [-0.0599]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0267, -0.0599,  0.0278, -0.0267, -0.0267,  0.1168,  0.1660, -0.0599],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0251,  0.0290,  0.1678, -0.0585],\n",
      "        [-0.0251,  0.0290,  0.1678, -0.0585],\n",
      "        [-0.0054,  0.0519,  0.1401,  0.0162],\n",
      "        [-0.0128,  0.0272,  0.1574,  0.0702],\n",
      "        [-0.0251,  0.0290,  0.1678, -0.0585],\n",
      "        [-0.0251,  0.0290,  0.1678, -0.0585],\n",
      "        [-0.0251,  0.0290,  0.1678, -0.0585],\n",
      "        [-0.0251,  0.0290,  0.1678, -0.0585]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 1, 1, 0, 1, 0, 2]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0290],\n",
      "        [-0.0251],\n",
      "        [ 0.0519],\n",
      "        [ 0.0272],\n",
      "        [-0.0251],\n",
      "        [ 0.0290],\n",
      "        [-0.0251],\n",
      "        [ 0.1678]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0290, -0.0251,  0.0519,  0.0272, -0.0251,  0.0290, -0.0251,  0.1678],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0029,  0.0024,  0.0852,  0.1193],\n",
      "        [-0.0235,  0.0304,  0.1695, -0.0573],\n",
      "        [-0.0235,  0.0304,  0.1695, -0.0573],\n",
      "        [-0.0029,  0.0024,  0.0852,  0.1193],\n",
      "        [-0.0235,  0.0304,  0.1695, -0.0573],\n",
      "        [-0.0029,  0.0024,  0.0852,  0.1193],\n",
      "        [-0.0235,  0.0304,  0.1695, -0.0573],\n",
      "        [-0.0235,  0.0304,  0.1695, -0.0573]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 3, 2, 0, 0, 2, 3]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0024],\n",
      "        [-0.0235],\n",
      "        [-0.0573],\n",
      "        [ 0.0852],\n",
      "        [-0.0235],\n",
      "        [-0.0029],\n",
      "        [ 0.1695],\n",
      "        [-0.0573]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0024, -0.0235, -0.0573,  0.0852, -0.0235, -0.0029,  0.1695, -0.0573],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0219,  0.0316,  0.1713, -0.0558],\n",
      "        [-0.0016,  0.0035,  0.0868,  0.1205],\n",
      "        [-0.0219,  0.0316,  0.1713, -0.0558],\n",
      "        [-0.0219,  0.0316,  0.1713, -0.0558],\n",
      "        [-0.0219,  0.0316,  0.1713, -0.0558],\n",
      "        [-0.0219,  0.0316,  0.1713, -0.0558],\n",
      "        [-0.0016,  0.0035,  0.0868,  0.1205],\n",
      "        [-0.0219,  0.0316,  0.1713, -0.0558]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 0, 1, 2, 2, 0, 2, 2]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0558],\n",
      "        [-0.0016],\n",
      "        [ 0.0316],\n",
      "        [ 0.1713],\n",
      "        [ 0.1713],\n",
      "        [-0.0219],\n",
      "        [ 0.0868],\n",
      "        [ 0.1713]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0558, -0.0016,  0.0316,  0.1713,  0.1713, -0.0219,  0.0868,  0.1713],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0204,  0.0329,  0.1732, -0.0544],\n",
      "        [-0.0204,  0.0329,  0.1732, -0.0544],\n",
      "        [-0.0204,  0.0329,  0.1732, -0.0544],\n",
      "        [ 0.1093,  0.0303,  0.2544,  0.0304],\n",
      "        [ 0.0858, -0.0309,  0.3208,  0.0405],\n",
      "        [-0.0204,  0.0329,  0.1732, -0.0544],\n",
      "        [-0.0204,  0.0329,  0.1732, -0.0544],\n",
      "        [-0.0094,  0.0309,  0.1610,  0.0738]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 3, 1, 0, 0, 3, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0204],\n",
      "        [-0.0544],\n",
      "        [-0.0544],\n",
      "        [ 0.0303],\n",
      "        [ 0.0858],\n",
      "        [-0.0204],\n",
      "        [-0.0544],\n",
      "        [-0.0094]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0204, -0.0544, -0.0544,  0.0303,  0.0858, -0.0204, -0.0544, -0.0094],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0012,  0.0056,  0.0901,  0.1229],\n",
      "        [-0.0187,  0.0340,  0.1750, -0.0528],\n",
      "        [-0.0187,  0.0340,  0.1750, -0.0528],\n",
      "        [-0.0187,  0.0340,  0.1750, -0.0528],\n",
      "        [-0.0187,  0.0340,  0.1750, -0.0528],\n",
      "        [-0.0187,  0.0340,  0.1750, -0.0528],\n",
      "        [-0.0187,  0.0340,  0.1750, -0.0528],\n",
      "        [-0.0016,  0.0554,  0.1441,  0.0195]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 2, 3, 0, 3, 1, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0012],\n",
      "        [ 0.0340],\n",
      "        [ 0.1750],\n",
      "        [-0.0528],\n",
      "        [-0.0187],\n",
      "        [-0.0528],\n",
      "        [ 0.0340],\n",
      "        [-0.0016]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0012,  0.0340,  0.1750, -0.0528, -0.0187, -0.0528,  0.0340, -0.0016],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0171,  0.0353,  0.1767, -0.0511],\n",
      "        [-0.0171,  0.0353,  0.1767, -0.0511],\n",
      "        [ 0.0026,  0.0067,  0.0916,  0.1241],\n",
      "        [ 0.0026,  0.0067,  0.0916,  0.1241],\n",
      "        [-0.0171,  0.0353,  0.1767, -0.0511],\n",
      "        [-0.0006,  0.0563,  0.1450,  0.0204],\n",
      "        [-0.0171,  0.0353,  0.1767, -0.0511],\n",
      "        [-0.0171,  0.0353,  0.1767, -0.0511]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 3, 0, 0, 1, 3, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0353],\n",
      "        [-0.0171],\n",
      "        [ 0.1241],\n",
      "        [ 0.0026],\n",
      "        [-0.0171],\n",
      "        [ 0.0563],\n",
      "        [-0.0511],\n",
      "        [ 0.0353]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0353, -0.0171,  0.1241,  0.0026, -0.0171,  0.0563, -0.0511,  0.0353],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0155,  0.0367,  0.1782, -0.0494],\n",
      "        [ 0.0004,  0.0573,  0.1459,  0.0213],\n",
      "        [-0.0155,  0.0367,  0.1782, -0.0494],\n",
      "        [-0.0155,  0.0367,  0.1782, -0.0494],\n",
      "        [ 0.0040,  0.0077,  0.0929,  0.1252],\n",
      "        [-0.0054,  0.0342,  0.1644,  0.0776],\n",
      "        [ 0.0040,  0.0077,  0.0929,  0.1252],\n",
      "        [-0.0155,  0.0367,  0.1782, -0.0494]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 1, 2, 2, 2, 0, 0, 0]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0367],\n",
      "        [ 0.0573],\n",
      "        [ 0.1782],\n",
      "        [ 0.1782],\n",
      "        [ 0.0929],\n",
      "        [-0.0054],\n",
      "        [ 0.0040],\n",
      "        [-0.0155]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0367,  0.0573,  0.1782,  0.1782,  0.0929, -0.0054,  0.0040, -0.0155],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0139,  0.0381,  0.1798, -0.0479],\n",
      "        [-0.0139,  0.0381,  0.1798, -0.0479],\n",
      "        [-0.0139,  0.0381,  0.1798, -0.0479],\n",
      "        [ 0.0055,  0.0088,  0.0942,  0.1263],\n",
      "        [-0.0139,  0.0381,  0.1798, -0.0479],\n",
      "        [-0.0139,  0.0381,  0.1798, -0.0479],\n",
      "        [-0.0139,  0.0381,  0.1798, -0.0479],\n",
      "        [ 0.0014,  0.0585,  0.1467,  0.0221]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 1, 0, 2, 3, 0, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0139],\n",
      "        [-0.0479],\n",
      "        [ 0.0381],\n",
      "        [ 0.0055],\n",
      "        [ 0.1798],\n",
      "        [-0.0479],\n",
      "        [-0.0139],\n",
      "        [ 0.0014]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0139, -0.0479,  0.0381,  0.0055,  0.1798, -0.0479, -0.0139,  0.0014],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0122,  0.0395,  0.1814, -0.0463],\n",
      "        [-0.0122,  0.0395,  0.1814, -0.0463],\n",
      "        [ 0.0071,  0.0099,  0.0955,  0.1273],\n",
      "        [-0.0122,  0.0395,  0.1814, -0.0463],\n",
      "        [-0.0122,  0.0395,  0.1814, -0.0463],\n",
      "        [ 0.0071,  0.0099,  0.0955,  0.1273],\n",
      "        [-0.0122,  0.0395,  0.1814, -0.0463],\n",
      "        [ 0.0071,  0.0099,  0.0955,  0.1273]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 3, 0, 3, 3, 2, 3, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0395],\n",
      "        [-0.0463],\n",
      "        [ 0.0071],\n",
      "        [-0.0463],\n",
      "        [-0.0463],\n",
      "        [ 0.0955],\n",
      "        [-0.0463],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 0.0099]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0395, -0.0463,  0.0071, -0.0463, -0.0463,  0.0955, -0.0463,  0.0099],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0106,  0.0408,  0.1828, -0.0444],\n",
      "        [ 0.0087,  0.0108,  0.0966,  0.1285],\n",
      "        [ 0.0087,  0.0108,  0.0966,  0.1285],\n",
      "        [-0.0106,  0.0408,  0.1828, -0.0444],\n",
      "        [-0.0106,  0.0408,  0.1828, -0.0444],\n",
      "        [ 0.0955, -0.0242,  0.3285,  0.0475],\n",
      "        [-0.0106,  0.0408,  0.1828, -0.0444],\n",
      "        [-0.0012,  0.0377,  0.1674,  0.0810]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 1, 2, 3, 0, 0, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0106],\n",
      "        [ 0.0966],\n",
      "        [ 0.0108],\n",
      "        [ 0.1828],\n",
      "        [-0.0444],\n",
      "        [ 0.0955],\n",
      "        [-0.0106],\n",
      "        [-0.0012]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0106,  0.0966,  0.0108,  0.1828, -0.0444,  0.0955, -0.0106, -0.0012],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0102,  0.0119,  0.0979,  0.1297],\n",
      "        [-0.0090,  0.0421,  0.1843, -0.0426],\n",
      "        [-0.0090,  0.0421,  0.1843, -0.0426],\n",
      "        [ 0.0102,  0.0119,  0.0979,  0.1297],\n",
      "        [-0.0090,  0.0421,  0.1843, -0.0426],\n",
      "        [-0.0090,  0.0421,  0.1843, -0.0426],\n",
      "        [-0.0090,  0.0421,  0.1843, -0.0426],\n",
      "        [ 0.1040, -0.0147,  0.0585,  0.2011]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 1, 2, 3, 3, 0, 3, 0]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0119],\n",
      "        [ 0.0421],\n",
      "        [ 0.1843],\n",
      "        [ 0.1297],\n",
      "        [-0.0426],\n",
      "        [-0.0090],\n",
      "        [-0.0426],\n",
      "        [ 0.1040]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0119,  0.0421,  0.1843,  0.1297, -0.0426, -0.0090, -0.0426,  0.1040],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0074,  0.0433,  0.1858, -0.0407],\n",
      "        [ 0.0118,  0.0129,  0.0992,  0.1310],\n",
      "        [-0.0074,  0.0433,  0.1858, -0.0407],\n",
      "        [ 0.0118,  0.0129,  0.0992,  0.1310],\n",
      "        [ 0.1054, -0.0137,  0.0595,  0.2023],\n",
      "        [-0.0074,  0.0433,  0.1858, -0.0407],\n",
      "        [ 0.0118,  0.0129,  0.0992,  0.1310],\n",
      "        [-0.0074,  0.0433,  0.1858, -0.0407]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 1, 0, 0, 0, 1, 0]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0074],\n",
      "        [ 0.1310],\n",
      "        [ 0.0433],\n",
      "        [ 0.0118],\n",
      "        [ 0.1054],\n",
      "        [-0.0074],\n",
      "        [ 0.0129],\n",
      "        [-0.0074]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0074,  0.1310,  0.0433,  0.0118,  0.1054, -0.0074,  0.0129, -0.0074],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0056,  0.0446,  0.1871, -0.0389],\n",
      "        [-0.0056,  0.0446,  0.1871, -0.0389],\n",
      "        [ 0.0642,  0.0480,  0.1360,  0.1086],\n",
      "        [-0.0056,  0.0446,  0.1871, -0.0389],\n",
      "        [ 0.0035,  0.0409,  0.1703,  0.0847],\n",
      "        [-0.0056,  0.0446,  0.1871, -0.0389],\n",
      "        [ 0.0035,  0.0409,  0.1703,  0.0847],\n",
      "        [-0.0056,  0.0446,  0.1871, -0.0389]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 1, 3, 0, 2, 0, 2]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0446],\n",
      "        [-0.0056],\n",
      "        [ 0.0480],\n",
      "        [-0.0389],\n",
      "        [ 0.0035],\n",
      "        [ 0.1871],\n",
      "        [ 0.0035],\n",
      "        [ 0.1871]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0446, -0.0056,  0.0480, -0.0389,  0.0035,  0.1871,  0.0035,  0.1871],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0040,  0.0461,  0.1884, -0.0372],\n",
      "        [ 0.0084,  0.0647,  0.1513,  0.0276],\n",
      "        [ 0.1085, -0.0114,  0.0611,  0.2045],\n",
      "        [ 0.0149,  0.0154,  0.1014,  0.1335],\n",
      "        [ 0.1223,  0.0410,  0.2655,  0.0417],\n",
      "        [ 0.0084,  0.0647,  0.1513,  0.0276],\n",
      "        [ 0.0050,  0.0421,  0.1712,  0.0858],\n",
      "        [-0.0040,  0.0461,  0.1884, -0.0372]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 1, 0, 1, 3, 1, 0, 1]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[0.1884],\n",
      "        [0.0647],\n",
      "        [0.1085],\n",
      "        [0.0154],\n",
      "        [0.0417],\n",
      "        [0.0647],\n",
      "        [0.0050],\n",
      "        [0.0461]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([0.1884, 0.0647, 0.1085, 0.0154, 0.0417, 0.0647, 0.0050, 0.0461],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[-0.0024,  0.0476,  0.1899, -0.0356],\n",
      "        [ 0.0164,  0.0166,  0.1025,  0.1347],\n",
      "        [-0.0024,  0.0476,  0.1899, -0.0356],\n",
      "        [ 0.0096,  0.0660,  0.1520,  0.0284],\n",
      "        [ 0.1100, -0.0101,  0.0620,  0.2055],\n",
      "        [-0.0024,  0.0476,  0.1899, -0.0356],\n",
      "        [-0.0024,  0.0476,  0.1899, -0.0356],\n",
      "        [ 0.0096,  0.0660,  0.1520,  0.0284]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 3, 2, 1, 0, 2, 1, 0]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[0.0476],\n",
      "        [0.1347],\n",
      "        [0.1899],\n",
      "        [0.0660],\n",
      "        [0.1100],\n",
      "        [0.1899],\n",
      "        [0.0476],\n",
      "        [0.0096]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([0.0476, 0.1347, 0.1899, 0.0660, 0.1100, 0.1899, 0.0476, 0.0096],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.1251,  0.0437,  0.2675,  0.0438],\n",
      "        [-0.0009,  0.0492,  0.1914, -0.0342],\n",
      "        [ 0.1251,  0.0437,  0.2675,  0.0438],\n",
      "        [ 0.0178,  0.0180,  0.1036,  0.1359],\n",
      "        [-0.0009,  0.0492,  0.1914, -0.0342],\n",
      "        [ 0.0178,  0.0180,  0.1036,  0.1359],\n",
      "        [-0.0009,  0.0492,  0.1914, -0.0342],\n",
      "        [ 0.0109,  0.0674,  0.1528,  0.0292]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 1, 1, 2, 2, 3, 3, 1]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1251],\n",
      "        [ 0.0492],\n",
      "        [ 0.0437],\n",
      "        [ 0.1036],\n",
      "        [ 0.1914],\n",
      "        [ 0.1359],\n",
      "        [-0.0342],\n",
      "        [ 0.0674]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1251,  0.0492,  0.0437,  0.1036,  0.1914,  0.1359, -0.0342,  0.0674],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.1079, -0.0154,  0.3364,  0.0558],\n",
      "        [ 0.0005,  0.0508,  0.1929, -0.0327],\n",
      "        [ 0.0122,  0.0688,  0.1535,  0.0301],\n",
      "        [ 0.0005,  0.0508,  0.1929, -0.0327],\n",
      "        [ 0.0193,  0.0192,  0.1047,  0.1372],\n",
      "        [ 0.0005,  0.0508,  0.1929, -0.0327],\n",
      "        [ 0.0193,  0.0192,  0.1047,  0.1372],\n",
      "        [ 0.0097,  0.0460,  0.1740,  0.0890]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 2, 1, 1, 3, 2, 1, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[0.0558],\n",
      "        [0.1929],\n",
      "        [0.0688],\n",
      "        [0.0508],\n",
      "        [0.1372],\n",
      "        [0.1929],\n",
      "        [0.0192],\n",
      "        [0.0097]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([0.0558, 0.1929, 0.0688, 0.0508, 0.1372, 0.1929, 0.0192, 0.0097],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0018,  0.0524,  0.1946, -0.0313],\n",
      "        [ 0.0111,  0.0473,  0.1750,  0.0900],\n",
      "        [ 0.0018,  0.0524,  0.1946, -0.0313],\n",
      "        [ 0.0018,  0.0524,  0.1946, -0.0313],\n",
      "        [ 0.0018,  0.0524,  0.1946, -0.0313],\n",
      "        [ 0.0018,  0.0524,  0.1946, -0.0313],\n",
      "        [ 0.0206,  0.0205,  0.1058,  0.1385],\n",
      "        [ 0.0134,  0.0702,  0.1543,  0.0310]], grad_fn=<ThAddmmBackward>) actions_v= tensor([1, 0, 2, 1, 2, 3, 0, 1]) actions_v.unsqueeze(-1)= tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0524],\n",
      "        [ 0.0111],\n",
      "        [ 0.1946],\n",
      "        [ 0.0524],\n",
      "        [ 0.1946],\n",
      "        [-0.0313],\n",
      "        [ 0.0206],\n",
      "        [ 0.0702]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0524,  0.0111,  0.1946,  0.0524,  0.1946, -0.0313,  0.0206,  0.0702],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0218,  0.0219,  0.1070,  0.1397],\n",
      "        [ 0.0029,  0.0541,  0.1964, -0.0299],\n",
      "        [ 0.1159, -0.0048,  0.0656,  0.2096],\n",
      "        [ 0.1295,  0.0478,  0.2705,  0.0470],\n",
      "        [ 0.0029,  0.0541,  0.1964, -0.0299],\n",
      "        [ 0.0124,  0.0487,  0.1761,  0.0910],\n",
      "        [ 0.0029,  0.0541,  0.1964, -0.0299],\n",
      "        [ 0.0029,  0.0541,  0.1964, -0.0299]], grad_fn=<ThAddmmBackward>) actions_v= tensor([2, 3, 1, 3, 0, 0, 2, 3]) actions_v.unsqueeze(-1)= tensor([[2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.1070],\n",
      "        [-0.0299],\n",
      "        [-0.0048],\n",
      "        [ 0.0470],\n",
      "        [ 0.0029],\n",
      "        [ 0.0124],\n",
      "        [ 0.1964],\n",
      "        [-0.0299]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.1070, -0.0299, -0.0048,  0.0470,  0.0029,  0.0124,  0.1964, -0.0299],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0156,  0.0734,  0.1560,  0.0327],\n",
      "        [ 0.0041,  0.0558,  0.1982, -0.0284],\n",
      "        [ 0.0041,  0.0558,  0.1982, -0.0284],\n",
      "        [ 0.0230,  0.0232,  0.1084,  0.1410],\n",
      "        [ 0.0156,  0.0734,  0.1560,  0.0327],\n",
      "        [ 0.0156,  0.0734,  0.1560,  0.0327],\n",
      "        [ 0.0156,  0.0734,  0.1560,  0.0327],\n",
      "        [ 0.1122, -0.0111,  0.3401,  0.0592]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 2, 0, 1, 1, 2, 1, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[0.0156],\n",
      "        [0.1982],\n",
      "        [0.0041],\n",
      "        [0.0232],\n",
      "        [0.0734],\n",
      "        [0.1560],\n",
      "        [0.0734],\n",
      "        [0.0592]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([0.0156, 0.1982, 0.0041, 0.0232, 0.0734, 0.1560, 0.0734, 0.0592],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0053,  0.0574,  0.2001, -0.0270],\n",
      "        [ 0.0053,  0.0574,  0.2001, -0.0270],\n",
      "        [ 0.0242,  0.0246,  0.1098,  0.1421],\n",
      "        [ 0.0053,  0.0574,  0.2001, -0.0270],\n",
      "        [ 0.0053,  0.0574,  0.2001, -0.0270],\n",
      "        [ 0.0053,  0.0574,  0.2001, -0.0270],\n",
      "        [ 0.0242,  0.0246,  0.1098,  0.1421],\n",
      "        [ 0.0053,  0.0574,  0.2001, -0.0270]], grad_fn=<ThAddmmBackward>) actions_v= tensor([0, 3, 2, 1, 3, 2, 1, 3]) actions_v.unsqueeze(-1)= tensor([[0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[ 0.0053],\n",
      "        [-0.0270],\n",
      "        [ 0.1098],\n",
      "        [ 0.0574],\n",
      "        [-0.0270],\n",
      "        [ 0.2001],\n",
      "        [ 0.0246],\n",
      "        [-0.0270]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([ 0.0053, -0.0270,  0.1098,  0.0574, -0.0270,  0.2001,  0.0246, -0.0270],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "states_v.shape= torch.Size([8, 16])  states_v= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "net(states_v) tensor([[ 0.0064,  0.0590,  0.2020, -0.0254],\n",
      "        [ 0.0252,  0.0258,  0.1115,  0.1433],\n",
      "        [ 0.0064,  0.0590,  0.2020, -0.0254],\n",
      "        [ 0.0175,  0.0767,  0.1585,  0.0344],\n",
      "        [ 0.0064,  0.0590,  0.2020, -0.0254],\n",
      "        [ 0.0164,  0.0530,  0.1799,  0.0940],\n",
      "        [ 0.0064,  0.0590,  0.2020, -0.0254],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 0.1196, -0.0001,  0.0693,  0.2126]], grad_fn=<ThAddmmBackward>) actions_v= tensor([3, 1, 0, 1, 2, 1, 3, 0]) actions_v.unsqueeze(-1)= tensor([[3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0]]) net(states_v).gather(1, actions_v.unsqueeze(-1))= tensor([[-0.0254],\n",
      "        [ 0.0258],\n",
      "        [ 0.0064],\n",
      "        [ 0.0767],\n",
      "        [ 0.2020],\n",
      "        [ 0.0530],\n",
      "        [-0.0254],\n",
      "        [ 0.1196]], grad_fn=<GatherBackward>) net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)= tensor([-0.0254,  0.0258,  0.0064,  0.0767,  0.2020,  0.0530, -0.0254,  0.1196],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-20053a87dc3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m#print('loss=',loss_t.abs())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "#DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "DEFAULT_ENV_NAME = \"FrozenLake-v0\"\n",
    "#DEFAULT_ENV_NAME = \"FrozenLake8x8-v0\"\n",
    "\n",
    "#MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99 #0.99\n",
    "BATCH_SIZE = 8 #32\n",
    "REPLAY_SIZE = 100 #10000\n",
    "LEARNING_RATE = 1e-4 #1e-4\n",
    "SYNC_TARGET_FRAMES = 10 #1000\n",
    "REPLAY_START_SIZE = 100 #10000\n",
    "HIDDEN_SIZE = 128 #Hidden layer size\n",
    "OUTPUT_SIZE = 16*4 #number of states multiply by number of actions per state\n",
    "\n",
    "\n",
    "#Epsilon control parameters - to control explore/exploite decision\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5 #10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "\n",
    "\n",
    "#Create a name tuppled object\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "#A class for replay buffer\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self\n",
    "\n",
    "    def sample(self, batch_size): #Method to append results from a mini batch of a given batch_size\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "#Ineriting class from Gym's ObservationWraper class\n",
    "#It converts the descrite inputs will have 16 float numbers, zero everywhere, except the currenl loction of the agent (as float 1)\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "#The Agent class\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer): #method to intialize the Agent\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer #Init the experience buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self): #method to reset the agent\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0): #A method to play a single step\n",
    "        done_reward = None\n",
    "\n",
    "        #make an explore/exploit decision\n",
    "        if np.random.random() < epsilon: #sampling from a unfied and check if result is below epsilon\n",
    "            #Explore - samling random action from the environment \n",
    "            action = env.action_space.sample() # sample a random action from the environment \n",
    "        else: #Exploit = pick an action by scoring the nn\n",
    "            #state_a = np.array([self.state], copy=False) #Convert the state to np array\n",
    "            #state_v = torch.tensor(state_a) # .to(device) \n",
    "            state_v = torch.tensor([self.state]) # .to(device) \n",
    "            #print(state_v)\n",
    "            q_vals_v = net(state_v) # sm(net(state_v))\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # Make a step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action) #perfrom a step on the agen's environment with the selected action\n",
    "        self.total_reward += reward #increment the total reward with the new reward received.\n",
    "        #new_state = new_state #Seems redundant...\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state) #Create a new experiance entry\n",
    "        self.exp_buffer.append(exp) #append the experiance entry\n",
    "        self.state = new_state \n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "\n",
    "#Calculate loos on a given mini-batch for net and tgt_net\n",
    "def calc_loss(batch, net, tgt_net):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    \n",
    "    #Create tensors for states, next_states, actions, rewards and done flags\n",
    "\n",
    "    states_v = torch.tensor(states) #.to(device)\n",
    "    print('states_v.shape=',states_v.shape,' states_v=', states_v)\n",
    "    \n",
    "    next_states_v = torch.tensor(next_states)#.to(device)\n",
    "#    print('next_states_v.shape=',next_states_v.shape,' next_states_v=', states_v)\n",
    "    actions_v = torch.tensor(actions) #.to(device)\n",
    "    rewards_v = torch.tensor(rewards) #.to(device)\n",
    "    done_mask = torch.ByteTensor(dones) #.to(device)\n",
    "    \n",
    "    #state_action_values = sm(net(states_v)).gather(1, actions_v.unsqueeze(-1)).squeeze(-1) # Applying the net on sates as input and then extract the Q-values based on the action taken\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1) # Applying the net on sates as input and then extract the Q-values based on the action taken\n",
    "    print('net(states_v)', net(states_v), 'actions_v=', actions_v, 'actions_v.unsqueeze(-1)=', actions_v.unsqueeze(-1),\n",
    "         'net(states_v).gather(1, actions_v.unsqueeze(-1))=', net(states_v).gather(1, actions_v.unsqueeze(-1)),\n",
    "         'net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)=', net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1))\n",
    "    \n",
    "    #next_state_values = sm(tgt_net(next_states_v)).max(1)[0] #apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1. Function max() returns both maximum values and indices of those values (so it calculates both max and argmax), which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result.\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0] #apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1. Function max() returns both maximum values and indices of those values (so it calculates both max and argmax), which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result.\n",
    "    next_state_values[done_mask] = 0.0 #Zero next state value in case current state is the last 1\n",
    "    next_state_values = next_state_values.detach() #detach the value from its computation graph to prevent gradients from flowing into the neural network\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v #Calculating the expected state action values based on Bellman equation \n",
    "\n",
    "    #loss = nn.MSELoss()(state_action_values, expected_state_action_values) #calculate the loss and return it\n",
    "    #loss = nn.CrossEntropyLoss()(state_action_values, expected_state_action_values) #calculate the loss and return it\n",
    "    loss = nn.KLDivLoss()(state_action_values, expected_state_action_values) #calculate the loss and return it\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Setup for GPU\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "#     parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
    "#                         help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "#     parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
    "#                         help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
    "#     args = parser.parse_args()\n",
    "#     device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#    env = wrappers.make_env(args.env)\n",
    "\n",
    "    env = DiscreteOneHotWrapper(gym.make(DEFAULT_ENV_NAME)) #creating a new environment \n",
    "    \n",
    "\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n  # 4 actions (up, down, left, right) in the case of FrozenLake\n",
    "\n",
    "    net = DQN(obs_size, HIDDEN_SIZE, n_actions) #.to(device)\n",
    "    tgt_net = DQN(obs_size, HIDDEN_SIZE, n_actions)#.to(device)\n",
    "#    net = DQN(obs_size, HIDDEN_SIZE, OUTPUT_SIZE) #.to(device)\n",
    "#    tgt_net = DQN(obs_size, HIDDEN_SIZE, OUTPUT_SIZE)#.to(device)\n",
    "    writer = SummaryWriter(comment=\"-\") #(comment=\"-\" + args.env)\n",
    "    print(net)\n",
    "\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer) #Creating a new instance of an agent\n",
    "    epsilon = EPSILON_START #Init the greed factor - higer value means more explor and less exploit\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    iter_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None\n",
    "    \n",
    "    sm = nn.Softmax(dim=1) #creating a softmax function \n",
    "\n",
    "    while True:\n",
    "        iter_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - iter_idx / EPSILON_DECAY_LAST_FRAME) #Update epslion (greed factor) by reducing it baed on the iter  \n",
    "\n",
    "        reward = agent.play_step(net, epsilon) #, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (iter_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = iter_idx\n",
    "            ts = time.time()\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            #print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "            #    iter_idx, len(total_rewards), mean_reward, epsilon, speed))\n",
    "            writer.add_scalar(\"epsilon\", epsilon, iter_idx)\n",
    "            writer.add_scalar(\"speed\", speed, iter_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, iter_idx)\n",
    "            writer.add_scalar(\"reward\", reward, iter_idx)\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), \"-best.dat\")\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f -> %.3f, speed=%3.f, epsilon=%.3f, model saved\" % (best_mean_reward, mean_reward, speed, epsilon))\n",
    "                best_mean_reward = mean_reward\n",
    "            if mean_reward > 0.8: # args.reward:\n",
    "                print(\"Solved in %d frames!\" % iter_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        if iter_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE) #sample from the experiance buffer \n",
    "        loss_t = calc_loss(batch, net, tgt_net) #, device=device)\n",
    "        #print('loss=',loss_t.abs())\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
