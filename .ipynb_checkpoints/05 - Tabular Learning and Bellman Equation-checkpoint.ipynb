{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Learning and the Bellman Equation\n",
    "\n",
    "In the previous chapter, we got acquainted with our first Reinforcement Learning (**RL**) method, **cross-entropy**, and saw its strengths and weaknesses. In this new part of the book, we'll look at another group of methods, called **Q-learning**, which have much more flexibility and power.\n",
    "This chapter will establish the required background shared by those methods. We'll also revisit the FrozenLake environment and show how new concepts will fit with this environment and help us to address the issues of the environment's uncertainty.\n",
    "\n",
    "## Main Terms\n",
    "\n",
    "\n",
    "### Belman Equation of Value - Determinsitc Case\n",
    "\n",
    "![](img/Ch5-Fig3.png)\n",
    "\n",
    "\n",
    "The Value of state $S_0$ is equal to the maximum of the sum of the possible rewards and the value of the next step over all possible actions. So, to choose the best possible action, the agent needs to calculate the resulting values for every action and choose the maximum possible outcome. This is a recursive claulation.\n",
    "\n",
    "$V_0 = \\max\\in_{1...N}(r_a + \\gamma V_a)$ **Belman Equation**\n",
    "\n",
    "Where:\n",
    "* $\\gamma$ : discount value\n",
    "* $r_a$: reward of action $a$\n",
    "* $V_a$: value of next state given action $a$\n",
    "\n",
    "### Belman Equation of Value - Stochastic Case\n",
    "\n",
    "![](img/Ch5-Fig4.png)\n",
    "\n",
    "$V_0 = \\max_{a \\in A}\\mathbb{E}_{s~S}(r_{s,a} + \\gamma V_s) = \\max_{a \\in A} \\sum_{s \\in S}p_{a,0 \\rightarrow s}(r_{s,a} + \\gamma V_s)$   \n",
    "\n",
    "Where:\n",
    "* $p_{a,0 \\rightarrow s}$: probability of action $a$, issued in state $i$, to end up in state $j$\n",
    "\n",
    "### Value of action\n",
    "\n",
    "Value of action is equals the total reward we can get by executing action $a$ in state $s$ and can be defined via $V_s$. This quantity gave a name to the whole family of methods called **\"Q-learning\"**, because it is slightly more convenient in practice. In these methods, our primary objective is to get values of $Q$ for every pair of state and action. \n",
    "The Q value is:\n",
    "\n",
    "$Q_{s,a} = \\mathbb{E}_{s'~S}(r_{s,a} + \\gamma V_{s'}) = \\sum_{s \\in S}p_{a,s \\rightarrow s'}(r_{s,a} + \\gamma V_{s'})$   \n",
    "\n",
    "$Q$ for this state $s$ and action a equals the expected immediate reward and the discounted long-term reward of the destination state $s'$. \n",
    "We also can define $V_s$ via $Q_{s,a}$:\n",
    "\n",
    "$V_s = \\max_{a \\in A}Q_{s,a}$\n",
    "\n",
    "This just means that the value of some state equals to the value of the maximum action we can execute from this state. It may look very close to the value of state, but there is still a difference, which is important to understand. Finally, we can express $Q(s, a)$ via itself, which will be used in the next chapter's topic of **Q-learning**:\n",
    "\n",
    "$Q(s, a) = r_{s,a} + \\gamma \\max_{a' \\in A}Q(s',a')$\n",
    "\n",
    "\n",
    "### Value Integration Algorithm\n",
    "\n",
    "The  **\"value iteration algorithm\"** allows us to numerically calculate the values of states and values of actions of MDPs with known transition probabilities and rewards. \n",
    "\n",
    "The procedure (for **values of states**) includes the following steps:\n",
    "1. Initialize values of all states Vi to some initial value (usually zero) \n",
    "2. For every state s in the MDP, perform the Bellman update:\n",
    "$$Vs \\leftarrow \\max_{a \\in A} \\sum_{s'}p_{a,s \\rightarrow s'}(r_{s,a} + \\gamma V_s')$$\n",
    "3. Repeat step 2 for some large number of steps or until changes become too small\n",
    "\n",
    "For f action values (**Q-Value**), only minor modifications to the preceding procedure are required:\n",
    "1. Initialize all Qs,a to zero \n",
    "2. For every state s and every action a in this state, perform update: \n",
    "$$Q_{s,a} \\leftarrow \\sum_{s'}p_{a,s \\rightarrow s'}(r_{s,a} + \\gamma \\max_{a'} Q_{s',a'})$$\n",
    "3. Repeat step 2\n",
    "\n",
    "### Value iteration in practice\n",
    "\n",
    "The central data structures in this example are as follows: \n",
    "* **Reward table**: A dictionary with the composite key \"source state\" + \"action\" + \"target state\". The value is obtained from the immediate reward.\n",
    "* **Transitions table**: A dictionary keeping counters of the experienced transitions. The key is the composite \"state\" + \"action\" and the value is another dictionary that maps the target state into a count of times that we've seen it. For example, if in state 0 we execute action 1 ten times, after three times it leads us to state 4 and after seven times to state 5. Entry with the key (0, 1) in this table will be a dict {4: 3, 5: 7}. We use this table to estimate the probabilities of our transitions.\n",
    "* **Value table**: A dictionary that maps a state into the calculated value of this state.\n",
    "\n",
    "The overall logic of our code is simple: in the loop, we play 100 random steps from the environment, populating the reward and transition tables. After those 100 steps, we perform a value iteration loop over all states, updating our value table. Then we play several full episodes to check our improvements using the updated value table. If the average reward for those test episodes is above the 0.8 boundary, then we stop training. During test episodes, we also update our reward and transition tables to use all data from the environment.\n",
    "\n",
    "First the Agent class code, which will keep our tables and contain functions we'll be using in the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self): #Init method for the class\n",
    "        self.env = gym.make(ENV_NAME) #Creating a Gym environment\n",
    "        self.state = self.env.reset() #Reseting the environment - state stores the current state\n",
    "        self.rewards = collections.defaultdict(float) #Creating a dictionary for rewards table\n",
    "        #Examplae {(0, 0, 0): 0.0, - key: (\"source state\",\"action\",\"target state\"), Value: immediate reward\n",
    "        #          (0, 3, 1): 0.0}\n",
    "        self.transits = collections.defaultdict(collections.Counter) #Creating a dictionary for tranists table\n",
    "        #{(0, 0): Counter({0: 1947, 4: 947}), Key: (\"state\", \"action\"), Value: dictionary that with key: next state, value: count of times that we've seen it\n",
    "        # (0, 3): Counter({1: 112, 0: 172})}\n",
    "        self.values = collections.defaultdict(float) #creating a dictionary for values\n",
    "        #{0: 0.06752900040833315, Key: State, Value - Clauclated value of the state\n",
    "        # 4: 0.1010535385685904,\n",
    "        # 1: 0.06666611422673593}\n",
    "        \n",
    "    #Method to play n random stpes - n = count    \n",
    "    #This function is used to gather random experience from the environment and update reward and transition tables\n",
    "    def play_n_random_steps(self, count): \n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample() #get a sample from the action space frmo the environment (the sample is conducted by the environment)\n",
    "            new_state, reward, is_done, _ = self.env.step(action)  #make a step with the given action\n",
    "            self.rewards[(self.state, action, new_state)] = reward #store the returned reward in the rewards dictionery\n",
    "            self.transits[(self.state, action)][new_state] += 1 #increment the counter in the transits table\n",
    "            self.state = self.env.reset() if is_done else new_state #reste the envirment if n steps were taken\n",
    "            #print('Curren state=', self.state)\n",
    "        self.value_iteration() #populate the the value table\n",
    "    \n",
    "    #method to claculate the action,value (Q-Value) for a given state and action\n",
    "    #function calculates the value of the action from the state, using our transition, reward and values tables\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)] #get all coutns for all next states\n",
    "        total = sum(target_counts.values()) #sum all the counts into tooal\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items(): #iterate over all next states \n",
    "            reward = self.rewards[(state, action, tgt_state)] #obtain the reward for a given sata, action and next state \n",
    "            #implementation of the Bellman Equation\n",
    "            prob = (count / total)\n",
    "            action_value +=  prob * (reward + GAMMA * self.values[tgt_state]) #incrmenting the action_value using Bellman equation = sum(prob*(r + gamma * Vs))\n",
    "        return action_value\n",
    "    \n",
    "    #method to select the best action  in a given state\n",
    "    #The decision is done in a greedy way (exploit only, no explore)\n",
    "    #This action selection process is deterministic, as the play_n_random_steps() function introduces enough exploration.\n",
    "    def select_action(self, state): \n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n): #iterate over all possible actions\n",
    "            action_value = self.calc_action_value(state, action) #calculate the action value (Q-Value)\n",
    "            if best_value is None or best_value < action_value: #calulate the best value\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "    #method to play a single episode \n",
    "    #Find the best action to take and plays one full episode using the provided environment\n",
    "    #It makes use of test env and not the main environment \n",
    "    def play_episode(self, env): \n",
    "        total_reward = 0.0 #reseting total rewards\n",
    "        state = env.reset() #reset the environment \n",
    "        while True:\n",
    "            action = self.select_action(state) #Call method to select the best action (against the policy) \n",
    "            new_state, reward, is_done, _ = env.step(action) #make a step on the enviroment \n",
    "            self.rewards[(state, action, new_state)] = reward #store the returned reward in the rewards table/dictionery \n",
    "            self.transits[(state, action)][new_state] += 1 #increment the counter in the transits table\n",
    "            total_reward += reward #increment total reward with the reward \n",
    "            #print('state=',new_state, ' reward=reward', ' total reward=', total_reward)\n",
    "            if is_done: #if episode is completed\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self): #method to alculate and poulate the values table\n",
    "        for state in range(self.env.observation_space.n): #iterate over all states\n",
    "            state_values = [self.calc_action_value(state, action) #obtain the action value\n",
    "                            for action in range(self.env.action_space.n)] #for each action\n",
    "            self.values[state] = max(state_values) #store the max of state_values as the State's value\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FrozenLake8x8-v0'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lines in the preceding code snippet are the key piece in the training loop. First, we perform 100 random steps to fill our reward and transition tables with fresh data and then we run value iteration over all states. The rest of the code plays test episodes **using the value table as our policy**, then writes data into TensorBoard, tracks the best average reward, and checks for the training loop stop condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "Th is thte main loop to call the intial exploration (play_n_random_steps()) and then playnumer of test train episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.200\n",
      "Best reward updated 0.200 -> 0.250\n",
      "Best reward updated 0.250 -> 0.350\n",
      "Best reward updated 0.350 -> 0.400\n",
      "Best reward updated 0.400 -> 0.700\n",
      "Best reward updated 0.700 -> 0.750\n",
      "Best reward updated 0.750 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 45 iterations!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GAMMA = 0.95 #Reward discount rate\n",
    "TRAIN_EPISODES = 20 #mumber of test episodes\n",
    "EXPLORE_STEPS = 100 #number of intial environment exlpore steps\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #Intializing env, agent and writer\n",
    "    test_env = gym.make(ENV_NAME) #crating an instanse of the environment for testing (training?)\n",
    "    agent = Agent() #creating an object instance of the agent\n",
    "    writer = SummaryWriter(comment=\"-v-iteration\") #cratring an tesnorflow object\n",
    "\n",
    "    iter_no = 0 #iteration counter\n",
    "    best_reward = 0.0\n",
    "    #               Training loop\n",
    "    #===============================\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(EXPLORE_STEPS) #Play random 100 steps \n",
    "        #agent.value_iteration() #populate the the value table\n",
    " \n",
    "        reward = 0.0\n",
    "        for _ in range(TRAIN_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TRAIN_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the policy\n",
    "Using the trained policy above, we testing it on new epizods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward= 0.627\n"
     ]
    }
   ],
   "source": [
    "TEST_EPISODES = 1000\n",
    "env = gym.make(ENV_NAME) #Create a new environment\n",
    "total_reward = 0\n",
    "for i in range(TEST_EPISODES):\n",
    "    rewards = agent.play_episode(env)\n",
    "    #print('Episode: ',i,' Reward=', reward)\n",
    "    total_reward += rewards\n",
    "print('Average reward=', total_reward/TEST_EPISODES)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning for FrozenLake \n",
    "The difference relatievly to ``Value Iteration`` above is really minor. The most obvious change is to the value table. In the previous example, we kept the value of the state, so the key in the dictionary was just a state. Now we need to store values of the Q-function, which has two parameters: state and action, so the key in the value table is now a composite.\n",
    "\n",
    "The second difference is in the calc_action_value function. We just don't need it anymore, as our action values are stored in the value table. Finally, the most important change in the code is in the agent's value_iteration method. Before, it was just a wrapper around the calc_action_value call, which did the job of Bellman approximation. Now, as this function has gone and was replaced by a value table, we need to do this approximation in the value_iteration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.100\n",
      "Best reward updated 0.100 -> 0.200\n",
      "Best reward updated 0.200 -> 0.300\n",
      "Best reward updated 0.300 -> 0.400\n",
      "Best reward updated 0.400 -> 0.600\n",
      "Best reward updated 0.600 -> 0.800\n",
      "Best reward updated 0.800 -> 0.900\n",
      "Solved in 24 iterations!  test episodes =  240  total steps = 5758  average steps per episode = 23.991666666666667\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 10\n",
    "EXPLORE_STEPS = 100 #number of intial environment exlpore steps\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME) #Creating a Gym environment\n",
    "        self.state = self.env.reset() #Reseting the environment - state stores the current state\n",
    "        self.rewards = collections.defaultdict(float) #Creating a dictionary for rewards table\n",
    "        #Examplae {(0, 0, 0): 0.0, - key: (\"source state\",\"action\",\"target state\"), Value: immediate reward\n",
    "        #          (0, 3, 1): 0.0}\n",
    "        self.transits = collections.defaultdict(collections.Counter) #Creating a dictionary for tranists table\n",
    "        #Transites table example:\n",
    "        #{(0, 0): Counter({0: 1947, 4: 947}), Key: (\"state\", \"action\"), Value: dictionary that with key: next state, value: count of times that we've seen it\n",
    "        # (0, 3): Counter({1: 112, 0: 172})}\n",
    "        self.values = collections.defaultdict(float)\n",
    "        #VAlue table example\n",
    "        # {(4, 0): 0.09062526696040417, Key: (State, action), Value: Caluclated value of the state, value pairs\n",
    "        #  (4, 1): 0.06728916471587301,\n",
    "        #  (4, 2): 0.06721133905626454,\n",
    "\n",
    "    #Method to play n random stpes - n = count    \n",
    "    #This function is used to gather random experience from the environment and update reward and transition tables\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample() #get a sample from the action space frmo the environment (the sample is conducted by the environment)\n",
    "            new_state, reward, is_done, _ = self.env.step(action) #make a step with the given action\n",
    "            self.rewards[(self.state, action, new_state)] = reward #store the returned reward in the rewards dictionery\n",
    "            self.transits[(self.state, action)][new_state] += 1 #increment the counter in the transits table\n",
    "            self.state = self.env.reset() if is_done else new_state  #reset the envirment if n steps were taken\n",
    "            \n",
    "    #method to select the best action  in a given state\n",
    "    #The decision is done in a greedy way (exploit only, no explore)\n",
    "    #This action selection process is deterministic, as the play_n_random_steps() function introduces enough exploration.             \n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n): #iterate over all possible actions\n",
    "            action_value = self.values[(state, action)] #Get the action value (Q-Value)\n",
    "            if best_value is None or best_value < action_value:  #calulate the best value\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "    #method to play a single episode \n",
    "    #Find the best action to take and plays one full episode using the provided environment\n",
    "    #It makes use of test env and not the main environment \n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0  #reseting total rewards\n",
    "        step_count = 0\n",
    "        state = env.reset() #reset the environment \n",
    "        while True:\n",
    "            action = self.select_action(state) #Call method to select the best action (against the policy) \n",
    "            new_state, reward, is_done, _ = env.step(action) #make a step on the enviroment \n",
    "            self.rewards[(state, action, new_state)] = reward #store the returned reward in the rewards table/dictionery \n",
    "            self.transits[(state, action)][new_state] += 1 #increment the counter in the transits table\n",
    "            total_reward += reward #increment total reward with the new reward \n",
    "            step_count += 1\n",
    "            if is_done: #if episode is completed\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward, step_count\n",
    "\n",
    "    #method to alculate and poulate the values table\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n): #iterate over all states\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0 #reset action value pair\n",
    "                target_counts = self.transits[(state, action)] #get all current counts of all state action paris from the tranists table\n",
    "                total = sum(target_counts.values()) #summ all current counts to get total\n",
    "                for tgt_state, count in target_counts.items(): #iterate over all target states and their correspending counts\n",
    "                    reward = self.rewards[(state, action, tgt_state)]  #get the stored reward of the state, action, tgt_state from the rewards table \n",
    "                    best_action = self.select_action(tgt_state) #get the best action for the target state \n",
    "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)]) #use Bellman equation to calculate the new action value\n",
    "                self.values[(state, action)] = action_value #store the new calculated action value in the value table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-iteration\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    step_count = 0\n",
    "    episode_count = 0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(EXPLORE_STEPS) #play random steps\n",
    "        agent.value_iteration() #update the vaue table - not sure why this is an exposed method\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            this_episode_reward, steps = agent.play_episode(test_env) #play a test episode - however this method update the reward and the transit tables\n",
    "            reward += this_episode_reward\n",
    "            step_count += steps\n",
    "            \n",
    "        reward /= TEST_EPISODES\n",
    "        episode_count += TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no, ' test episodes = ', episode_count, ' total steps =', step_count, ' average steps per episode =', step_count/episode_count )\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward= 0.738  total steps = 41874  average steps per episode = 41.874\n"
     ]
    }
   ],
   "source": [
    "TEST_EPISODES = 1000\n",
    "env = gym.make(ENV_NAME) #Create a new environment\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "for i in range(TEST_EPISODES):\n",
    "    rewards, steps = agent.play_episode(env)\n",
    "    #print('Episode: ',i,' Reward=', reward)\n",
    "    total_reward += rewards\n",
    "    step_count += steps\n",
    "print('Average reward=', total_reward/TEST_EPISODES, ' total steps =', step_count, ' average steps per episode =', step_count/TEST_EPISODES)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
