{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "\n",
    "In this chapter, we'll try to apply the same theory to problems of much greater complexity: arcade games from the Atari 2600 platform, which are the de-facto benchmark of the RL research community. To deal with this new and more challenging goal, we'll talk about problems with the Value iteration method and introduce its variation, called Q-learning. In particular, we'll look at the application of Q-learning to so-called \"grid world\" environments, which is called tabular Q-learning, and then we'll discuss Q-learning in conjunction with neural networks. This combination has the name DQN. At the end of the chapter, we'll reimplement a DQN algorithm from the famous paper, Playing Atari with Deep Reinforcement Learning by V. Mnih and others, published in 2013, which started a new era in RL development.\n",
    "\n",
    "## Real-life value iteration \n",
    "The improvements we got in the FrozenLake environment by switching from Cross-Entropy to the Value iteration method are quite encouraging, so it's tempting to apply the value iteration method to more challenging problems. However, let's first look at the assumptions and limitations that our Value iteration method has.\n",
    "We will start with a quick recap of the method. The Value iteration method on every step does a loop on all states, and for every state, it performs an update of its value with a Bellman approximation. The variation of the same method for Q-values (values for actions) is almost the same, but we approximate and store values for every state and action. So, what's wrong with this process?\n",
    "The first obvious problem is the count of environment states and our ability to iterate over them. In the Value iteration, we assume that we know all states in our environment in advance, can iterate over them and can store value approximation associated with the state. It's definitely true for the simple \"grid world\" environment of FrozenLake, but what about other tasks? First, let's try to understand how scalable the Value iteration approach is, or, in other words, how many states we can easily iterate over in every loop. Even a moderate-sized computer can keep several billion float values in memory (8.5 billion in 32 GB of RAM), so the memory required for value tables doesn't look like a huge constraint. Iteration over billions of states and actions will be more memory intensive, but not an insurmountable problem.\n",
    "Nowadays, we have multicore systems that are mostly idle. The real problem is the number of samples required to get good approximations for state transition dynamics. Imagine that you have some environment with, say, a billion states (this corresponds approximately to a FrozenLake of size 31600 × 31600). To calculate even a rough approximation for every state of this environment, we'll need hundreds of billions of transitions evenly distributed over our states, which is not practical.\n",
    "To give you an example of an environment with a much larger number of potential states, let's consider the Atari 2600 game console again. This was very popular in the 1980s and many arcade-style games were available for it. The Atari console is archaic by today's gaming standards, but its games give an excellent set of RL problems that humans can master fairly quickly, but still are challenging for computers. Not surprisingly, this platform (using an emulator, of course) is a very popular benchmark among RL researches.\n",
    "Let's calculate the state space for the Atari platform. The resolution of the screen is 210 x 160 pixels, and every pixel has one of 128 colors. So, every frame of the screen has 210 × 160 = 33600 pixels and the total amount of different screens possible is $128^{33600}$ which is slightly more than $10^{70802}$. If we decide to just enumerate all possible states\n",
    "of Atari once, it will take billions of billions of years even for the fastest supercomputer. Also, 99(.9)% of this job will be a waste of time, as most of the combinations will never be shown during even long gameplay, so we'll never have samples of those states. However, the value iteration method wants to iterate over them just in case.\n",
    "Another problem with the value iteration approach is that it limits us to discrete action spaces. Indeed, both Q(s, a) and V(s) approximations assume that our actions are a mutually exclusive discrete set, which is not true for continuous control problems where actions can represent continuous variables, such as the angle of a steering wheel, the force on an actuator, or the temperature of a heater. This issue is much more challenging than the first, and we'll talk about it in the last part of the book, in chapters dedicated to continuous action space problems. For now, let's assume that we have a discrete count of actions and this count is not very large (orders of tens). How should we handle the state space size issue?\n",
    "\n",
    "# Tabular Q-learning\n",
    "First of all, do we really need to iterate over every state in the state space? We have an environment that can be used as a source of real-life samples of states. If some state in the state space is not shown to us by the environment, why should we care about its value? We can use states obtained from the environment to update values of states, which can save us lots of work.\n",
    "This modification of the Value iteration method is known as Q-learning, as mentioned earlier, and for cases with explicit state-to-value mappings, has the following steps:\n",
    "\n",
    "1. Start with an empty table, mapping states to values of actions.\n",
    "2. By interacting with the environment, obtain the **tuple** $s, a, r, s′$ ``(state, action, reward, and the new state)``. In this step, we need to decide which action to take, and there is no single proper way to make this decision. We discussed this problem as **exploration versus exploitation** and will talk a lot about this.\n",
    "3. Update the $Q(s, a)$ value using the Bellman approximation: $Q(s,a) \\leftarrow r + \\gamma \\max_{a' \\in A} Q_{s',a'}$\n",
    "4. Repeat from step 2.\n",
    "\n",
    "As in Value iteration, the end condition could be some threshold of the update or we can perform test episodes to estimate the expected reward from the policy. Another thing to note here is how to update the Q-values. As we take samples from the environment, it's generally a bad idea to just assign new values on top of existing values, as training can become unstable. What is usually done in practice is to update the $Q(s, a)$ with approximations using a **\"blending\"** technique, which is just averaging between old and new values of $Q$ using learning rate $\\alpha$ with a value from 0 to 1:\n",
    "\n",
    "$Q(s,a) \\leftarrow (1 - \\alpha) Q_{s,a} + \\alpha (r + \\gamma \\max_{a' \\in A} Q_{s',a'})$\n",
    "\n",
    "This allows values of $Q$ to converge smoothly, even if our environment is noisy.\n",
    "The final version of the algorithm is here:\n",
    "\n",
    "1. Start with an empty table for $Q(s,a)$\n",
    "2. Obtain the **tuple** ($s, a, r, s′)$ ``(state, action, reward, and the new state)`` from the environment. \n",
    "3. Make a Bellman update:\n",
    "$Q(s,a) \\leftarrow (1 - \\alpha) Q_{s,a} + \\alpha (r + \\gamma \\max_{a' \\in A} Q_{s',a'})$\n",
    "4. Check convergence conditions. If not met, repeat from step 2.\n",
    "\n",
    "As mentioned earlier, this method is called **tabular Q-learning**, as we keep a **table of states with their Q-values**. Let's try it on our FrozenLake environment. The whole example code is in Chapter06/01_frozenlake_q_learning.py.\n",
    "You may have noticed that this version used more iterations to solve the problem compared to the value iteration method from the previous chapter. The reason for that is that we're **no longer using the experience obtained during testing**. (In Chapter05/02_frozenlake_q_iteration.py, periodical tests cause an update of Q-table statistics. **Here we don't touch Q-values during the test**, which cause more iterations before the environment gets solved.) Overall, the total amount of samples required from the environment is almost the same. The reward chart in TensorBoard also shows good training dynamics, which are very similar to the value iteration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050 env. steps per iteration =  337\n",
      "Best reward updated 0.050 -> 0.100 env. steps per iteration =  259\n",
      "Best reward updated 0.100 -> 0.150 env. steps per iteration =  280\n",
      "Best reward updated 0.150 -> 0.200 env. steps per iteration =  242\n",
      "Best reward updated 0.200 -> 0.250 env. steps per iteration =  266\n",
      "Best reward updated 0.250 -> 0.300 env. steps per iteration =  622\n",
      "Best reward updated 0.300 -> 0.350 env. steps per iteration =  419\n",
      "Best reward updated 0.350 -> 0.450 env. steps per iteration =  504\n",
      "Best reward updated 0.450 -> 0.500 env. steps per iteration =  763\n",
      "Best reward updated 0.500 -> 0.550 env. steps per iteration =  398\n",
      "Best reward updated 0.550 -> 0.650 env. steps per iteration =  732\n",
      "Best reward updated 0.650 -> 0.700 env. steps per iteration =  775\n",
      "Best reward updated 0.700 -> 0.750 env. steps per iteration =  605\n",
      "Best reward updated 0.750 -> 0.800 env. steps per iteration =  975\n",
      "Best reward updated 0.800 -> 0.850 env. steps per iteration =  791\n",
      "Solved in 7367 iterations!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import collections\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"\n",
    "GAMMA = 0.9 #Discount factor\n",
    "ALPHA = 0.2 #Learning rate\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    #Initialize the agent\n",
    "    def __init__(self): \n",
    "        self.env = gym.make(ENV_NAME) #reate a new envrionment \n",
    "        self.state = self.env.reset() #reset the env.\n",
    "        self.values = collections.defaultdict(float) #initiate an empty values table\n",
    "\n",
    "    #methos to execute a single step sample of the envrionment\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample() #Obtain a single sampled action from the environment \n",
    "        old_state = self.state #safe the prev. state\n",
    "        new_state, reward, is_done, _ = self.env.step(action) #Perform the sampled action on the env\n",
    "        self.state = self.env.reset() if is_done else new_state #Store the new obtained state or reste if done\n",
    "        return (old_state, action, reward, new_state) #return a tuple (s,a,r,s')\n",
    "\n",
    "    #Method to select the best action in a given state\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n): #iterate over all possible actions of the environment \n",
    "            action_value = self.values[(state, action)] #Get the current action value (Q-Value) from the values table\n",
    "            if best_value is None or best_value < action_value: #calculate the best value and he best action\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action #return the best value and the best action\n",
    "    \n",
    "    #Method to update the value table \n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_val = r + GAMMA * best_v #caluclate the new value with Bellman equation\n",
    "        old_val = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA #Update the value with wights based on the learning rate\n",
    "\n",
    "    #Method to play a sinlge episode\n",
    "    #We don't update the tables while runing the episode\n",
    "    def play_episode(self, env):\n",
    "        state = env.reset() #Create a new env. (not using the environment used to upadate the value table)\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state) #get the best action for the current state\n",
    "            new_state, reward, is_done, _ = env.step(action) #mae a step in the env with the best action\n",
    "            total_reward += reward #increment total reward with the new reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state #keep the new state s the new state\n",
    "            steps += 1\n",
    "        return total_reward, steps\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent() #Create a new agent object\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        s, a, r, next_s = agent.sample_env() #samle the environmet\n",
    "        agent.value_update(s, a, r, next_s) #update the value table\n",
    "\n",
    "        total_reward = 0.0\n",
    "        total_steps = 0\n",
    "        for _ in range(TEST_EPISODES): #Test on the neviroment using ghe new tables\n",
    "            reward, steps = agent.play_episode(test_env)\n",
    "            total_reward += reward\n",
    "            total_steps += steps\n",
    "        total_reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if total_reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, total_reward), 'env. steps per iteration = ', total_steps)\n",
    "            best_reward = total_reward\n",
    "        if total_reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward= 0.585  Av. steps per episode =  45.015\n"
     ]
    }
   ],
   "source": [
    "TEST_EPISODES = 1000\n",
    "env = gym.make(ENV_NAME) #Create a new environment\n",
    "total_reward = 0\n",
    "total_steps = 0\n",
    "for i in range(TEST_EPISODES):\n",
    "    rewards, steps = agent.play_episode(env)\n",
    "    #print('Episode: ',i,' Reward=', reward)\n",
    "    total_reward += rewards\n",
    "    total_steps += steps\n",
    "print('Average reward=', total_reward/TEST_EPISODES, ' Av. steps per episode = ', total_steps/TEST_EPISODES)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning\n",
    "\n",
    "For enviroment with continues states, we can use **a nonlinear representation that maps both state and action onto a value**. In machine learning this is called a **\"regression problem\"**. The concrete way to represent and train such a representation can vary, but, as you may have already guessed from this section's title, using a deep neural network is one of the most popular options, especially when dealing with observations represented as screen images. With this in mind, let's make modifications to the Q-learning algorithm:\n",
    "We do a modifcation to the Tabluar Q-Learninng Algorithem above to accomudate very large state,action space and to use DNN:\n",
    "\n",
    "1. Initialize $Q(s, a)$ with some initial approximation \n",
    "2. By interacting with the environment, obtain the tuple $(s, a, r, s′)$ \n",
    "3. Calculate loss: $L =(Q_{s,a} − r)^2$ if episode has ended or $L =(Q_{s,a} − (r + \\gamma \\max_{a' \\in A} Q_{s',a'}))^2$ otherwise\n",
    "4. Update $Q(s, a)$ using the stochastic gradient descent (SGD) algorithm, by minimizing the loss with respect to the model parameters\n",
    "5. Repeat from step 2 until converged\n",
    "\n",
    "### Interaction with the environment\n",
    "\n",
    "First of all, we need to interact with the environment somehow to receive data to train on. In simple environments, such as FrozenLake, we can act randomly, but is this the best strategy to use? Imagine the game of Pong. What's the probability of winning a single point by randomly moving the paddle? It's not zero but it's extremely small, which just means that we'll need to wait for a very long time for such a rare situation. As an alternative, we can use our Q function approximation as a source of behavior (as we did before in the value iteration method, when we remembered our experience during testing).\n",
    "\n",
    "If our representation of Q is good, then the experience that we get from the environment will show the agent relevant data to train on. However, we're in trouble when our approximation is not perfect (at the beginning of the training, for example). In such a case, our agent can be stuck with bad actions for some states without ever trying to behave differently. This **exploration versus exploitation dilemma** was mentioned briefly in Chapter 1, What is Reinforcement Learning?. On the one hand, our agent needs to explore the environment to build a complete picture of transitions and action outcomes. On the other hand, we should use interaction with the environment efficiently: we shouldn't waste time by randomly trying actions we've already tried and have learned their outcomes. As you can see, random behavior is better at the beginning of the training when our Q approximation is bad, as it gives us more uniformly distributed information about the environment states. As our training progresses, random behavior becomes inefficient and we want to fall back to our Q approximation to decide how to act.\n",
    "\n",
    "A method which performs such a mix of two extreme behaviors is known as an **epsilon-greedy method**, which just means switching between random and $Q$ policy using the probability hyperparameter $\\epsilon$. By varying $\\epsilon$ we can select the ratio of random actions. The usual practice is to start with $\\epsilon = 1.0$ (100% random actions) and slowly decrease it to some small value such as 5% or 2% of random actions. Using an epsilon-greedy method helps both to explore the environment in the beginning and to stick to good policy at the end of the training. There are other solutions to the **\"exploration versus exploitation\"** problem, and we'll discuss some of them in part three of the book. This problem is **one of the fundamental open questions in RL and an active area of research, which is not even close to being resolved completely**.\n",
    "\n",
    "### SGD optimization\n",
    "The core of our Q-learning procedure is borrowed from the supervised learning. Indeed, we are trying to approximate a complex, nonlinear function $Q(s, a)$ with a neural network. To do this, we calculate targets for this function using the Bellman equation and then pretend that we have a supervised learning problem at hand. That's okay, but one of the fundamental requirements for SGD optimization is that the training data is **independent and identically distributed (i.i.d)**.\n",
    "\n",
    "In our case, data that we're going to use for the SGD update doesn't fulfill these criteria:\n",
    "1. Our samples are not independent. Even if we accumulate a large batch of data samples, they all will be very close to each other, as they belong to the same episode.\n",
    "2. Distribution of our training data won't be identical to samples provided by the optimal policy that we want to learn. Data that we have is a result of some other policy (our current policy, random, or both in the case of $\\epsilon$-greedy), but we don't want to learn how to play randomly: we want an optimal policy with the best reward.\n",
    "\n",
    "To deal with this nuisance, we usually need to use a **large buffer of our past experience and sample training data from it, instead of using our latest experience**. This method is called **replay buffer**. The simplest implementation is a buffer of **fixed size**, with new data added to the end of the buffer so that it pushes the oldest experience out of it. \n",
    "\n",
    "**Replay buffer** allows us to train on more-or-less independent data, but data will still be fresh enough to train on samples generated by our recent policy.\n",
    "\n",
    "### Correlation between steps\n",
    "Another practical issue with the default training procedure is also related to the lack of\n",
    "i.i.d in our data, but in a slightly different manner. The Bellman equation provides us with the value of $Q(s, a)$ via $Q(s′, a′)$ (which has the name of **bootstrapping**). However, both states $s$ and $s′$ have only one step between them. This makes them very similar and it's really hard for neural networks to distinguish between them. When we perform an update of our network's parameters, to make $Q(s, a)$ closer to the desired result, we indirectly can alter the value produced for $Q(s′, a′)$ and other states nearby. This can make our training really unstable, like chasing our own tail: when we update $Q$ for state $s$, then on subsequent states we discover that $Q(s′, a′)$ becomes worse, but attempts to update it can spoil our $Q(s, a)$ approximation, and so on.\n",
    "\n",
    "To make training more stable, there is a **trick, called target network**, when we keep a copy of our network and use it for the $Q(s′, a′)$ value in the Bellman equation. This network is synchronized with our main network only periodically, for example, once in $N$ steps (where $N$ is usually quite a large hyperparameter, such as 1k or 10k training iterations).\n",
    "\n",
    "### The Markov property\n",
    "Our RL methods use MDP formalism as their basis, which **assumes that the environment obeys the Markov property**: observation from the environment is all that we need to act optimally (in other words, our observations allow us to distinguish states from one another). As we've seen on the preceding Pong's screenshot, one single image from the Atari game is not enough to capture all important information (using only one image we have no idea about the speed and direction of objects, like the ball and our opponent's paddle). This obviously violates the Markov property and moves our single-frame Pong environment into the area of **partially observable MDPs (POMDP)**. A POMDP is basically MDP without the Markov property and they are very important in practice. For example, for most card games where you don't see your opponents' cards, game observations are POMDPs, because current observation (your cards and cards on the table) could correspond to different cards in your opponents' hands.\n",
    "We'll not discuss POMPDs in detail in this book, so, for now, we'll use a small technique to push our environment back into the MDP domain. **The solution is maintaining several observations from the past and using them as a state**. In the case of Atari games, we usually stack $k$ subsequent frames together and use them as the observation at every state. This **allows our agent to deduct the dynamics of the current state**, for instance, to get the speed of the ball and its direction. The usual \"classical\" **number of $k$ for Atari is four**. Of course, it's just a hack, as there can be longer dependencies in the environment, but for most of the games it works well.\n",
    "\n",
    "## The final form of DQN (Deep Q-Learning) training\n",
    "There are many more tips and tricks that researchers have discovered to make **DQN** training more stable and efficient, and we'll cover the best of them in the next chapter. However, **$\\epsilon$-greedy**, **replay buffer**, and **target network** form the basis that allows DeepMind to successfully train a DQN on a set of 49 Atari games and demonstrate the efficiency of this approach applied to complicated environments.\n",
    "\n",
    "The original paper (without target network) was published at the end of 2013 ([Playing Atari with Deep Reinforcement Learning 1312.5602v1, Mnih and others.)](https://arxiv.org/pdf/1312.5602.pdf), and they used seven games for testing. Later, at the beginning of 2015, a revised version of the article, with 49 different games, was published in Nature ([Human-Level Control Through Deep Reinforcement Learning doi:10.1038/nature14236, Mnih and others.](https://www.nature.com/articles/nature14236))\n",
    "\n",
    "The algorithm for DQN from the preceding papers has the following steps:\n",
    "1. Initialize parameters for $Q(s, a)$ and $\\hat Q (s, a)$ with random weights, **$\\epsilon\\leftarrow 1.0$**, and **empty replay buffer**\n",
    "2. With probability $\\epsilon$, select a random action $a$, otherwise $a = \\arg \\max_a Q_{s,a}$\n",
    "3. Execute action $a$ in an **emulator** and observe reward $r$ and the next state $s′$ \n",
    "4. Store transition $(s, a, r, s′)$ in the **replay buffer**\n",
    "5. Sample a random **minibatch** of transitions from the **replay buffer**\n",
    "6. For every transition in the buffer, calculate target $y = r$ if the episode has ended at this step or $y = r + \\gamma \\max_{a' \\in A} \\hat Q_{s',a'}$ otherwise\n",
    "7. Calculate loss: $L = (Q_{s,a} − y)^2$\n",
    "8. Update $Q(s, a)$ using the **SGD algorithm** by minimizing the loss in respect to model parameters\n",
    "9. Every $N$ steps copy weights from $Q$ to $\\hat Q_t$\n",
    "10. Repeat from step 2 until converged\n",
    "\n",
    "\n",
    "## DQN on FrozenLake\n",
    "### DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#Class for setting the Deep Q-Learning Network\n",
    "#For the FrozenLake game the w use as simple fully connected single hidden layer dnn\n",
    "class DQN(nn.Module):\n",
    "    #Method to intialize the DNN\n",
    "    def __init__(self, obs_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    #method to perform a single forward operation\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN  Training on FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "total_iter=1101, delta_iteration=1101, games=139, mean reward=0.010, eps=0.69\n",
      "total_iter=4487, delta_iteration=3386, games=436, mean reward=0.020, eps=0.66\n",
      "total_iter=4856, delta_iteration=369, games=53, mean reward=0.030, eps=0.65\n",
      "total_iter=12640, delta_iteration=7784, games=924, mean reward=0.040, eps=0.57\n",
      "total_iter=12806, delta_iteration=166, games=15, mean reward=0.050, eps=0.57\n",
      "total_iter=14321, delta_iteration=1515, games=119, mean reward=0.060, eps=0.56\n",
      "total_iter=14598, delta_iteration=277, games=19, mean reward=0.070, eps=0.55\n",
      "total_iter=14863, delta_iteration=265, games=25, mean reward=0.080, eps=0.55\n",
      "total_iter=18601, delta_iteration=3738, games=331, mean reward=0.090, eps=0.51\n",
      "total_iter=18635, delta_iteration=34, games=3, mean reward=0.100, eps=0.51\n",
      "total_iter=31280, delta_iteration=12645, games=934, mean reward=0.110, eps=0.39\n",
      "total_iter=31296, delta_iteration=16, games=3, mean reward=0.120, eps=0.39\n",
      "total_iter=31681, delta_iteration=385, games=26, mean reward=0.130, eps=0.38\n",
      "total_iter=31689, delta_iteration=8, games=1, mean reward=0.140, eps=0.38\n",
      "total_iter=31702, delta_iteration=13, games=1, mean reward=0.150, eps=0.38\n",
      "total_iter=32092, delta_iteration=390, games=30, mean reward=0.160, eps=0.38\n",
      "total_iter=37616, delta_iteration=5524, games=350, mean reward=0.170, eps=0.32\n",
      "total_iter=39878, delta_iteration=2262, games=135, mean reward=0.180, eps=0.30\n",
      "total_iter=40308, delta_iteration=430, games=16, mean reward=0.190, eps=0.30\n",
      "total_iter=47509, delta_iteration=7201, games=382, mean reward=0.200, eps=0.22\n",
      "total_iter=48491, delta_iteration=982, games=50, mean reward=0.210, eps=0.22\n",
      "total_iter=49032, delta_iteration=541, games=26, mean reward=0.220, eps=0.21\n",
      "total_iter=49046, delta_iteration=14, games=1, mean reward=0.230, eps=0.21\n",
      "total_iter=49373, delta_iteration=327, games=10, mean reward=0.240, eps=0.21\n",
      "total_iter=49379, delta_iteration=6, games=1, mean reward=0.250, eps=0.21\n",
      "total_iter=49494, delta_iteration=115, games=8, mean reward=0.260, eps=0.21\n",
      "total_iter=49507, delta_iteration=13, games=1, mean reward=0.270, eps=0.20\n",
      "total_iter=49842, delta_iteration=335, games=15, mean reward=0.280, eps=0.20\n",
      "total_iter=49871, delta_iteration=29, games=1, mean reward=0.290, eps=0.20\n",
      "total_iter=49908, delta_iteration=37, games=1, mean reward=0.300, eps=0.20\n",
      "total_iter=49976, delta_iteration=68, games=4, mean reward=0.310, eps=0.20\n",
      "total_iter=51149, delta_iteration=1173, games=49, mean reward=0.320, eps=0.19\n",
      "total_iter=51159, delta_iteration=10, games=1, mean reward=0.330, eps=0.19\n",
      "total_iter=51286, delta_iteration=127, games=2, mean reward=0.340, eps=0.19\n",
      "total_iter=51313, delta_iteration=27, games=1, mean reward=0.350, eps=0.19\n",
      "total_iter=54860, delta_iteration=3547, games=126, mean reward=0.360, eps=0.15\n",
      "total_iter=55359, delta_iteration=499, games=18, mean reward=0.370, eps=0.15\n",
      "total_iter=55462, delta_iteration=103, games=4, mean reward=0.380, eps=0.15\n",
      "total_iter=55515, delta_iteration=53, games=3, mean reward=0.390, eps=0.14\n",
      "total_iter=60932, delta_iteration=5417, games=194, mean reward=0.400, eps=0.09\n",
      "total_iter=61070, delta_iteration=138, games=3, mean reward=0.410, eps=0.09\n",
      "total_iter=61288, delta_iteration=218, games=6, mean reward=0.420, eps=0.09\n",
      "total_iter=61475, delta_iteration=187, games=4, mean reward=0.430, eps=0.09\n",
      "total_iter=61829, delta_iteration=354, games=13, mean reward=0.440, eps=0.08\n",
      "total_iter=61972, delta_iteration=143, games=6, mean reward=0.450, eps=0.08\n",
      "total_iter=61985, delta_iteration=13, games=1, mean reward=0.460, eps=0.08\n",
      "total_iter=62039, delta_iteration=54, games=2, mean reward=0.470, eps=0.08\n",
      "total_iter=62083, delta_iteration=44, games=1, mean reward=0.480, eps=0.08\n",
      "total_iter=66706, delta_iteration=4623, games=127, mean reward=0.490, eps=0.03\n",
      "total_iter=66724, delta_iteration=18, games=1, mean reward=0.500, eps=0.03\n",
      "total_iter=66735, delta_iteration=11, games=1, mean reward=0.510, eps=0.03\n",
      "total_iter=66833, delta_iteration=98, games=6, mean reward=0.520, eps=0.03\n",
      "total_iter=66902, delta_iteration=69, games=1, mean reward=0.530, eps=0.03\n",
      "total_iter=66933, delta_iteration=31, games=1, mean reward=0.540, eps=0.03\n",
      "total_iter=67012, delta_iteration=79, games=4, mean reward=0.550, eps=0.03\n",
      "total_iter=67056, delta_iteration=44, games=1, mean reward=0.560, eps=0.03\n",
      "total_iter=67105, delta_iteration=49, games=1, mean reward=0.570, eps=0.03\n",
      "total_iter=67111, delta_iteration=6, games=1, mean reward=0.580, eps=0.03\n",
      "total_iter=67123, delta_iteration=12, games=1, mean reward=0.590, eps=0.03\n",
      "total_iter=67172, delta_iteration=49, games=2, mean reward=0.600, eps=0.03\n",
      "total_iter=67319, delta_iteration=147, games=2, mean reward=0.610, eps=0.03\n",
      "total_iter=67569, delta_iteration=250, games=6, mean reward=0.620, eps=0.02\n",
      "total_iter=67626, delta_iteration=57, games=1, mean reward=0.630, eps=0.02\n",
      "total_iter=67720, delta_iteration=94, games=2, mean reward=0.640, eps=0.02\n",
      "total_iter=67772, delta_iteration=52, games=1, mean reward=0.650, eps=0.02\n",
      "total_iter=67830, delta_iteration=58, games=2, mean reward=0.660, eps=0.02\n",
      "total_iter=67853, delta_iteration=23, games=1, mean reward=0.670, eps=0.02\n",
      "total_iter=67873, delta_iteration=20, games=1, mean reward=0.680, eps=0.02\n",
      "total_iter=67919, delta_iteration=46, games=1, mean reward=0.690, eps=0.02\n",
      "total_iter=67990, delta_iteration=71, games=2, mean reward=0.700, eps=0.02\n",
      "total_iter=68707, delta_iteration=717, games=16, mean reward=0.710, eps=0.01\n",
      "total_iter=68772, delta_iteration=65, games=1, mean reward=0.720, eps=0.01\n",
      "total_iter=68882, delta_iteration=110, games=3, mean reward=0.730, eps=0.01\n",
      "total_iter=69376, delta_iteration=494, games=9, mean reward=0.740, eps=0.01\n",
      "total_iter=69442, delta_iteration=66, games=4, mean reward=0.750, eps=0.01\n",
      "total_iter=69471, delta_iteration=29, games=1, mean reward=0.760, eps=0.01\n",
      "total_iter=69552, delta_iteration=81, games=1, mean reward=0.770, eps=0.01\n",
      "total_iter=69778, delta_iteration=226, games=9, mean reward=0.780, eps=0.01\n",
      "total_iter=69828, delta_iteration=50, games=1, mean reward=0.790, eps=0.01\n",
      "total_iter=69959, delta_iteration=131, games=3, mean reward=0.800, eps=0.01\n",
      "total_iter=70093, delta_iteration=134, games=4, mean reward=0.810, eps=0.01\n",
      "Solved in 70093 frames!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "#DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "DEFAULT_ENV_NAME = \"FrozenLake-v0\"\n",
    "#DEFAULT_ENV_NAME = \"FrozenLake8x8-v0\"\n",
    "\n",
    "#MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99 #0.99 #reward discount\n",
    "BATCH_SIZE = 32 #32\n",
    "REPLAY_SIZE = 10000 #10000\n",
    "LEARNING_RATE = 1e-4 #1e-4\n",
    "SYNC_TARGET_FRAMES = 1000 #1000\n",
    "REPLAY_START_SIZE = 10000 #10000\n",
    "HIDDEN_SIZE = 128 #Hidden layer size\n",
    "\n",
    "#Epsilon control parameters - to control explore/exploite decision\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5 #10**5\n",
    "EPSILON_START = .7\n",
    "EPSILON_FINAL = 0.01\n",
    "\n",
    "\n",
    "#Create a name tuppled object\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "#A class for replay buffer\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self\n",
    "\n",
    "    def sample(self, batch_size): #Method to append results from a mini batch of a given batch_size\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "#Ineriting class from Gym's ObservationWraper class\n",
    "#It converts the descrite inputs will have 16 float numbers, zero everywhere, except the currenl loction of the agent (as float 1)\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "#The Agent class\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer): #method to intialize the Agent\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer #Init the experience buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self): #method to reset the agent\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def best_action():\n",
    "            #state_a = np.array([self.state], copy=False) #Convert the state to np array\n",
    "            #state_v = torch.tensor(state_a) # .to(device) \n",
    "            state_v = torch.tensor([self.state]) # .to(device) \n",
    "            #print(state_v)\n",
    "            #get the best action for the current state\n",
    "            q_vals_v = net(state_v) # sm(net(state_v))\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "            return action\n",
    "\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0): #A method to play a single step\n",
    "        done_reward = None\n",
    "\n",
    "        #make an explore/exploit decision\n",
    "        if np.random.random() < epsilon: #Explor: sampling from a unfied and check if result is below epsilon\n",
    "            action = env.action_space.sample() # sample a random action from the environment \n",
    "        else: #Exploit: pick an action by scoring the nn\n",
    "            action = self.best_action()\n",
    "\n",
    "\n",
    "        # Make a step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action) #perfrom a step on the agent's environment with the selected action\n",
    "        self.total_reward += reward #increment the total reward with the new reward received.\n",
    "        #new_state = new_state #Seems redundant...\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state) #Create a new experiance entry\n",
    "        self.exp_buffer.append(exp) #append the experiance entry\n",
    "        self.state = new_state \n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "    \n",
    "    def play_episode(self, env, net):\n",
    "        state = env.reset() #Create a new env. (not using the environment used to upadate the value table)\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            _, action = self.best_action() #get the best action for the current state\n",
    "            new_state, reward, is_done, _ = env.step(action) #mae a step in the env with the best action\n",
    "            total_reward += reward #increment total reward with the new reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state #keep the new state s the new state\n",
    "            steps += 1\n",
    "        return total_reward, steps\n",
    "\n",
    "\n",
    "#Calculate loos on a given mini-batch for net and tgt_net\n",
    "def calc_loss(batch, net, tgt_net):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    \n",
    "    #Create tensors for states, next_states, actions, rewards and done flags\n",
    "    states_v = torch.tensor(states) #.to(device)\n",
    "    #print('states_v.shape=',states_v.shape,' states_v=', states_v)\n",
    "    \n",
    "    next_states_v = torch.tensor(next_states)#.to(device)\n",
    "    #print('next_states_v.shape=',next_states_v.shape,' next_states_v=', next_states_v)\n",
    "    actions_v = torch.tensor(actions) #.to(device)\n",
    "    rewards_v = torch.tensor(rewards) #.to(device)\n",
    "    done_mask = torch.ByteTensor(dones) #.to(device)\n",
    "\n",
    "    #state_action_values = sm(net(states_v)).gather(1, actions_v.unsqueeze(-1)).squeeze(-1) # Applying the net on sates as input and then extract the Q-values based on the action taken\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1) # Applying the net on sates as input and then extract the Q-values based on the action taken\n",
    "    #print('net(states_v)', net(states_v), 'actions_v=', actions_v, 'actions_v.unsqueeze(-1)=', actions_v.unsqueeze(-1),\n",
    "    #      'net(states_v).gather(1, actions_v.unsqueeze(-1))=', net(states_v).gather(1, actions_v.unsqueeze(-1)),\n",
    "    #      'net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)=', net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1))\n",
    "    \n",
    "    #next_state_values = sm(tgt_net(next_states_v)).max(1)[0] #apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1. Function max() returns both maximum values and indices of those values (so it calculates both max and argmax), which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result.\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0] #apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1. Function max() returns both maximum values and indices of those values (so it calculates both max and argmax), which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result.\n",
    "    next_state_values[done_mask] = 0.0 #Zero next state value in case current state is the last 1\n",
    "    next_state_values = next_state_values.detach() #detach the value from its computation graph to prevent gradients from flowing into the neural network\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v #Calculating the expected state action values based on Bellman equation \n",
    "    #print('next_state_values:', next_state_values, 'expected_state_action_values:', expected_state_action_values)\n",
    "    \n",
    "    \n",
    "    loss = nn.MSELoss()(state_action_values, expected_state_action_values) #calculate the loss and return it\n",
    "    #print('loss:', loss)\n",
    "    #print('====================================================================')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Setup for GPU\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "#     parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
    "#                         help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "#     parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
    "#                         help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
    "#     args = parser.parse_args()\n",
    "#     device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#    env = wrappers.make_env(args.env)\n",
    "\n",
    "    env = DiscreteOneHotWrapper(gym.make(DEFAULT_ENV_NAME)) #creating a new environment \n",
    "    \n",
    "\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n  # 4 actions (up, down, left, right) in the case of FrozenLake\n",
    "\n",
    "    net = DQN(obs_size, HIDDEN_SIZE, n_actions) #.to(device)\n",
    "    tgt_net = DQN(obs_size, HIDDEN_SIZE, n_actions)#.to(device)\n",
    "#    net = DQN(obs_size, HIDDEN_SIZE, OUTPUT_SIZE) #.to(device)\n",
    "#    tgt_net = DQN(obs_size, HIDDEN_SIZE, OUTPUT_SIZE)#.to(device)\n",
    "    writer = SummaryWriter(comment=\"-\") #(comment=\"-\" + args.env)\n",
    "    print(net)\n",
    "\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer) #Creating a new instance of an agent\n",
    "    epsilon = EPSILON_START #Init the greed factor - higer value means more explor and less exploit\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    iter_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None\n",
    "    \n",
    "    sm = nn.Softmax(dim=1) #creating a softmax function \n",
    "    prev_idx = 0\n",
    "    prev_games = 0\n",
    "\n",
    "    while True:\n",
    "        iter_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - iter_idx / EPSILON_DECAY_LAST_FRAME) #Update epslion (greed factor) by reducing it baed on the iter  \n",
    "\n",
    "        reward = agent.play_step(net, epsilon) #, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (iter_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = iter_idx\n",
    "            ts = time.time()\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "#             print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "#                 iter_idx, len(total_rewards), mean_reward, epsilon, speed))\n",
    "            writer.add_scalar(\"epsilon\", epsilon, iter_idx)\n",
    "            writer.add_scalar(\"speed\", speed, iter_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, iter_idx)\n",
    "            writer.add_scalar(\"reward\", reward, iter_idx)\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), \"-best.dat\") #Save (serialize the model parameters)\n",
    "                3\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"total_iter=%d, delta_iteration=%d, games=%d, mean reward=%.3f, eps=%.2f\" \n",
    "                          % (iter_idx, iter_idx - prev_idx, len(total_rewards) - prev_games, mean_reward, epsilon))\n",
    "                    agent.explore_cnt = 0\n",
    "                    agent.exploite_cnt = 0\n",
    "                    prev_idx = iter_idx\n",
    "                    prev_games = len(total_rewards)\n",
    "                    tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "                best_mean_reward = mean_reward\n",
    "            if mean_reward > 0.8: # args.reward:\n",
    "                print(\"Solved in %d frames!\" % iter_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        if iter_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict()) #load the model parameters to the target net\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE) #sample from the experiance buffer \n",
    "        loss_t = calc_loss(batch, net, tgt_net) #, device=device)\n",
    "        #print('loss=',loss_t.abs())\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "This chart shows the change in Epsilon and mean rewards through the iterations.\n",
    "![](img/fig6-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('net.0.weight',\n",
       "              tensor([[-0.0760, -0.2173,  0.2194,  ..., -0.2344, -0.0173,  0.0881],\n",
       "                      [-0.2444,  0.0782,  0.1192,  ...,  0.2748, -0.1500,  0.2343],\n",
       "                      [ 0.1910, -0.1036,  0.1478,  ...,  0.1170, -0.1721,  0.0808],\n",
       "                      ...,\n",
       "                      [ 0.1321, -0.0216,  0.0821,  ..., -0.1359,  0.1232, -0.0817],\n",
       "                      [-0.0653,  0.2222,  0.1947,  ..., -0.2278,  0.1962,  0.1978],\n",
       "                      [-0.1780,  0.0648, -0.2420,  ..., -0.1714, -0.0635, -0.1386]])),\n",
       "             ('net.0.bias',\n",
       "              tensor([-0.2203,  0.1481,  0.0939,  0.1047,  0.0848,  0.0566,  0.1941, -0.1399,\n",
       "                       0.0489, -0.2296,  0.1513,  0.1908,  0.0456, -0.0679, -0.1231,  0.1601,\n",
       "                      -0.0535, -0.1717, -0.2272,  0.0862, -0.2288,  0.1366,  0.1227,  0.2410,\n",
       "                       0.1662, -0.0878,  0.2100, -0.2190, -0.1651, -0.1694,  0.0623, -0.2102,\n",
       "                      -0.0369,  0.1186,  0.0658,  0.0773, -0.1361, -0.1084,  0.0526,  0.1518,\n",
       "                      -0.1263, -0.1336, -0.2440,  0.1189,  0.2406, -0.1869,  0.1768,  0.0900,\n",
       "                      -0.1854,  0.0224,  0.1497,  0.0846, -0.2208,  0.1455,  0.1966,  0.0613,\n",
       "                       0.0647, -0.1358, -0.0978, -0.1481,  0.1970, -0.2211, -0.1810,  0.0376,\n",
       "                       0.1202,  0.1720, -0.2061,  0.1567, -0.2355,  0.1142,  0.3082,  0.1563,\n",
       "                      -0.1872,  0.1058, -0.1643, -0.0778, -0.2475, -0.2422,  0.1729,  0.1876,\n",
       "                       0.2330,  0.0935,  0.1565, -0.0088,  0.2240, -0.0907, -0.2001,  0.0323,\n",
       "                       0.1180,  0.1371, -0.0061, -0.2384,  0.0304,  0.1957,  0.2267,  0.1132,\n",
       "                      -0.2077, -0.1307, -0.2129, -0.1129,  0.0802,  0.2000,  0.0615,  0.1118,\n",
       "                      -0.0846,  0.1293,  0.0517,  0.1893,  0.1183, -0.2222,  0.0618, -0.2108,\n",
       "                       0.2388, -0.2431, -0.2349, -0.1960, -0.1287, -0.1994, -0.0625,  0.1914,\n",
       "                      -0.2333,  0.0967, -0.0203,  0.0547,  0.0286, -0.1993,  0.2590,  0.1414])),\n",
       "             ('net.2.weight',\n",
       "              tensor([[-0.0256, -0.0959,  0.0618,  0.0938,  0.0840,  0.0998, -0.0014,  0.0744,\n",
       "                        0.0728,  0.0276, -0.0037,  0.0549,  0.0444, -0.0223,  0.0095,  0.0521,\n",
       "                       -0.0204, -0.0275,  0.0549,  0.0968,  0.0743,  0.0409,  0.1464, -0.0178,\n",
       "                        0.0107,  0.0998, -0.0078,  0.0006, -0.0610,  0.0407,  0.1114, -0.0672,\n",
       "                        0.0953, -0.0223,  0.0168,  0.0342, -0.0503,  0.0493,  0.0648,  0.1291,\n",
       "                        0.1442, -0.0124, -0.0266,  0.1465, -0.0114, -0.0289,  0.0990, -0.0765,\n",
       "                       -0.0447,  0.0471,  0.0352,  0.0062, -0.0339, -0.0227,  0.0503, -0.0179,\n",
       "                        0.0596,  0.0532, -0.0635,  0.0307,  0.1115,  0.0258, -0.0463, -0.0113,\n",
       "                        0.0257,  0.0079,  0.0638,  0.0653,  0.0189, -0.0332,  0.0244,  0.0142,\n",
       "                        0.0132,  0.1095,  0.0113,  0.0106, -0.0348,  0.0427,  0.0405, -0.0040,\n",
       "                        0.0968, -0.0117, -0.0172,  0.1267,  0.0634,  0.0458, -0.0118, -0.0587,\n",
       "                        0.0177,  0.0192, -0.1125, -0.0011,  0.1426,  0.0828,  0.0313,  0.0716,\n",
       "                       -0.0346,  0.0076,  0.0179, -0.0021, -0.0058,  0.0467, -0.0800,  0.0594,\n",
       "                        0.1046,  0.0691, -0.0085,  0.0954, -0.0295,  0.0397, -0.0161,  0.0356,\n",
       "                        0.1179,  0.0223,  0.0402, -0.0100,  0.0147, -0.0703, -0.0787,  0.1340,\n",
       "                        0.0800,  0.0552,  0.0741,  0.1008, -0.0004, -0.0816,  0.0599, -0.0528],\n",
       "                      [ 0.0275, -0.0964,  0.0944,  0.1091, -0.0810, -0.0329, -0.0992, -0.0199,\n",
       "                        0.1468,  0.0671, -0.0270,  0.0681, -0.0323,  0.0263,  0.0417,  0.0013,\n",
       "                        0.0528, -0.0729,  0.0212,  0.0747,  0.0377,  0.0365,  0.1026,  0.0644,\n",
       "                        0.0361,  0.0112, -0.0396,  0.0487,  0.0013,  0.0370,  0.1045, -0.0629,\n",
       "                        0.0684,  0.0974,  0.0350,  0.0176, -0.0932,  0.0796, -0.0450,  0.0407,\n",
       "                        0.1966,  0.1218, -0.0783,  0.0514,  0.0660, -0.0038,  0.0830, -0.0713,\n",
       "                       -0.0011, -0.0047,  0.1088,  0.0010,  0.0446,  0.0083,  0.1179,  0.0186,\n",
       "                        0.0369, -0.0198,  0.0073,  0.0168, -0.0017,  0.0512, -0.0212,  0.1815,\n",
       "                        0.1210, -0.0369, -0.0587, -0.0746,  0.0157, -0.0064,  0.1094,  0.1402,\n",
       "                       -0.0604, -0.0236,  0.0177,  0.0221, -0.0612, -0.0834,  0.0257,  0.0602,\n",
       "                        0.0755,  0.0016,  0.0328,  0.0579, -0.0324,  0.0145, -0.0237,  0.0945,\n",
       "                       -0.0255,  0.0022, -0.0786,  0.0872,  0.1703,  0.0671,  0.0869,  0.0403,\n",
       "                        0.0767, -0.0422,  0.0007,  0.1006,  0.0458,  0.0864, -0.0896,  0.1598,\n",
       "                        0.1355,  0.0440,  0.0361,  0.0670,  0.0213,  0.0500,  0.0669,  0.0091,\n",
       "                        0.0101,  0.0238,  0.0721, -0.0488, -0.0582,  0.0614,  0.0069, -0.0510,\n",
       "                        0.0595,  0.0345,  0.0346,  0.0953,  0.0010,  0.0083,  0.0603,  0.0259],\n",
       "                      [ 0.0818,  0.0979,  0.0912,  0.0422,  0.0052,  0.0605, -0.0422, -0.0580,\n",
       "                        0.0907, -0.0105, -0.0248,  0.0701, -0.0403,  0.0153,  0.1528,  0.0038,\n",
       "                       -0.0579,  0.0397,  0.0186, -0.0006,  0.0742, -0.0792, -0.0048,  0.0551,\n",
       "                        0.1086,  0.0781, -0.0089, -0.0454, -0.0211,  0.0295,  0.1176,  0.0409,\n",
       "                        0.0519,  0.0470, -0.0743, -0.0726, -0.0609,  0.0365, -0.0549,  0.0739,\n",
       "                        0.0752,  0.0267,  0.0058,  0.0297,  0.0643, -0.0634, -0.0497, -0.0769,\n",
       "                        0.0071,  0.0626,  0.1313, -0.0451, -0.0507, -0.0031,  0.0764, -0.0934,\n",
       "                       -0.0088, -0.0971,  0.0958,  0.0101,  0.0193, -0.0278, -0.0611,  0.0558,\n",
       "                        0.1293, -0.0353, -0.0092, -0.0771,  0.0448,  0.0175,  0.1228,  0.0399,\n",
       "                       -0.0007, -0.0129,  0.0882, -0.0552,  0.0788,  0.0019, -0.0304, -0.0006,\n",
       "                        0.0814,  0.0249,  0.0488,  0.1245,  0.0133,  0.0365,  0.0847, -0.0727,\n",
       "                       -0.0633,  0.0901, -0.0861, -0.0108,  0.0452,  0.0340,  0.0359,  0.0481,\n",
       "                        0.0599, -0.0699,  0.0221, -0.0784, -0.0007,  0.0009, -0.0055,  0.1073,\n",
       "                        0.1530,  0.0767,  0.0176,  0.0959,  0.0425, -0.0757,  0.0004, -0.0076,\n",
       "                        0.0968, -0.0133,  0.0245, -0.0444, -0.0050, -0.0622,  0.0746,  0.0945,\n",
       "                        0.0539, -0.0156,  0.0617,  0.0386, -0.0165, -0.0617,  0.0891, -0.0702],\n",
       "                      [ 0.0888, -0.0241, -0.0602, -0.0214, -0.0388,  0.0613, -0.0806, -0.0452,\n",
       "                       -0.0160, -0.0149,  0.0809,  0.1324, -0.0130,  0.1129, -0.0005,  0.1168,\n",
       "                        0.0810,  0.0474, -0.0126, -0.0661, -0.0811, -0.0000, -0.0008,  0.1076,\n",
       "                        0.0315,  0.0340,  0.0847, -0.0582,  0.0271,  0.0450,  0.0055,  0.0293,\n",
       "                        0.0513,  0.0157, -0.0326, -0.0953,  0.0316,  0.0402, -0.0609, -0.0545,\n",
       "                        0.0728, -0.0588, -0.0849,  0.0674,  0.0496,  0.0374,  0.0843, -0.0426,\n",
       "                        0.0331, -0.0087,  0.1215,  0.0963,  0.0433,  0.0626,  0.0451, -0.0314,\n",
       "                        0.0224,  0.0064,  0.0026, -0.1081,  0.0204,  0.0302,  0.0077,  0.0375,\n",
       "                        0.1018, -0.0155,  0.0175, -0.0227, -0.0428,  0.0383,  0.0515,  0.1134,\n",
       "                        0.0058,  0.0680, -0.0112, -0.0286,  0.0515, -0.0671,  0.1510,  0.0291,\n",
       "                        0.0310,  0.0286,  0.0616,  0.1286, -0.0196,  0.0169, -0.0166, -0.0418,\n",
       "                        0.0240, -0.0240, -0.0267, -0.0439,  0.0878, -0.0209, -0.0556, -0.0024,\n",
       "                       -0.0312, -0.0675,  0.0794,  0.0333, -0.0683, -0.0694,  0.0679,  0.0729,\n",
       "                        0.0990,  0.0250, -0.0792,  0.0676,  0.0910, -0.0495, -0.0161,  0.0509,\n",
       "                        0.1234, -0.0196, -0.0569,  0.0263,  0.0179,  0.0092, -0.1026,  0.0280,\n",
       "                        0.0514,  0.0261, -0.0146,  0.1289, -0.0699, -0.0654, -0.0075, -0.0106]])),\n",
       "             ('net.2.bias', tensor([-0.0144, -0.0155,  0.0908,  0.1053]))])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10^5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
