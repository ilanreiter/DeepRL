{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Externtions\n",
    "\n",
    "Since 2015, many improvements have been proposed to DQN algorithem, along with tweaks to the basic architecture, which significantly improve convergence, stability and sample efficiency of the basic DQN invented by DeepMind. In this chapter, we'll take a deeper look at some of those ideas. Very conveniently, in October 2017, DeepMind published a paper called Rainbow: Combining Improvements in Deep Reinforcement Learning ([1](https://arxiv.org/abs/1710.02298) Hessel and others, 2017), which presented the seven most important improvements to DQN, some of which were invented in 2015, but some of which are very recent. In this paper, state-of-the-art results on the Atari Games suite were reached, just by combining all those seven methods together.\n",
    "The DQN extensions we'll become familiar with are as follows:\n",
    "*  **N-steps DQN:** How to improve convergence speed and stability with a simple unrolling of the Bellman equation and why it's not an ultimate solution\n",
    "* **Double DQN:** How to deal with DQN overestimation of the values of actions\n",
    "* **Noisy networks:** How to make exploration more efficient by adding noise to the network weights\n",
    "* **Prioritized replay buffer:** Why uniform sampling of our experience is not the best way to train\n",
    "* **Dueling DQN:** How to improve convergence speed by making our network's architecture closer represent the problem we're solving\n",
    "* **Categorical DQN:** How to go beyond the single expected value of action and work with full distributions\n",
    "\n",
    "## The PyTorch Agent Net library\n",
    "\n",
    "The implementaiton of this chapter is based on a PTAN library: \n",
    "To be able to focus only on the significant parts, it would be useful to have as small and concise version of a DQN as possible, preferably with reusable code pieces. This will be extremely helpful when you're experimenting with some methods published in papers or your own ideas. In that case, you don't need to reimplement the same functionality again and again, fighting with the inevitable bugs.\n",
    "With this in mind, some time ago I started to implement my own toolkit for the deep RL domain. I called it PTAN, which stands for PyTorch Agent Net, as it was inspired by another open-source library called AgentNet (https://github.com/ yandexdataschool/AgentNet). The basic design principles I tried to follow in PTAN are as follows:\n",
    "* Being as simple and clean as possible \n",
    "* PyTorch-nativeness \n",
    "* Containing small, reusable pieces of functionality \n",
    "* Extensibility and flexibility\n",
    "The library is available in GitHub: https://github.com/Shmuma/ptan. All the subsequent examples were implemented using version 0.3 of PTAN, which can be installed in your virtual environment by running the following:\n",
    "\n",
    "pip install ptan==0.3 \n",
    "\n",
    "Let's look at the basic building blocks that PTAN provides.\n",
    "\n",
    "### Agent\n",
    "\n",
    "The agent entity provides a unified way of **bridging observations from the environment and the actions** that we want to execute. So far, we've seen only a simple, stateless DQN agent that uses a neural net to obtain actions' values from the current observation and behaves greedily on those values. We've used epsilon-greedy behavior to explore the environment, but this doesn't change the picture much.\n",
    "In the RL field, this could be more complicated. For example, instead of predicting the values of the actions, our agent can predict probability distribution over actions. Such agents are called policy agents and we'll talk about those methods in part three of the book. The other requirement could be some kind of memory in the agent. For example, very often one observation (or even k last observation) is not enough to make a decision about the action and we want to keep some memory in the agent to capture the necessary information. There is a whole subdomain of RL which tries to address this complication with **Partially-Observable Markov Decision Process (POMDP)** formalism. We'll briefly touch on this case in the last part of the book.\n",
    "To capture all those variants and make the code flexible, the agent in the PTAN is implemented as an extensible hierarchy of classes with the ptan.agent.BaseAgent abstract class at the top. From the high level, the agent needs to accept the batch of observation (in the form of a NumPy array) and return the batch of actions that the agent wants to take. The batch is used to make the processing more efficient, as processing several observations in one pass in GPU is frequently much faster than processing them individually. The abstract base class doesn't define the type of input and output, which makes it very flexible and easy to extend. For example, in the continuous domain, our actions won't any longer be indices of discrete actions, but float values.\n",
    "The agent that corresponds to our current DQN requirements is `ptan.agent.DQNAgent`, which uses the provided PyTorch `nn.Module` to convert a batch of observations into action values. To convert the network's output into actual actions to be taken, the DQNAgent class needs the second object to be passed on creation: action selector.\n",
    "The purpose of action selector is to convert the output of the network (usually it's a vector of numbers) into some action. In a discrete action space case, the action will be one or several action indices to be taken. There are two action selectors in the PTAN that we'll need: `ptan.actions.ArgmaxActionSelector` and `ptan.actions. EpsilonGreedyActionSelector`. As you may guess from the names, the first one (ArgmaxActionSelector) applies **argmax** to the provided values, which corresponds to greedy actions over Q-values.\n",
    "\n",
    "The second action selector supports **epsilon-greedy** behavior, by having **epsilon** as a parameter and with this probability taking the random action instead of the greedy selection. To combine all this together, to create the agent for **CartPole**, with epsilongreedy action selection, we can write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1]), [None])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym \n",
    "import ptan\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(env.observation_space.shape[0], 256), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, env.action_space.n) \n",
    ") \n",
    "\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1) \n",
    "agent = ptan.agent.DQNAgent(net, action_selector)\n",
    "\n",
    "obs = np.array([env.reset()], dtype=np.float32)\n",
    "agent(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in:\n",
    "(array([1]), [None])  \n",
    "Which is a tuple where the The first itemis a batch of actions to take, while the second value is related to stateful agents and should be ignored.\n",
    "During the run, we can change the epsilon attribute in our action selector to change the random action probability during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent's experience\n",
    "The second important abstraction in PTAN is the so-called experience source. \n",
    "In our DQN example in the previous chapter, we worked with one-step experience pieces, which include four things:\n",
    "* The **observed state** of the environment at some time step: $s_t$ \n",
    "* The **action** the agent has taken: $a_t$\n",
    "* The **reward** the agent has obtained: $r_t$ \n",
    "* The observation of the **next state**: $s_{t+1}$\n",
    "\n",
    "We used those values ($s_t$ , $a_t$ , $r_t$ , $s_{t+1}$) to update our $Q$ approximation using the **Bellman equation**. However, for a general case, we can be interested in longer chains of experience, including more time steps of the agent's interaction with the environment.\n",
    "Bellman's equation also could be unrolled to longer experience chains.\n",
    "\n",
    "$$Q(s_t , a_t) = \\mathbb{E} [r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} +  ... + \\gamma^k \\max_a Q(s_{t+k}, a)]$$\n",
    "\n",
    "One of the methods to improve DQN stability and convergence, discussed in this chapter, does just this: by unrolling the Bellman's equation to k steps forward (when k is usually 2...5), we significantly improve the speed of our training convergence.\n",
    "To support this situation in a generic way, in PTAN we have the `ptan.experience.ExperienceSourceFirstLast` class, which takes the environment and the agent and provides to us the stream of experience tuples:\n",
    "\n",
    "($s_t$ , $a_t$ , $r_t$ , $s_{t+k}$), where $R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} +  ... + \\gamma^{k-1}r_{t+k-1}$\n",
    "\n",
    "When $k=1, R_t = r_t$\n",
    "\n",
    "This class automatically handles end-of-episode situations, letting us know about them by setting the last tuple entry to None. In such cases, a reset of the environment is performed automatically. Class ExperienceSourceFirstLast exposes the iterator interface, generating on every iteration the tuple with experience. The example of this class is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
