{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trading Using RL\n",
    "Using DQN to make buy/sell/hold decision for a given stock.\n",
    "\n",
    "In this chapter, we’ll implement our own OpenAI Gym environment, which simulates the stock market, and apply the DQN method that we’ve just learned in Chapters 6, Deep Q-Networks, and Chapter 7, DQN Extensions, to train the agent that will trade stocks to maximize the profit.\n",
    "\n",
    "## Data\n",
    "In our example, we’ll use the Russian stock market prices for the period of\n",
    "2015-2016, which is placed in Chapter08/data/ch08-small-quotes.tgz and has to be unpacked before model training.\n",
    "Inside the archive, we have CSV files with M1 bars, which means that every row in the CSV corresponds to a single minute in time and price movement during this minute is captured with four prices: open, high, low, and close. Here, an open price is the price at the beginning of the minute, high is the maximum price during the interval, low is the minimum price, and the close price is the last price of the minute time interval. Every minute interval is called bar and allows us to have an idea of price movement within the interval. For example, in the YNDX_160101_161231.csv file (which is Yandex company stocks for 2016), we have 130k lines of this form:\n",
    "\n",
    "| DATE  |  TIME |  OPEN |  HIGH |    LOW |  CLOSE |  VOL |\n",
    "| ------ | ------- | ------- | ------- | -------- | -------- | ------ |\n",
    "| 20160104 | 100100 | 1148.9000000 | 1148.9000000 | 1148.9000000 | 1148.9000000 | 0 |\n",
    "\n",
    "   \n",
    "The first two columns are the date and time for the minute, the next four columns are open, high, low, and close prices and the last value represents the amount of buy and sell orders performed during the bar. The exact interpretation of this number is stock and market-dependent, but usually, volumes give you an idea about how active the market was.\n",
    "The typical way to represent those prices is called a candlestick chart, where every bar is shown as a candle. Part of Yandex’s quotes for one day in February 2016 is shown in the following chart. Every file in the archive contains the M1 data for one year and it will be used in this chapter’s example:\n",
    "\n",
    "## Problem statements and key decisions \n",
    "\n",
    "In our example, we’ll just scratch the surface a bit with our RL tools and our problem will be formulated as simply as possible, using price as an observation. We will investigate whether it will be possible for our agent to learn when the best time is to buy one single share and then close the position to maximize the profit. The purpose of this example is to show how flexible the RL model can be and what the first steps are that you usually need to take to apply RL to a real-life use case.\n",
    "As you already know, to formulate RL problems three things are needed: \n",
    "* observation of the environment\n",
    "* possible actions\n",
    "* reward system\n",
    "In previous chapters, all three were already given to us and the internal machinery of the environment was hidden. Now we’re in a different situation, so **we need to decide ourselves what our agent will see and what set of actions it can take**. The reward system is also not given as a strict set of rules, rather it is guided by our feelings and knowledge of the domain, but we still have lots of flexibility here.\n",
    "\n",
    "Flexibility, in this case, is good and bad at the same time. It’s good that we have the freedom to pass some information to the agent that we feel will be important to learn efficiently. For example, you can pass to the trading agent not only prices but also the information about news or important statistics to be published (which is known to influence financial markets a lot). The bad part is that this flexibility usually means that to find a good agent, you need to try lots of variants of data representation and it’s not always obvious which will work better. In our case, we’ll implement the basic trading agent in its simplest form. The observation will include the following information:\n",
    "* $N$ past bars, where each have open, high, low, and close prices\n",
    "* An indication that the share was bought some time ago (it will be possible to have only one share at a time)\n",
    "* Profit or loss we currently have from our current position (the share bought)\n",
    "\n",
    "At every step, which will be after every minute’s bar, the agent can take one of the following actions:\n",
    "* **Do nothing** Skip the bar without taking actions\n",
    "* **Buy** a share: If the agent has already got the share, nothing will be bought, otherwise we’ll pay the commission, which is usually some small percentage of the current price\n",
    "* **Close the position**: If we’ve got no share previously bought, nothing will happen, otherwise we’ll pay the commission for the trade\n",
    "\n",
    "The reward that the agent receives could be expressed in various ways. On the one hand, we can split the reward into multiple steps during our ownership of the share. In that case, the reward on every step will be equal to the last bar’s movement. On the other hand, the agent can receive reward only after the close action and receive full reward at once. At the first sight, both variants should have the same final result, but maybe with different convergence speed. However, in practice, the difference could be dramatic. We’ll implement both variants to have a chance to compare them.\n",
    "One last decision to make is how to represent the prices in our environment observation. Ideally, we would like our agent to be independent on actual price values and take into account relative movement, such as “stock has grown 1% during the last bar” or “stock has lost 5%.\" This makes sense, as different stocks’ prices can vary, but they can have similar movement patterns. In finance, there exists a branch of analytics called “technical analysis,\" which studies such patterns to help to make predictions from them. We would like our system to be able to discover them (if they exist). To achieve this, we’ll convert every bar “open, high, low, and close” prices to three numbers showing high, low, and close prices represented as a percentage to the open price.\n",
    "This representation has its own drawbacks, as we’re potentially losing the information about key price levels. For example, it’s known that markets have a tendency to bounce from round price numbers (like 8000 per bitcoin) and levels which were turning points in the past. However, as already stated, we’re not implementing “Wall Street Killer” here, but playing with the data and checking the concept. The representation in the form of relative price movement will help the system to find repeating patterns in the price level (if they exist, of course), regardless of the absolute price position. Potentially, the neural network (NN) could learn this on its own (it’s just the mean price which needs to be subtracted from the absolute price values), but relative representation simplifies the NN’s task.\n",
    "\n",
    "## The trading environment \n",
    "As we have lots of code that is supposed to work with OpenAI Gym, we’ll implement the trading functionality following Gym’s Env class API, which should be familiar to you. Our environment is implemented in the StocksEnv class in the Chapter08/lib/environ.py module. It uses several internal classes to keep its state and encode observations. Let’s first look at the public API class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "import enum\n",
    "import numpy as np\n",
    "\n",
    "from lib import data #data is a pthon module that reads the CSV files\n",
    "\n",
    "DEFAULT_BARS_COUNT = 10\n",
    "DEFAULT_COMMISSION_PERC = 0.1\n",
    "\n",
    "#Enumerting the possible actions\n",
    "class Actions(enum.Enum): \n",
    "    Hold = 0 #Hold\n",
    "    Buy = 1  #Buy\n",
    "    Sell = 2 #Sell\n",
    "\n",
    "\n",
    "class State:\n",
    "    #_init_ method\n",
    "    def __init__(self, bars_count, commission_perc, reset_on_close, reward_on_close=True, volumes=True):\n",
    "        assert isinstance(bars_count, int) #Checking validity of the the input arguments\n",
    "        assert bars_count > 0\n",
    "        assert isinstance(commission_perc, float)\n",
    "        assert commission_perc >= 0.0\n",
    "        assert isinstance(reset_on_close, bool)\n",
    "        assert isinstance(reward_on_close, bool)\n",
    "        self.bars_count = bars_count #Count of bars in data\n",
    "        self.commission_perc = commission_perc #comission percentage\n",
    "        self.reset_on_close = reset_on_close #Reset on close - bollean flag\n",
    "        self.reward_on_close = reward_on_close #rewared on close- boolean flag\n",
    "        self.volumes = volumes #day's volume\n",
    "\n",
    "    #reset method\n",
    "    def reset(self, prices, offset): \n",
    "        assert isinstance(prices, data.Prices)\n",
    "        assert offset >= self.bars_count-1\n",
    "        self.have_position = False\n",
    "        self.open_price = 0.0\n",
    "        self._prices = prices\n",
    "        self._offset = offset #on every reset of the environment, the random offset in time series will be chosen. Otherwise, we’ll start from the beginning of the data.\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        # [h, l, c] * bars + position_flag + rel_profit (since open)\n",
    "        if self.volumes:\n",
    "            return (4 * self.bars_count + 1 + 1, )\n",
    "        else:\n",
    "            return (3*self.bars_count + 1 + 1, )\n",
    "\n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Convert current state into numpy array.\n",
    "        \"\"\"\n",
    "        res = np.ndarray(shape=self.shape, dtype=np.float32)\n",
    "        shift = 0\n",
    "        for bar_idx in range(-self.bars_count+1, 1):\n",
    "            res[shift] = self._prices.high[self._offset + bar_idx]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.low[self._offset + bar_idx]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.close[self._offset + bar_idx]\n",
    "            shift += 1\n",
    "            if self.volumes:\n",
    "                res[shift] = self._prices.volume[self._offset + bar_idx]\n",
    "                shift += 1\n",
    "        res[shift] = float(self.have_position)\n",
    "        shift += 1\n",
    "        if not self.have_position:\n",
    "            res[shift] = 0.0\n",
    "        else:\n",
    "            res[shift] = (self._cur_close() - self.open_price) / self.open_price\n",
    "        return res\n",
    "\n",
    "    def _cur_close(self):\n",
    "        \"\"\"\n",
    "        Calculate real close price for the current bar\n",
    "        \"\"\"\n",
    "        open = self._prices.open[self._offset] #on every reset of the environment, the random offset in time series will be chosen. Otherwise, we’ll start from the beginning of the data.\n",
    "        rel_close = self._prices.close[self._offset]\n",
    "        return open * (1.0 + rel_close)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one step in our price, adjust offset, check for the end of prices\n",
    "        and handle position change\n",
    "        :param action:\n",
    "        :return: reward, done\n",
    "        \"\"\"\n",
    "        assert isinstance(action, Actions)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        close = self._cur_close()\n",
    "        if action == Actions.Buy and not self.have_position:\n",
    "            self.have_position = True\n",
    "            self.open_price = close\n",
    "            reward -= self.commission_perc\n",
    "        elif action == Actions.Sell and self.have_position:\n",
    "            reward -= self.commission_perc #subtract sell position\n",
    "            done |= self.reset_on_close\n",
    "            if self.reward_on_close: #claculate reward on close (Sell)\n",
    "                reward += 100.0 * (close - self.open_price) / self.open_price\n",
    "            self.have_position = False\n",
    "            self.open_price = 0.0\n",
    "\n",
    "        self._offset += 1\n",
    "        prev_close = close\n",
    "        close = self._cur_close()\n",
    "        done |= self._offset >= self._prices.close.shape[0]-1\n",
    "        if self.have_position and not self.reward_on_close:\n",
    "            reward += 100.0 * (close - prev_close) / prev_close\n",
    "#         print('State.step: ', 'action=', action, 'prev.price', round(prev_close,1), ' open.price=', round(self.open_price,1),\n",
    "#               'curr.price=',  round(close,1), ' reward=', round(reward,1), ' offset=', self._offset ,  ' done=', done)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "\n",
    "class State1D(State):\n",
    "    \"\"\"\n",
    "    State with shape suitable for 1D convolution\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def shape(self):\n",
    "        if self.volumes:\n",
    "            return (6, self.bars_count)\n",
    "        else:\n",
    "            return (5, self.bars_count)\n",
    "\n",
    "    def encode(self):\n",
    "        res = np.zeros(shape=self.shape, dtype=np.float32)\n",
    "        ofs = self.bars_count-1\n",
    "        res[0] = self._prices.high[self._offset-ofs:self._offset+1]\n",
    "        res[1] = self._prices.low[self._offset-ofs:self._offset+1]\n",
    "        res[2] = self._prices.close[self._offset-ofs:self._offset+1]\n",
    "        if self.volumes:\n",
    "            res[3] = self._prices.volume[self._offset-ofs:self._offset+1]\n",
    "            dst = 4\n",
    "        else:\n",
    "            dst = 3\n",
    "        if self.have_position:\n",
    "            res[dst] = 1.0\n",
    "            res[dst+1] = (self._cur_close() - self.open_price) / self.open_price\n",
    "        return res\n",
    "\n",
    "\n",
    "class StocksEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, prices, bars_count=DEFAULT_BARS_COUNT,\n",
    "                 commission=DEFAULT_COMMISSION_PERC, reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False, volumes=False):\n",
    "        assert isinstance(prices, dict)\n",
    "        self._prices = prices\n",
    "        if state_1d:\n",
    "            self._state = State1D(bars_count, commission, reset_on_close, reward_on_close=reward_on_close,\n",
    "                                  volumes=volumes)\n",
    "        else:\n",
    "            self._state = State(bars_count, commission, reset_on_close, reward_on_close=reward_on_close,\n",
    "                                volumes=volumes)\n",
    "        self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self._state.shape, dtype=np.float32)\n",
    "        self.random_ofs_on_reset = random_ofs_on_reset\n",
    "        self.seed()\n",
    "\n",
    "    def reset(self):\n",
    "        # make selection of the instrument and it's offset. Then reset the state\n",
    "        self._instrument = self.np_random.choice(list(self._prices.keys()))\n",
    "        prices = self._prices[self._instrument]\n",
    "        bars = self._state.bars_count\n",
    "        if self.random_ofs_on_reset:\n",
    "            offset = self.np_random.choice(prices.high.shape[0]-bars*10) + bars #on every reset of the environment, the random offset in time series will be chosen. Otherwise, we’ll start from the beginning of the data.\n",
    "        else:\n",
    "            offset = bars # Otherwise, we’ll start from the beginning of the data.\n",
    "        self._state.reset(prices, offset)\n",
    "        return self._state.encode()\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        action = Actions(action_idx)\n",
    "        reward, done = self._state.step(action)\n",
    "        obs = self._state.encode()\n",
    "        info = {\"instrument\": self._instrument, \"offset\": self._state._offset}\n",
    "#        print('StockEnv: obs=', obs)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2 ** 31\n",
    "        return [seed1, seed2]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dir(cls, data_dir, **kwargs):\n",
    "        prices = {file: data.load_relative(file) for file in data.price_files(data_dir)}\n",
    "        return StocksEnv(prices, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "In this example, two architectures of DQN are used:\n",
    "* a simple feed-forward network with three layers \n",
    "* a network with 1D convolution and a feature extractor, followed by two fully connected layers to output Q values.\n",
    "\n",
    "Both of them use the dueling architecture described in the previous chapter. Double DQN and two-step Bellman unrolling have also been used.\n",
    "The rest of the process is the same as in the classical DQN (from Chapter 6, Deep Q-Networks).\n",
    "Both models are in Chapter08/lib/models.py and are very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))\n",
    "        self.register_buffer(\"epsilon_weight\", torch.zeros(out_features, in_features))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))\n",
    "            self.register_buffer(\"epsilon_bias\", torch.zeros(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.epsilon_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.epsilon_bias.normal_()\n",
    "            bias = bias + self.sigma_bias * self.epsilon_bias\n",
    "        return F.linear(input, self.weight + self.sigma_weight * self.epsilon_weight, bias)\n",
    "\n",
    "\n",
    "class SimpleFFDQN(nn.Module):\n",
    "    def __init__(self, obs_len, actions_n):\n",
    "        super(SimpleFFDQN, self).__init__()\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        val = self.fc_val(x)\n",
    "        adv = self.fc_adv(x)\n",
    "        return val + adv - adv.mean()\n",
    "\n",
    "\n",
    "class DQNConv1D(nn.Module):\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQNConv1D, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        val = self.fc_val(conv_out)\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        return val + adv - adv.mean()\n",
    "\n",
    "\n",
    "class DQNConv1DLarge(nn.Module):\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQNConv1DLarge, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        val = self.fc_val(conv_out)\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        return val + adv - adv.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code \n",
    "\n",
    "We have two very similar training modules in this example: one for the feed-forward model and one for 1D convolutions. For both of them, there is nothing new added to our examples from Chapter 7, DQN Extensions: \n",
    "* They’re using epsilon-greedy action selection to perform exploration. The epsilon linearly decays over the first 1M steps from 1.0 to 0.1.\n",
    "* A simple experience replay buffer of size 100k is being used, which is initially populated with 10k transitions.\n",
    "* For every 1000 steps, we calculate the mean value for the fixed set of states to check the dynamics of the Q-values during the training.\n",
    "* For every 100k steps, we perform validation: 100 episodes are played on the training data and on previously unseen quotes. Characteristics of orders are recorded in TensorBoard, such as the mean profit, the mean count of bars, and share held. This step allows us to check for overfitting conditions.\n",
    "\n",
    "\n",
    "To start the training, you need to pass training data with the --data option, which could be an individual CSV file of the whole directory with files. By default, the training module uses Yandex quotes for 2016 (file data/YNDX_160101_161231. csv). For the validation data, there is an option --valdata, which takes Yandex 2015 quotes by default. Another required option will be -r, which is used to pass the name of the run. This name will be used in the TensorBoard run name and to create directories with saved models.\n",
    "\n",
    "### Train Feed Forward Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/YNDX_160101_161231.csv\n",
      "Read done, got 131542 rows, 99752 filtered, 0 open prices adjusted\n",
      "Reading data/YNDX_150101_151231.csv\n",
      "Read done, got 130566 rows, 104412 filtered, 0 open prices adjusted\n",
      "SimpleFFDQN(\n",
      "  (fc_val): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc_adv): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "732: done 100 games, mean reward -0.115, mean steps 6.40, speed 961.42 f/s, eps 1.00\n",
      "1372: done 200 games, mean reward -0.141, mean steps 5.96, speed 930.43 f/s, eps 1.00\n",
      "2041: done 300 games, mean reward -0.152, mean steps 5.90, speed 818.35 f/s, eps 1.00\n",
      "2720: done 400 games, mean reward -0.171, mean steps 5.91, speed 679.52 f/s, eps 1.00\n",
      "3387: done 500 games, mean reward -0.190, mean steps 5.87, speed 912.59 f/s, eps 1.00\n",
      "4047: done 600 games, mean reward -0.198, mean steps 5.84, speed 970.95 f/s, eps 1.00\n",
      "4743: done 700 games, mean reward -0.193, mean steps 5.88, speed 1176.87 f/s, eps 1.00\n",
      "5449: done 800 games, mean reward -0.191, mean steps 5.91, speed 947.88 f/s, eps 0.99\n",
      "6153: done 900 games, mean reward -0.195, mean steps 5.93, speed 915.56 f/s, eps 0.99\n",
      "6896: done 1000 games, mean reward -0.192, mean steps 5.99, speed 1206.53 f/s, eps 0.99\n",
      "7639: done 1100 games, mean reward -0.196, mean steps 6.04, speed 833.75 f/s, eps 0.99\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-bae23d3d31ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mstep_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON_STOP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON_START\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEPSILON_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mstates_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstates_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mstates_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_agent_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ptan/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, states, agent_states)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mq_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-bf43a7bc92c3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0madv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib import common_stocks, validation\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BARS_COUNT = 10\n",
    "TARGET_NET_SYNC = 1000\n",
    "DEFAULT_STOCKS = \"data/YNDX_160101_161231.csv\"\n",
    "DEFAULT_VAL_STOCKS = \"data/YNDX_150101_151231.csv\"\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "\n",
    "REWARD_STEPS = 2\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_STEP = 1000\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_STOP = 0.1\n",
    "EPSILON_STEPS = 1000000\n",
    "\n",
    "CHECKPOINT_EVERY_STEP = 1000000\n",
    "VALIDATION_EVERY_STEP = 100000\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "#     parser.add_argument(\"--data\", default=DEFAULT_STOCKS, help=\"Stocks file or dir to train on, default=\" + DEFAULT_STOCKS)\n",
    "#     parser.add_argument(\"--year\", type=int, help=\"Year to be used for training, if specified, overrides --data option\")\n",
    "#     parser.add_argument(\"--valdata\", default=DEFAULT_VAL_STOCKS, help=\"Stocks data for validation, default=\" + DEFAULT_VAL_STOCKS)\n",
    "#     parser.add_argument(\"-r\", \"--run\", required=True, help=\"Run name\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#control arguments\n",
    "    args_cuda = False\n",
    "    args_data = DEFAULT_STOCKS\n",
    "    args_year = 2016\n",
    "    args_valdata = DEFAULT_VAL_STOCKS\n",
    "    args_run = 'run_1'\n",
    "    device = torch.device(\"cuda\" if args_cuda else \"cpu\")\n",
    "    \n",
    "    saves_path = os.path.join(\"saves\", args_run)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "\n",
    "    if args_year is not None or os.path.isfile(args_data):\n",
    "        if args_year is not None:\n",
    "            stock_data = data.load_year_data(args_year)\n",
    "        else:\n",
    "            stock_data = {\"YNDX\": data.load_relative(args_data)}\n",
    "        env = StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False, volumes=False)\n",
    "        env_tst = StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    elif os.path.isdir(args_data):\n",
    "        env = environ.StocksEnv.from_dir(args_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "        env_tst = environ.StocksEnv.from_dir(args_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    else:\n",
    "        raise RuntimeError(\"No data to train on\")\n",
    "#    env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "    val_data = {\"YNDX\": data.load_relative(args_valdata)}\n",
    "    env_val = StocksEnv(val_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "\n",
    "    writer = SummaryWriter(comment=\"-simple-\" + args_run)\n",
    "    net = SimpleFFDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    print(net)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(EPSILON_START)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, GAMMA, steps_count=REWARD_STEPS)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # main training loop\n",
    "    step_idx = 0\n",
    "    eval_states = None\n",
    "    best_mean_val = None\n",
    "\n",
    "    with common_stocks.RewardTracker(writer, np.inf, group_rewards=100) as reward_tracker:\n",
    "        while True:\n",
    "            step_idx += 1\n",
    "            buffer.populate(1)\n",
    "            selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)\n",
    "\n",
    "            new_rewards = exp_source.pop_rewards_steps()\n",
    "            if new_rewards:\n",
    "                reward_tracker.reward(new_rewards[0], step_idx, selector.epsilon)\n",
    "\n",
    "            if len(buffer) < REPLAY_INITIAL:\n",
    "                continue\n",
    "\n",
    "            if eval_states is None:\n",
    "                print(\"Initial buffer populated, start training\")\n",
    "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "                eval_states = np.array(eval_states, copy=False)\n",
    "\n",
    "            if step_idx % EVAL_EVERY_STEP == 0:\n",
    "                mean_val = common_stocks.calc_values_of_states(eval_states, net, device=device)\n",
    "                writer.add_scalar(\"values_mean\", mean_val, step_idx)\n",
    "                if best_mean_val is None or best_mean_val < mean_val:\n",
    "                    if best_mean_val is not None:\n",
    "                        print(\"%d: Best mean value updated %.3f -> %.3f\" % (step_idx, best_mean_val, mean_val))\n",
    "                    best_mean_val = mean_val\n",
    "                    torch.save(net.state_dict(), os.path.join(saves_path, \"mean_val-%.3f.data\" % mean_val))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(BATCH_SIZE)\n",
    "            loss_v = common_stocks.calc_loss(batch, net, tgt_net.target_model, GAMMA ** REWARD_STEPS, device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step_idx % TARGET_NET_SYNC == 0:\n",
    "                tgt_net.sync()\n",
    "\n",
    "            if step_idx % CHECKPOINT_EVERY_STEP == 0:\n",
    "                idx = step_idx // CHECKPOINT_EVERY_STEP\n",
    "                torch.save(net.state_dict(), os.path.join(saves_path, \"checkpoint-%3d.data\" % idx))\n",
    "\n",
    "            if step_idx % VALIDATION_EVERY_STEP == 0:\n",
    "                res = validation.validation_run(env_tst, net, device=device)\n",
    "                for key, val in res.items():\n",
    "                    writer.add_scalar(key + \"_test\", val, step_idx)\n",
    "                res = validation.validation_run(env_val, net, device=device)\n",
    "                for key, val in res.items():\n",
    "                    writer.add_scalar(key + \"_val\", val, step_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/YNDX_160101_161231.csv': Prices(open=array([ 1156.90002441,  1150.59997559,  1150.19995117, ...,  1245.5       ,\n",
       "         1246.        ,  1244.        ], dtype=float32), high=array([ 0.00086438,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.00361736], dtype=float32), low=array([-0.0033711 , -0.00017378, -0.00060855, ..., -0.00080289,\n",
       "        -0.00160514, -0.00040193], dtype=float32), close=array([-0.0033711 , -0.00017378, -0.00043471, ..., -0.00080289,\n",
       "        -0.00080257,  0.00361736], dtype=float32), volume=array([  43.,    5.,  165., ...,  200.,  231.,  191.], dtype=float32))}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(5, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
