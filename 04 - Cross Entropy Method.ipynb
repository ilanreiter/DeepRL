{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - The Cross Entropy Method\n",
    "\n",
    "\n",
    "## Taxonomy of RL methods \n",
    "\n",
    "The cross-entropy method falls into the model-free and policy-based category of methods. These notions are new, so let's spend some time exploring them. All methods in RL can be classified into various aspects:\n",
    "\n",
    "* Model-free or model-based \n",
    "* Value-based or policy-based \n",
    "* On-policy or off-policy\n",
    "\n",
    "There are other ways that you can taxonomize RL methods, but for now we're interested in the preceding three.\n",
    "Let's define them, as your problem specifics can influence your decision on a particular method.\n",
    "\n",
    "The term **model-free** means that the method **doesn't build a model** of the environment or reward; it just directly connects observations to actions (or values that are related to actions). In other words, the agent takes current observations and does some computations on them, and the result is the action that it should take. In contrast, **model-based** methods try to **predict what the next observation and/ or reward will be**. Based on this prediction, the agent is trying to choose the best possible action to take, very often making such predictions multiple times to **look more and more steps into the future**.\n",
    "\n",
    "Both classes of methods have strong and weak sides, but usually pure **model-based methods are used in deterministic environments**, such as board games with strict rules. On the other hand, model-free methods are usually easier to train as it's hard to build good models of complex environments with rich observations. \n",
    "\n",
    "All of the methods described in this book are from the **model-free** category, as those methods have been the most active area of research for the past few years. Only recently have researchers started to mix the benefits from both worlds (for example, refer to DeepMind's papers on imagination in agents. This approach will be described in Chapter 17, Beyond Model-Free – Imagination).\n",
    "\n",
    "By looking from an other angle, **policy-based methods** are directly approximating the policy of the agent, that is, what **actions the agent should carry out at every step**. Policy is usually represented by **probability distribution over the available actions**. \n",
    "\n",
    "In contrast, the method could be **value-based**. In this case, instead of the probability of actions, the agent calculates the **value of every possible action** and chooses the action with the best value. Both of those families of methods are equally popular and we'll discuss value-based methods in the next part of the book. Policy methods will be the topic of part three.\n",
    "\n",
    "The third important classification of methods is **on-policy** versus **off-policy**. We'll discuss this distinction more in parts two and three of the book, but for now, it will be enough to explain **off-policy** as the ability of the method to **learn on old historical data** (obtained by a previous version of the agent or recorded by human demonstration or just seen by the same agent several episodes ago).\n",
    "So, our **cross-entropy method is model-free, policy-based, and on-policy**, which means the following:\n",
    "\n",
    "* It **doesn't build any model** of the environment; it just says to the agent what to do at every step\n",
    "* It **approximates the policy of the agent**\n",
    "* It requires **fresh data** obtained from the environment\n",
    "\n",
    "## Practical cross-entropy \n",
    "\n",
    "The cross-entropy method description is split into two unequal parts: practical and theoretical.\n",
    "The practical part is intuitive in its nature, while the theoretical explanation of why cross-entropy works, and what's happening is more sophisticated.\n",
    "You may remember that the central, trickiest thing in RL is the agent, which is trying to accumulate as much total reward as possible by communicating with the environment. In practice, we follow a common ML approach and replace all of the complications of the agent with some kind of nonlinear trainable function, which maps the agent's input (observations from the environment) to some output. The details of the output that this function produces may depend on a particular method or a family of methods, as described in the previous section (such as value-based versus policy-based methods). As our cross-entropy method is policy-based, our **nonlinear function (neural network) produces policy**, which basically says for every observation which action the agent should take.\n",
    "![](img/fig4-1.png)\n",
    "\n",
    "\n",
    "In practice, **policy** is usually represented as **probability distribution over actions**, which makes it very similar to a classification problem, with the amount of classes being equal to amount of actions we can carry out. This abstraction makes our agent very simple: it needs to pass an observation from the environment to the network, get probability distribution over actions, and **perform random sampling using probability distribution to get an action to carry out**. This random sampling adds randomness to our agent, which is a good thing, as at the beginning of the training when our weights are random, the agent behaves randomly. After the agent gets an action to issue, it fires the action to the environment and obtains the next observation and reward for the last action. Then the loop continues.\n",
    "\n",
    "During the agent's lifetime, its experience is present as **episodes**. Every episode is a **sequence of observations** that the agent has got from the environment, actions it has issued, and rewards for these actions. Imagine that our agent has played several such episodes. For every episode, we can calculate the total reward that the agent has claimed. It can be **discounted or not discounted**, but for simplicity, let's assume a discount factor of gamma = 1, which means just a sum of all local rewards for every episode. This total reward shows how good this episode was for the agent. Let's illustrate this with a diagram, which contains four episodes (note that different episodes have different values for Oi\n",
    ", ai , and ri ):\n",
    "\n",
    "![](img/fig4-2.png)\n",
    "\n",
    "Every cell represents the agent's step in the episode. Due to randomness in the environment and the way that the agent selects actions to take, some episodes will be better than others. **The core of the cross-entropy method is to throw away bad episodes and train on better ones**. So, the steps of the method are as follows:\n",
    "\n",
    "1. Play **N number of episodes** using our current model and environment. \n",
    "2. Calculate the total **reward for every episode** and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
    "3. Throw away all episodes with a reward below the boundary.\n",
    "4. **Train on the remaining \"elite\" episodes** using observations as the input and issued actions as the desired output.\n",
    "5. Repeat from step 1 until we become satisfied with the result.\n",
    "\n",
    "So, that's all about the cross-entropy method description. With the preceding procedure, **our neural network learns how to repeat actions, which leads to a larger reward**, constantly moving the boundary higher and higher. Despite the simplicity of this method, it works well in simple environments, it's easy to implement, and it's quite robust to hyperparameters changing, which makes it an ideal baseline method to try. Let's now apply it to our CartPole environment.\n",
    "\n",
    "## Cross-entropy on CartPole\n",
    "The whole code for this example is in ``Chapter04/01_cartpole.py``, but the following are the most important parts. Our model's core is a one-hidden-layer neural network, with ReLU and 128 hidden neurons (which is absolutely arbitrary). Other hyperparameters are also set almost randomly and aren't tuned, as the method is robust and converges very quickly.\n",
    "\n",
    "### The  network\n",
    "* Input - a single observation from the environment as an input vector\n",
    "* Output- a number for every action we can perform - \n",
    "\n",
    "The output from the network is a probability distribution over actions, so a straightforward way to proceed would be to include softmax nonlinearity after the last layer. However, in the preceding network we don't apply softmax to increase the numerical stability of the training process. Rather than calculating softmax (which uses exponentiation) and then calculating cross-entropy loss (which uses logarithm of probabilities), we'll use the PyTorch class, ``nn.CrossEntropyLoss``, which **combines both softmax and cross-entropy** in a single, more numerically stable expression. CrossEntropyLoss requires raw, unnormalized values from the network (also called logits), and the downside of this is that we need to remember to apply softmax every time we need to get probabilities from our network's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym  #for environment simulation - CartPole\n",
    "from collections import namedtuple #for helper clases (namedtuple)\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter #to allow loging into tensor board\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "#Creating a Net class that inherits nn.Module class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "HIDDEN_SIZE = 128 #Hidden layer size\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70 #70th percentile - 30% of episodes sorte by reward \n",
    "\n",
    "#Creating helper clasess which are named tupled\n",
    "#Episode represent one single step that our agent made in the episode. We'll use episode steps from elite episodes as training data.\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps']) \n",
    "#EpisodeStep is a single episode stored as total undiscounted reward and a collection of EpisodeStep\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The batch loop\n",
    "One very important fact to understand in this function logic is that the training of our network and the generation of our episodes are performed at the same time. They are not completely in parallel, but every time our loop accumulates enough episodes (16), it passes control to this function caller, which is supposed to train the network using the gradient descent. So, when yield is returned, the network will have different, slightly better (we hope) behavior.\n",
    "We don't need to explore proper synchronization, as our training and data gathering activities are performed at the same thread of execution, but you need to understand those constant jumps from network training to its utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate batches with episodes\n",
    "#Our function is a generator, so every time the yield operator is executed,\n",
    "#the control is transferred to the outer iteration loop and then continues after the yield line.\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = [] #create empyty list for batches\n",
    "    episode_reward = 0.0 #initial reward\n",
    "    episode_steps = [] #create empyty list for episode steps\n",
    "    \n",
    "    obs = env.reset() #resting the environment - return a list of 4 observed numbers\n",
    "    sm = nn.Softmax(dim=1) #creating a softmax function \n",
    "    \n",
    "    \n",
    "    #Infinite loop\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) #create a float tensor from the new observation - 1x4 tensor\n",
    "        act_probs_v = sm(net(obs_v)) # execute the nn on the new observation and apply softmax on it\n",
    "        act_probs = act_probs_v.data.numpy()[0] #Obtain the action probablities by accessing the output tensor data filed and create a numpy array from it\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) #select the action (number btwn 0 - n) by sampling from the return probablities\n",
    "        next_obs, reward, is_done, _ = env.step(action) #making the next step in the enviroment with the selected action\n",
    "        episode_reward += reward #incrementing the episode reward with the reurned rewars\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action)) #append the the  episode step with the current obs and reward and not those returned as the next ones.\n",
    "        \n",
    "        #when the eppisodes ends - the episode ends when the stick has fallen down despite our efforts\n",
    "        if is_done: #if the episode is done (As returned from the environment)\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps)) #append the entire episode\n",
    "            episode_reward = 0.0 #reset the episode rewards\n",
    "            episode_steps = [] #create empyty list for episode steps\n",
    "            next_obs = env.reset() #reset the enviornmnet\n",
    "            if len(batch) == batch_size: \n",
    "                yield batch #return the currnt batch as a generator\n",
    "                batch = [] #reset the batch\n",
    "        obs = next_obs #assign next_obs to obs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter batch function\n",
    "This function is at the **core of the cross-entropy method**: from the given batch of episodes and percentile value, it calculates a boundary reward, which is used to filter elite episodes to train on. To obtain the boundary reward, we're using NumPy's percentile function, which from the list of values and the desired percentile, calculates the percentile's value. Then we will calculate mean reward, which is used only for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters the batches to yield only the top best eposidoe above a given percentile\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch)) #for each episode in the batch, extract the rewards\n",
    "    #print('rewards:', rewards)\n",
    "    reward_bound = np.percentile(rewards, percentile) #claulate prentiles for all rewards in the batch \n",
    "    reward_mean = float(np.mean(rewards)) #calcualte the rewards mean\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "\n",
    "    #Filter off our episodes. For every episode in the batch,\n",
    "    #we will check that the episode has a higher total reward than our boundary and if it has,\n",
    "    #we will populate lists of observations and actions that we will train on.\n",
    "    #This code will reject 70% of the episodes and \n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound: #non qualified episode\n",
    "            #print('rejected reward=',example.reward)\n",
    "            continue\n",
    "        #else:\n",
    "            #print('accepted reward=',example.reward)\n",
    "\n",
    "        #Accumilating steps only from qualifed episodes\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps)) #append the qualified eppisode's observations\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))  #append the qualifiedd eppisode's actions\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs) #create a tenso rof the observations\n",
    "    train_act_v = torch.LongTensor(train_act) #create a tensor of the actions\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code\n",
    "\n",
    "In the beginning, we will create all the required objects: the environment, our neural network, the objective function, the optimizer, and the summary writer for TensorBoard. The commented line creates a monitor to write videos of your agent's performance.\n",
    "\n",
    "In the training loop, we will iterate our batches (which are a list of Episode objects), then we perform filtering of the elite episodes using the filter_batch function. The result is variables of observations and taken actions, the reward boundary used for filtering and the mean reward. After that, we zero gradients of our network and pass observations to the network, obtaining its action scores. These scores are passed to the objective function, which calculates cross-entropy between the network output and the actions that the agent took. The idea of this is to reinforce our network to carry out those \"elite\" actions which have led to good rewards. Then, we will calculate gradients on the loss and ask the optimizer to adjust our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.691, reward_mean=28.9, reward_bound=38.0\n",
      "1: loss=0.686, reward_mean=24.9, reward_bound=26.5\n",
      "2: loss=0.670, reward_mean=27.1, reward_bound=30.5\n",
      "3: loss=0.655, reward_mean=28.4, reward_bound=29.0\n",
      "4: loss=0.652, reward_mean=46.1, reward_bound=50.0\n",
      "5: loss=0.642, reward_mean=39.5, reward_bound=50.5\n",
      "6: loss=0.632, reward_mean=44.8, reward_bound=49.0\n",
      "7: loss=0.627, reward_mean=58.1, reward_bound=65.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 128 #Hidden layer size\n",
    "BATCH_SIZE = 16 #episods per batch\n",
    "PERCENTILE = 70 #70th percentile - 30% of episodes sorte by reward \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\") #Initialize the env that is sourced from the gym package\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True) - \n",
    "    obs_size = env.observation_space.shape[0] #Get the observation size - 4 in the case of CartPole\n",
    "    n_actions = env.action_space.n  # 2 actions (left, right) in the case of CartPole\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions) #Create the nn from the Net class with input size = obs_size and output size = n_actions\n",
    "            # Net(\n",
    "            #   (net): Sequential(\n",
    "            #     (0): Linear(in_features=4, out_features=128, bias=True)\n",
    "            #     (1): ReLU()\n",
    "            #     (2): Linear(in_features=128, out_features=2, bias=True)\n",
    "            #   )\n",
    "            # )\n",
    "    objective = nn.CrossEntropyLoss() #populate the objective function to be CrossEntropy_Loss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01) #Set the Optimizer to be Adam\n",
    "    writer = SummaryWriter(comment=\"-cartpole\") #intialize the logs for the tensorboard\n",
    "\n",
    "    #This is the main loop for the nn training\n",
    "    #==========================================\n",
    "    #It loops of the generstor that is returned from the iterate_batches() function\n",
    "    #Each bacth includes BATCH_SIZE episodes\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)): #iterate_batch returns the iteration number and the list of episods oas named-tuples\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE) #Obtain the filter (top 30%) episodeds\n",
    "        #obs_v - tensor of n x 4 of all observation - n is the number of all accumlated observation of the qualified episodes\n",
    "        #acts_v - array of n actions (0 or 1)\n",
    "        #reward_b - reward threshold that represnet the 70% perecntile for the bacth\n",
    "        #reward_m - mean reward for the batch \n",
    "        \n",
    "        optimizer.zero_grad() #Reset the gradient for the optimizer\n",
    "        action_scores_v = net(obs_v) #Generate predited actions by applying the nn on all the batch's observations and getting the actions scores\n",
    "                                     #action_scores_v is a tensor of shape n 2 (2 actions)\n",
    "        loss_v = objective(action_scores_v, acts_v)  #calculating the loss by Cross Entropy between the current net output and the actual taken action\n",
    "        loss_v.backward() #run back propagation\n",
    "        optimizer.step() #run optimization \n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b)) #Print results \n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no) #Add the loss as scallar to the tensorboard log\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no) #Add  the reward_bound as scallar to the tensorboard log\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no) #Add  the reward_mean as scallar to the tensorboard log\n",
    "        if reward_m > 50: # > 199: #stop the loop if the mean reward has reached 199\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.storage_type>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_v.storage_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the tensorboad charts:\n",
    " ``$ cd C:\\Users\\ilanr_000\\OneDrive\\Python\\DeepRL\\DeepRL``\n",
    " ``$ tensorboard --logdir .\\runs --host localhost``\n",
    " Should see somthing like:\n",
    "![](img/Tensorboard-fig-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy on FrozenLake \n",
    "The next environment we'll try to solve using the cross-entropy method is FrozenLake. Its world is from the so-called \"grid world\" category, when your agent lives in a grid of size 4 × 4 and can move in four directions: up, down, left, and right. The agent always starts at a top-left position, and its goal is to reach the bottom-right cell of the grid. There are holes in the fixed cells of the grid and if you get into those holes, the episode ends and your reward is zero. If the agent reaches the destination cell, then it obtains the reward 1.0 and the episode ends.\n",
    "To make life more complicated, the world is slippery (it's a frozen lake after all), so the agent's actions do not always turn out as expected: there is a 33% chance that it will slip to the right or to the left. You want the agent to move left, for example, but there is a 33% probability that it will indeed move left, a 33% chance that it will end up in the cell above, and a 33% chance that it will end up in the cell below. As we'll see at the end of the section, this makes progress difficult.\n",
    "\n",
    "![](img/Fig5-1.png)\n",
    "\n",
    "### Gym Environment for FrozenLake\n",
    "\n",
    "Our observation space is discrete, which means that it's just a number from zero to 15 inclusive. Obviously, this number is our current position in the grid. The action space is also discrete, but can be from zero to three. Our network from the CartPole example expects a vector of numbers. To get this, we can apply the traditional \"one-hot encoding\" of discrete inputs, which means that input to our network will have 16 float numbers and zero everywhere, except the index that we'll encode. To minimize changes in our code, we can use the ObservationWrapper class from Gym and implement our DiscreteOneHotWrapper class.\n",
    "With that wrapper applied to the environment, both the observation space and action space are 100% compatible with our CartPole solution (source code Chapter04/02_ frozenlake_naive.py). So we can use the same code as above.\n",
    "However, by launching it, we can see that this doesn't improve the score over time.\n",
    "To understand what's going on, we need to look deeper at the reward structure of both environments. In CartPole, every step of the environment gives us the reward 1.0, until the moment that the pole falls. So, the longer our agent balanced the pole, the more reward it obtained. Due to randomness in our agent's behavior, different episodes were of different lengths, which gave us a pretty normal distribution of the episodes' rewards. After choosing a reward boundary, we rejected less successful episodes and learned how to repeat better ones (by training on successful episodes' data).\n",
    "This is shown in the following diagram:\n",
    "![](img/Fig7-1.png)\n",
    "In the FrozenLake environment, episodes and their reward look different. We get the reward of 1.0 only when we reach the goal, and this reward says nothing about how good each episode was. Was it quick and efficient or did we make four rounds on the lake before we randomly stepped into the final cell? We don't know, it's just 1.0 reward and that's it. The distribution of rewards for our episodes are also problematic. There are only two kinds of episodes possible, with zero reward (failed) and one reward (successful), and failed episodes will obviously dominate in the beginning of the training. So, our percentile selection of \"elite\" episodes is totally wrong and gives us bad examples to train on. This is the reason for our training failure.\n",
    "![](img/Fig8-1.png)\n",
    "This example shows us the limitations of the cross-entropy method: \n",
    "* For training, our **episodes** have to be **finite** and, preferably, **short**\n",
    "* The total reward for the **episodes** should have enough **variability** to **separate good episodes from bad ones**\n",
    "* There is no **intermediate indication** about whether the agent has succeeded or failed\n",
    "\n",
    "Later in the book, we'll become familiar with other methods, which address these limitations. For now, if you're curious about how FrozenLake can be solved using cross-entropy, here is a list of tweaks of the code that you need to make (the full example is in Chapter04/03_frozenlake_tweaked.py):\n",
    "\n",
    "* **Larger batches of played episodes:** In CartPole, it was enough to have 16 episodes on every iteration, but FrozenLake requires at least 100 just to get some successful episodes.\n",
    "* **Discount factor applied to reward:** To make the total reward for the episode depend on episode length, and add variety in episodes, we can use a discounted total reward with the discount factor 0.9 or 0.95. In this case, the reward for shorter episodes will be higher than the reward for longer ones.\n",
    "* **Keeping \"elite\" episodes for a longer time:** In the CartPole training, we sampled episodes from the environment, trained on the best ones, and threw them away. In FrozenLake, a successful episode is a much rarer animal, so we need to keep them for several iterations to train on them.\n",
    "* **Decrease learning rate:** This will give our network time to average more training samples.\n",
    "* **Much longer training time:** Due to the sparsity of successful episodes, and the random outcome of our actions, it's much harder for our network to get an idea of the best behavior to perform in any particular situation. To reach 50% successful episodes, about 5k training iterations are required.\n",
    "\n",
    "To incorporate all these into our code, we need to change the filter_batch function to calculate discounted reward and return \"elite\" episodes for us to keep:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "16\n",
      "0: loss=1.395, reward_mean=0.020, reward_bound=0.000, batch=2\n",
      "1: loss=1.391, reward_mean=0.000, reward_bound=0.000, batch=2\n",
      "2: loss=1.384, reward_mean=0.030, reward_bound=0.000, batch=5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-99eab860fa17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m#It loops of the generstor that is returned from the iterate_batches() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m#Each bacth includes BATCH_SIZE episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#iterate_batch returns the iteration number and the list of episods oas named-tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mreward_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#extratc the rewar and claulate  mean reward for all episodes the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mfull_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENTILE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Obtain the filter (top 30%) episodeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-99eab860fa17>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mobs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#create a float tensor from the new observation - 1x16 tensor with theobserved loction of the tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# execute the nn on the new observation and apply softmax on it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Obtain the action probablities by accessing the output tensor data filed and create a numpy array from it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#print('act_probs=', act_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-99eab860fa17>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstdv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import random\n",
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "e = gym.make(\"FrozenLake-v0\") #Creating a new Frozen Lake environment \n",
    "e.observation_space #Discrete(16) - one of the 16 possible poistions for the agent on a 4x4 grid\n",
    "e.action_space #Discrete(4) - 4 possible directions: 0 to 3  the agent can move\n",
    "\n",
    "e.reset() #reset the environment - output is 0 - intial position is top left corner of the grid\n",
    "e.render() #renders the grid:(s - start, F-Free cell, H - hole, G - Goal)\n",
    "# SFFF  \n",
    "# FHFH\n",
    "# FFFH\n",
    "# HFFG\n",
    "print(e.observation_space.n)\n",
    "\n",
    "#Creating an heriting class from Gym's ObservationWraper class\n",
    "#It converts the descrite inputs will have 16 float numbers, zero everywhere, except the currenl loction of the agent (as float 1)\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "#Creating a Net class that inherits nn.Module class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "#==================================================================================================================\n",
    "## Function to generate batches with episodes\n",
    "#Our function is a generator, so every time the yield operator is executed,\n",
    "#the control is transferred to the outer iteration loop and then continues after the yield line.\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = [] #create empyty list for batches\n",
    "    episode_reward = 0.0 #initial reward\n",
    "    episode_steps = [] #create empyty list for episode steps\n",
    "    \n",
    "    obs = env.reset() #resting the environment - return a list of 4 observed numbers\n",
    "    sm = nn.Softmax(dim=1) #creating a softmax function \n",
    "    \n",
    "    \n",
    "    #Infinite loop\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) #create a float tensor from the new observation - 1x16 tensor with theobserved loction of the tensor\n",
    "        act_probs_v = sm(net(obs_v)) # execute the nn on the new observation and apply softmax on it\n",
    "        act_probs = act_probs_v.data.numpy()[0] #Obtain the action probablities by accessing the output tensor data filed and create a numpy array from it\n",
    "        #print('act_probs=', act_probs)\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) #select the action (number btwn 0 - n) by sampling from the return probablities\n",
    "        #print('action=',action)\n",
    "        next_obs, reward, is_done, _ = env.step(action) #making the next step on the enviroment with the selected action\n",
    "        episode_reward += reward #incrementing the episode reward with the reurned rewars\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action)) #append the the  episode step with the current obs and reward and not those returned as the next ones.\n",
    "        \n",
    "        #when the eppisodes ends - the episode ends when the stick has fallen down despite our efforts\n",
    "        if is_done: #if the episode is done (As returned from the environment)\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps)) #append the entire episode results\n",
    "            episode_reward = 0.0 #reset the episode rewards\n",
    "            episode_steps = [] #create empyty list for episode steps\n",
    "            next_obs = env.reset() #reset the enviornmnet\n",
    "            if len(batch) == batch_size: #if max number of episodes have been executed\n",
    "                yield batch #return the currnt batch as a generator\n",
    "                batch = [] #reset the batch\n",
    "        obs = next_obs #assign next_obs to obs\n",
    "\n",
    "#==================================================================================================================\n",
    "\n",
    "#filters the batches to yield only the top best eposidoe above a given percentile\n",
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch)) #for each episode in the batch, extract the rewards - here we also add Gamma for the discount rate\n",
    "    #print('Disc rewards:', disc_rewards)\n",
    "    reward_bound = np.percentile(disc_rewards, percentile) #claulate prentiles for all rewards in the batch \n",
    "    #print('reward bound=',  reward_bound)\n",
    "    #reward_mean = float(np.mean(disc_rewards)) #calcualte the rewards mean\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "\n",
    "    #Filter off our episodes. For every episode in the batch,\n",
    "    #we will check that the episode has a higher total reward than our boundary and if it has,\n",
    "    #we will populate lists of observations and actions that we will train on.\n",
    "    #This code will reject 70% of the episodes and \n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        #print('discounted_reward=', discounted_reward,' reward bound=',  reward_bound)\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound\n",
    "\n",
    "HIDDEN_SIZE = 128 #Hidden layer size\n",
    "BATCH_SIZE = 100 #batch size - number of episodes in a single batch\n",
    "PERCENTILE = 5 #5th percentile - 30% of episodes sorte by reward \n",
    "GAMMA = 0.95 #Reward discount rate\n",
    "MAX_MEAN_REWARD = 0.8\n",
    "\n",
    "#Creating helper clasess which are named tupled\n",
    "#Episode represent one single step that our agent made in the episode. We'll use episode steps from elite episodes as training data.\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps']) \n",
    "#EpisodeStep is a single episode stored as total undiscounted reward and a collection of EpisodeStep\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\")) #Initialize the env that is sourced from the gym package with a wraper to conver inputs and outputs\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True) - \n",
    "    obs_size = env.observation_space.shape[0] #Get the observation size - 16 in the case of FrozenLake\n",
    "    n_actions = env.action_space.n  # 4 actions (left, right, up, down) in the case of FrozenLake\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions) #Create the nn from the Net class with input size = obs_size and output size = n_actions\n",
    "        # Net(\n",
    "        #   (net): Sequential(\n",
    "        #     (0): Linear(in_features=16, out_features=128, bias=True)\n",
    "        #     (1): ReLU()\n",
    "        #     (2): Linear(in_features=128, out_features=4, bias=True)\n",
    "        #   )\n",
    "        # )    \n",
    "\n",
    "    objective = nn.CrossEntropyLoss() #populate the objective function to be CrossEntropy_Loss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001) #Set the Optimizer to be Adam\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-tweaked\") #intialize the logs for the tensorboard\n",
    "    \n",
    "    full_batch = []\n",
    "\n",
    "    #This is the main loop for the nn training\n",
    "    #==========================================\n",
    "    #It loops of the generstor that is returned from the iterate_batches() function\n",
    "    #Each bacth includes BATCH_SIZE episodes\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)): #iterate_batch returns the iteration number and the list of episods oas named-tuples\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch)))) #extratc the rewar and claulate  mean reward for all episodes the batch \n",
    "        full_batch, train_obs, train_acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE) #Obtain the filter (top 30%) episodeds\n",
    "        #reward_b - reward threshold that represnet the PERCENTILE perecntile for the bacth\n",
    "        #print('filtered batch length=', len(acts),'length full batch=',len(full_batch))\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        \n",
    "        train_obs_v = torch.FloatTensor(train_obs) #tensor of n x 16 of all observation - n is the number of all accumlated observation of the qualified episodes\n",
    "        train_acts_v = torch.LongTensor(train_acts)  #acts_v - tensor of n actions (0 or 1,2,3)\n",
    "        full_batch = full_batch[-500:]\n",
    "        \n",
    "        ## NN Retraining\n",
    "        ##==============\n",
    "        optimizer.zero_grad() #Reset the gradient for the optimizer\n",
    "        #print('obs_v length=', len(obs_v),)\n",
    "        train_action_scores_v = net(train_obs_v) #Generate predicted actions by applying the nn on all the commulative batch's observations and getting the actions scores\n",
    "                                     #action_scores_v is a tensor of shape n x 2 (2 actions)\n",
    "        loss_v = objective(train_action_scores_v, train_acts_v) #calculating the loss by Cross Entropy between the current net output and the actual taken action\n",
    "        #print('action scores=',train_action_scores_v, 'tran acts=',train_acts_v)\n",
    "        loss_v.backward() #run back propagation\n",
    "        optimizer.step() #run optimization \n",
    "        \n",
    "        print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch))) #Print results \n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "#        if True: #reward_mean > 0.8: #stop the loop if the mean reward has reached 0.8\n",
    "        if reward_mean > MAX_MEAN_REWARD: #stop the loop if the mean reward has reached 0.8\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view Tensorboard:\n",
    "\n",
    "$ ``cd  cd .\\OneDrive\\Python\\DeepRL\\DeepRL\\``\n",
    "\n",
    "$ `` tensorboard --logdir .\\runs --host localhost``\n",
    "\n",
    "![](img/Tensorboard-fig-2.png)\n",
    "\n",
    "### Applying the trained model\n",
    "At this point after we trained neural net, we can play the game and measure our success:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful games ratio = 0.015\n"
     ]
    }
   ],
   "source": [
    "nGames = 1000 #number of games/episodes to play\n",
    "\n",
    "obs = env.reset() #resting the environment - return a list of 4 observed numbers\n",
    "sm = nn.Softmax(dim=1) #creating a softmax function \n",
    "success = 0 #success counter\n",
    "for game in (range(0,nGames)):\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) #create a float tensor from the new observation - 1x16 tensor with theobserved loction of the tensor\n",
    "        #print('obs=',obs)\n",
    "        act_probs_v = sm(net(obs_v)) # execute the nn on the new observation and apply softmax on it\n",
    "        act_probs = act_probs_v.data.numpy()[0] #Obtain the action probablities by accessing the output tensor data filed and create a numpy array from it\n",
    "        #print('act_probs=', act_probs)\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) #select the action (number btwn 0 - n) by sampling from the return probablities\n",
    "        #print('action=',action)\n",
    "        next_obs, reward, game_over, _ = env.step(action) #making the next step on the enviroment with the selected action\n",
    "        if game_over: \n",
    "            #print('Game ',game, 'over - reward=', reward)\n",
    "            if reward == 1: #succesful game \n",
    "                success += 1;\n",
    "            next_obs = env.reset() #resting the environment for a new game\n",
    "            break\n",
    "        obs = next_obs #assign next_obs to obs\n",
    "print('Successful games ratio =', success/nGames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
