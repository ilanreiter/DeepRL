{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI Gym\n",
    "\n",
    "This Cpater is covering the OpenAI Gym API.\n",
    "We implement randomly behaving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The anatomy of the agent\n",
    "Defenitions:\n",
    "* **Agent**: A person or a thing that takes an active role. In practice, it's some piece of code, \n",
    "    which implements some policy. Basically, this policy must decide what action is needed at every time step,\n",
    "    given our observations.\n",
    "* **Environment**: Some model of the world, which is external to the agent and has the responsibility of providing us\n",
    "    with observations and giving us rewards. It changes its state based on our actions.\n",
    "*  **Episodes**: the agent interactions with the environment is divided into a sequence of steps called episodes. Episodes can be finite, like in a game of chess, or infinite like the Voyager 2 mission.\n",
    "\n",
    "Implemented in Python for a simplistic situation.\n",
    "We will define an environment that gives the agent random rewards for a limited number of steps, regardless of the agent's actions. \n",
    "This scenario is not very useful, but will allow us to focus on specific methods in both the **environment** and the **agent** classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self): #Intitialise the Environment method to initialize its internal state\n",
    "        self.steps_left = 100 #In this case the state is just a counter that limiyts the number of steps the agent is allows to take to interact with the environment\n",
    "    \n",
    "    def get_observation(self): #Method to return the current environment's observation to the agent. It is usually implemented as some function of the internal state of the environment.  state:\n",
    "        return [0.0, 0.0, 0.0] #In our example, the observation vector is always zero, as the environment basically has no interna state\n",
    "\n",
    "    def get_actions(self): #Method to allow the agent to query the set of actions it can execute.\n",
    "        #Normally, the set of actions that the agent can execute does not change over time, but some actions can become impossible in different states (for example, not every move is possible in any position of the TicTacToe game).\n",
    "        return [0, 1] #In our case, there are only two actions that the agent can carry out, encoded with the integers 0 and 1:\n",
    "\n",
    "    def is_done(self): #Method to indicate the end of the episode.\n",
    "        return self.steps_left == 0 #TRue if steps_left eq 0\n",
    "\n",
    "    def action(self, action): #Handles/Responds to the Agent's action and check if the episode is completed\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -= 1 #Else return a random reward number and decrement the steps_left - in this case the enviromnet ignors the Agent's action\n",
    "        return random.random()      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this example, the agent ingnores the observations received from the environment\n",
    "#The agent selects actions randomly instead\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0 #?Intialize total rewards\n",
    "\n",
    "    def step(self, env): #Execute a step \n",
    "        current_obs = env.get_observation() #Requesrt observations from the environment\n",
    "        actions = env.get_actions() ##Requesrt possible actions from the environment\n",
    "        reward = env.action(random.choice(actions)) #Select random action and request the environment to execute it (through the action method)\n",
    "        self.total_reward += reward #Increment total reward by the last step returned reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main body of agent + environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 49.9068\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Environment() # Create and instance of the Environment class \n",
    "    agent = Agent()     # Create and instance of the Agent class \n",
    "\n",
    "    while not env.is_done(): #Loop of the steps\n",
    "        agent.step(env) #Agent executes the next step\n",
    "\n",
    "    print(\"Total reward got: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
