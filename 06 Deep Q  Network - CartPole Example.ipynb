{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "\n",
    "In this chapter, we'll try to apply the same theory to problems of much greater complexity: arcade games from the Atari 2600 platform, which are the de-facto benchmark of the RL research community. To deal with this new and more challenging goal, we'll talk about problems with the Value iteration method and introduce its variation, called Q-learning. In particular, we'll look at the application of Q-learning to so-called \"grid world\" environments, which is called tabular Q-learning, and then we'll discuss Q-learning in conjunction with neural networks. This combination has the name DQN. At the end of the chapter, we'll reimplement a DQN algorithm from the famous paper, Playing Atari with Deep Reinforcement Learning by V. Mnih and others, published in 2013, which started a new era in RL development.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning\n",
    "\n",
    "For enviroment with continues states, we can use **a nonlinear representation that maps both state and action onto a value**. In machine learning this is called a **\"regression problem\"**. The concrete way to represent and train such a representation can vary, but, as you may have already guessed from this section's title, using a deep neural network is one of the most popular options, especially when dealing with observations represented as screen images. With this in mind, let's make modifications to the Q-learning algorithm:\n",
    "We do a modifcation to the Tabluar Q-Learninng Algorithem above to accomudate very large state,action space and to use DNN:\n",
    "\n",
    "1. Initialize $Q(s, a)$ with some initial approximation \n",
    "2. By interacting with the environment, obtain the tuple $(s, a, r, s′)$ \n",
    "3. Calculate loss: $L =(Q_{s,a} − r)^2$ if episode has ended or $L =(Q_{s,a} − (r + \\gamma \\max_{a' \\in A} Q_{s',a'}))^2$ otherwise\n",
    "4. Update $Q(s, a)$ using the stochastic gradient descent (SGD) algorithm, by minimizing the loss with respect to the model parameters\n",
    "5. Repeat from step 2 until converged\n",
    "\n",
    "### Interaction with the environment\n",
    "\n",
    "First of all, we need to interact with the environment somehow to receive data to train on. In simple environments, such as FrozenLake, we can act randomly, but is this the best strategy to use? Imagine the game of Pong. What's the probability of winning a single point by randomly moving the paddle? It's not zero but it's extremely small, which just means that we'll need to wait for a very long time for such a rare situation. As an alternative, we can use our Q function approximation as a source of behavior (as we did before in the value iteration method, when we remembered our experience during testing).\n",
    "\n",
    "If our representation of Q is good, then the experience that we get from the environment will show the agent relevant data to train on. However, we're in trouble when our approximation is not perfect (at the beginning of the training, for example). In such a case, our agent can be stuck with bad actions for some states without ever trying to behave differently. This **exploration versus exploitation dilemma** was mentioned briefly in Chapter 1, What is Reinforcement Learning?. On the one hand, our agent needs to explore the environment to build a complete picture of transitions and action outcomes. On the other hand, we should use interaction with the environment efficiently: we shouldn't waste time by randomly trying actions we've already tried and have learned their outcomes. As you can see, random behavior is better at the beginning of the training when our Q approximation is bad, as it gives us more uniformly distributed information about the environment states. As our training progresses, random behavior becomes inefficient and we want to fall back to our Q approximation to decide how to act.\n",
    "\n",
    "A method which performs such a mix of two extreme behaviors is known as an **epsilon-greedy method**, which just means switching between random and $Q$ policy using the probability hyperparameter $\\epsilon$. By varying $\\epsilon$ we can select the ratio of random actions. The usual practice is to start with $\\epsilon = 1.0$ (100% random actions) and slowly decrease it to some small value such as 5% or 2% of random actions. Using an epsilon-greedy method helps both to explore the environment in the beginning and to stick to good policy at the end of the training. There are other solutions to the **\"exploration versus exploitation\"** problem, and we'll discuss some of them in part three of the book. This problem is **one of the fundamental open questions in RL and an active area of research, which is not even close to being resolved completely**.\n",
    "\n",
    "### SGD optimization\n",
    "The core of our Q-learning procedure is borrowed from the supervised learning. Indeed, we are trying to approximate a complex, nonlinear function $Q(s, a)$ with a neural network. To do this, we calculate targets for this function using the Bellman equation and then pretend that we have a supervised learning problem at hand. That's okay, but one of the fundamental requirements for SGD optimization is that the training data is **independent and identically distributed (i.i.d)**.\n",
    "\n",
    "In our case, data that we're going to use for the SGD update doesn't fulfill these criteria:\n",
    "1. Our samples are not independent. Even if we accumulate a large batch of data samples, they all will be very close to each other, as they belong to the same episode.\n",
    "2. Distribution of our training data won't be identical to samples provided by the optimal policy that we want to learn. Data that we have is a result of some other policy (our current policy, random, or both in the case of $\\epsilon$-greedy), but we don't want to learn how to play randomly: we want an optimal policy with the best reward.\n",
    "\n",
    "To deal with this nuisance, we usually need to use a **large buffer of our past experience and sample training data from it, instead of using our latest experience**. This method is called **replay buffer**. The simplest implementation is a buffer of **fixed size**, with new data added to the end of the buffer so that it pushes the oldest experience out of it. \n",
    "\n",
    "**Replay buffer** allows us to train on more-or-less independent data, but data will still be fresh enough to train on samples generated by our recent policy.\n",
    "\n",
    "### Correlation between steps\n",
    "Another practical issue with the default training procedure is also related to the lack of\n",
    "i.i.d in our data, but in a slightly different manner. The Bellman equation provides us with the value of $Q(s, a)$ via $Q(s′, a′)$ (which has the name of **bootstrapping**). However, both states $s$ and $s′$ have only one step between them. This makes them very similar and it's really hard for neural networks to distinguish between them. When we perform an update of our network's parameters, to make $Q(s, a)$ closer to the desired result, we indirectly can alter the value produced for $Q(s′, a′)$ and other states nearby. This can make our training really unstable, like chasing our own tail: when we update $Q$ for state $s$, then on subsequent states we discover that $Q(s′, a′)$ becomes worse, but attempts to update it can spoil our $Q(s, a)$ approximation, and so on.\n",
    "\n",
    "To make training more stable, there is a **trick, called target network**, when we keep a copy of our network and use it for the $Q(s′, a′)$ value in the Bellman equation. This network is synchronized with our main network only periodically, for example, once in $N$ steps (where $N$ is usually quite a large hyperparameter, such as 1k or 10k training iterations).\n",
    "\n",
    "### The Markov property\n",
    "Our RL methods use MDP formalism as their basis, which **assumes that the environment obeys the Markov property**: observation from the environment is all that we need to act optimally (in other words, our observations allow us to distinguish states from one another). As we've seen on the preceding Pong's screenshot, one single image from the Atari game is not enough to capture all important information (using only one image we have no idea about the speed and direction of objects, like the ball and our opponent's paddle). This obviously violates the Markov property and moves our single-frame Pong environment into the area of **partially observable MDPs (POMDP)**. A POMDP is basically MDP without the Markov property and they are very important in practice. For example, for most card games where you don't see your opponents' cards, game observations are POMDPs, because current observation (your cards and cards on the table) could correspond to different cards in your opponents' hands.\n",
    "We'll not discuss POMPDs in detail in this book, so, for now, we'll use a small technique to push our environment back into the MDP domain. **The solution is maintaining several observations from the past and using them as a state**. In the case of Atari games, we usually stack $k$ subsequent frames together and use them as the observation at every state. This **allows our agent to deduct the dynamics of the current state**, for instance, to get the speed of the ball and its direction. The usual \"classical\" **number of $k$ for Atari is four**. Of course, it's just a hack, as there can be longer dependencies in the environment, but for most of the games it works well.\n",
    "\n",
    "## The final form of DQN (Deep Q-Learning) training\n",
    "There are many more tips and tricks that researchers have discovered to make **DQN** training more stable and efficient, and we'll cover the best of them in the next chapter. However, **$\\epsilon$-greedy**, **replay buffer**, and **target network** form the basis that allows DeepMind to successfully train a DQN on a set of 49 Atari games and demonstrate the efficiency of this approach applied to complicated environments.\n",
    "\n",
    "The original paper (without target network) was published at the end of 2013 ([Playing Atari with Deep Reinforcement Learning 1312.5602v1, Mnih and others.)](https://arxiv.org/pdf/1312.5602.pdf), and they used seven games for testing. Later, at the beginning of 2015, a revised version of the article, with 49 different games, was published in Nature ([Human-Level Control Through Deep Reinforcement Learning doi:10.1038/nature14236, Mnih and others.](https://www.nature.com/articles/nature14236))\n",
    "\n",
    "The algorithm for DQN from the preceding papers has the following steps:\n",
    "1. Initialize parameters for $Q(s, a)$ and $\\hat Q (s, a)$ with random weights, **$\\epsilon\\leftarrow 1.0$**, and **empty replay buffer**\n",
    "2. With probability $\\epsilon$, select a random action $a$, otherwise $a = \\arg \\max_a Q_{s,a}$\n",
    "3. Execute action $a$ in an **emulator** and observe reward $r$ and the next state $s′$ \n",
    "4. Store transition $(s, a, r, s′)$ in the **replay buffer**\n",
    "5. Sample a random **minibatch** of transitions from the **replay buffer**\n",
    "6. For every transition in the buffer, calculate target $y = r$ if the episode has ended at this step or $y = r + \\gamma \\max_{a' \\in A} \\hat Q_{s',a'}$ otherwise\n",
    "7. Calculate loss: $L = (Q_{s,a} − y)^2$\n",
    "8. Update $Q(s, a)$ using the **SGD algorithm** by minimizing the loss in respect to model parameters\n",
    "9. Every $N$ steps copy weights from $Q$ to $\\hat Q_t$\n",
    "10. Repeat from step 2 until converged\n",
    "\n",
    "\n",
    "## DQN on FrozenLake\n",
    "### DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#Class for setting the Deep Q-Learning Network\n",
    "#For the FrozenLake game the w use as simple fully connected single hidden layer dnn\n",
    "class DQN(nn.Module):\n",
    "    #Method to intialize the DNN\n",
    "    def __init__(self, obs_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    " \n",
    "    #method to perform a single forward operation\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN  Training on FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "total_iter=93, delta_iteration=77, games=1, mean reward=0.233, eps=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:129: DeprecationWarning: `Tracer` is deprecated since version 5.1, directly use `IPython.core.debugger.Pdb.set_trace()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-5-691a678aa84d>\u001b[0m(130)\u001b[0;36mcalc_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    128 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    129 \u001b[0;31m    \u001b[0mTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 130 \u001b[0;31m    \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    131 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    132 \u001b[0;31m    \u001b[0;31m#Create tensors for states, next_states, actions, rewards and done flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> states\n",
      "*** NameError: name 'states' is not defined\n",
      "ipdb> device\n",
      "'cpu'\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-5-691a678aa84d>\u001b[0m(133)\u001b[0;36mcalc_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    131 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    132 \u001b[0;31m    \u001b[0;31m#Create tensors for states, next_states, actions, rewards and done flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 133 \u001b[0;31m    \u001b[0mstates_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    134 \u001b[0;31m    \u001b[0;31m#print('states_v.shape=',states_v.shape,' states_v=', states_v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    135 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-5-691a678aa84d>\u001b[0m(136)\u001b[0;36mcalc_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    134 \u001b[0;31m    \u001b[0;31m#print('states_v.shape=',states_v.shape,' states_v=', states_v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    135 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 136 \u001b[0;31m    \u001b[0mnext_states_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    137 \u001b[0;31m    \u001b[0;31m#print('next_states_v.shape=',next_states_v.shape,' next_states_v=', next_states_v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    138 \u001b[0;31m    \u001b[0mactions_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> states_v\n",
      "tensor([[ 0.0018, -0.2357,  0.0469,  0.3270],\n",
      "        [-0.0347, -0.0338,  0.0048,  0.0195],\n",
      "        [ 0.0179,  0.6216, -0.0733, -0.9484],\n",
      "        [ 0.0153,  0.6305, -0.1417, -1.0967],\n",
      "        [-0.0233, -0.4277,  0.0757,  0.6200],\n",
      "        [ 0.0669, -0.2017, -0.1543, -0.1469],\n",
      "        [ 0.0050, -0.4078,  0.0301,  0.6129],\n",
      "        [-0.0237,  0.0235,  0.0083,  0.0419],\n",
      "        [ 0.0394, -0.2223, -0.0296,  0.2878],\n",
      "        [-0.0127,  0.1981, -0.0490, -0.3983],\n",
      "        [ 0.0378, -0.1651, -0.0432,  0.2340],\n",
      "        [ 0.0515, -0.4153,  0.0146,  0.6475],\n",
      "        [-0.0028,  0.2285,  0.0004, -0.3379],\n",
      "        [ 0.0263,  0.2060, -0.0290, -0.2768],\n",
      "        [-0.1181, -0.1946,  0.0447,  0.1860],\n",
      "        [ 0.0776,  0.4266, -0.1554, -0.8651],\n",
      "        [ 0.1562,  0.5764, -0.2037, -1.2802],\n",
      "        [ 0.0018, -0.0203,  0.0230, -0.0040],\n",
      "        [-0.0870, -0.3689,  0.2003,  0.8777],\n",
      "        [ 0.1261,  0.4090, -0.1067, -0.4118],\n",
      "        [-0.0225, -0.0167,  0.0213,  0.0034],\n",
      "        [ 0.0207, -0.6048, -0.0068,  0.8307],\n",
      "        [ 0.0498,  0.7915,  0.0171, -1.0157],\n",
      "        [ 0.0654,  0.8284, -0.1972, -1.5416],\n",
      "        [-0.0096,  0.3659, -0.0293, -0.5838],\n",
      "        [-0.0504, -0.4027, -0.0188,  0.3663],\n",
      "        [ 0.0545,  0.5633, -0.0627, -0.9653],\n",
      "        [ 0.1001,  0.5831, -0.0487, -0.8515],\n",
      "        [-0.0237,  0.2314, -0.0452, -0.3219],\n",
      "        [ 0.0417,  0.3636,  0.0098, -0.5735],\n",
      "        [-0.0496, -0.0289,  0.0390,  0.0370],\n",
      "        [ 0.0619,  0.3771, -0.0829, -0.6498]])\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-5-691a678aa84d>\u001b[0m(138)\u001b[0;36mcalc_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    136 \u001b[0;31m    \u001b[0mnext_states_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    137 \u001b[0;31m    \u001b[0;31m#print('next_states_v.shape=',next_states_v.shape,' next_states_v=', next_states_v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 138 \u001b[0;31m    \u001b[0mactions_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    139 \u001b[0;31m    \u001b[0mrewards_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    140 \u001b[0;31m    \u001b[0mdone_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> next_states_v\n",
      "tensor([[-0.0029, -0.4315,  0.0535,  0.6341],\n",
      "        [-0.0354,  0.1613,  0.0052, -0.2717],\n",
      "        [ 0.0304,  0.8177, -0.0923, -1.2632],\n",
      "        [ 0.0279,  0.8272, -0.1636, -1.4303],\n",
      "        [-0.0318, -0.6238,  0.0881,  0.9356],\n",
      "        [ 0.0628, -0.3943, -0.1573,  0.0934],\n",
      "        [-0.0032, -0.2131,  0.0423,  0.3298],\n",
      "        [-0.0232,  0.2185,  0.0092, -0.2481],\n",
      "        [ 0.0349, -0.4169, -0.0238,  0.5710],\n",
      "        [-0.0087,  0.0037, -0.0570, -0.1215],\n",
      "        [ 0.0345,  0.0307, -0.0385, -0.0720],\n",
      "        [ 0.0432, -0.2204,  0.0275,  0.3595],\n",
      "        [ 0.0018,  0.4236, -0.0064, -0.6304],\n",
      "        [ 0.0305,  0.0113, -0.0345,  0.0066],\n",
      "        [-0.1220, -0.3904,  0.0484,  0.4925],\n",
      "        [ 0.0861,  0.2339, -0.1727, -0.6251],\n",
      "        [ 0.1678,  0.7735, -0.2293, -1.6292],\n",
      "        [ 0.0014,  0.1745,  0.0229, -0.2894],\n",
      "        [-0.0943, -0.5661,  0.2179,  1.2261],\n",
      "        [ 0.1343,  0.6055, -0.1150, -0.7361],\n",
      "        [-0.0228, -0.2121,  0.0214,  0.3027],\n",
      "        [ 0.0086, -0.7998,  0.0098,  1.1213],\n",
      "        [ 0.0657,  0.5962, -0.0033, -0.7177],\n",
      "        [ 0.0819,  0.6361, -0.2280, -1.3163],\n",
      "        [-0.0023,  0.1712, -0.0410, -0.3005],\n",
      "        [-0.0584, -0.5975, -0.0115,  0.6529],\n",
      "        [ 0.0658,  0.3690, -0.0820, -0.6929],\n",
      "        [ 0.1117,  0.3887, -0.0657, -0.5745],\n",
      "        [-0.0190,  0.4271, -0.0517, -0.6285],\n",
      "        [ 0.0490,  0.5586, -0.0017, -0.8631],\n",
      "        [-0.0502, -0.2246,  0.0397,  0.3417],\n",
      "        [ 0.0695,  0.5733, -0.0959, -0.9674]])\n",
      "ipdb> c\n",
      "> \u001b[0;32m<ipython-input-5-691a678aa84d>\u001b[0m(130)\u001b[0;36mcalc_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    128 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    129 \u001b[0;31m    \u001b[0mTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 130 \u001b[0;31m    \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    131 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    132 \u001b[0;31m    \u001b[0;31m#Create tensors for states, next_states, actions, rewards and done flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n",
      "Exiting Debugger.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "#DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "DEFAULT_ENV_NAME = \"CartPole-v0\"\n",
    "#DEFAULT_ENV_NAME = \"FrozenLake8x8-v0\"\n",
    "\n",
    "LEARNING_RATE = 1e-3 #1e-4\n",
    "\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99 #0.99 #reward discount\n",
    "BATCH_SIZE = 32 #32\n",
    "REPLAY_SIZE = 10000 #10000\n",
    "REPLAY_START_SIZE = 10000 #10000\n",
    "SYNC_TARGET_FRAMES = 1000 #1000\n",
    "\n",
    "HIDDEN_SIZE = 128 #Hidden layer size\n",
    "\n",
    "#Epsilon control parameters - to control explore/exploite decision\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5 #10**5\n",
    "EPSILON_START = .99\n",
    "EPSILON_FINAL = 0.05\n",
    "\n",
    "\n",
    "#Create a name tuppled object\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "#A class for replay buffer\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self\n",
    "\n",
    "    def sample(self, batch_size): #Method to append results from a mini batch of a given batch_size\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "#Ineriting class from Gym's ObservationWraper class\n",
    "#It converts the descrite inputs will have 16 float numbers, zero everywhere, except the currenl loction of the agent (as float 1)\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "#         assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "#         self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.array(observation)\n",
    "        return res\n",
    "\n",
    "\n",
    "#The Agent class\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer): #method to intialize the Agent\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer #Init the experience buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self): #method to reset the agent\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def best_action(self, net):\n",
    "        #state_a = np.array([self.state], copy=False) #Convert the state to np array\n",
    "        #state_v = torch.tensor(state_a).to(device) \n",
    "        state_v = torch.FloatTensor([self.state]).to(device) \n",
    "        \n",
    "        #print(state_v)\n",
    "        #get the best action for the current state\n",
    "        q_vals_v = net(state_v) # sm(net(state_v))\n",
    "        _, act_v = torch.max(q_vals_v, dim=1)\n",
    "        action = int(act_v.item())\n",
    "        return action\n",
    "\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device = 'cpu'): #A method to play a single step\n",
    "        done_reward = None\n",
    "        #make an explore/exploit decision\n",
    "        if np.random.random() < epsilon: #Explor: sampling from a unfied and check if result is below epsilon\n",
    "            action = self.env.action_space.sample() # sample a random action from the environment \n",
    "        else: #Exploit: pick an action by scoring the nn\n",
    "            action = self.best_action(net)\n",
    "        # Make a step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action) #perfrom a step on the agent's environment with the selected action\n",
    "        self.total_reward += reward #increment the total reward with the new reward received\n",
    "        #print('reward=', reward)\n",
    "        #new_state = new_state #Seems redundant...\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state) #Create a new experiance entry\n",
    "        self.exp_buffer.append(exp) #append the experiance entry\n",
    "        self.state = new_state \n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "    \n",
    "    def play_episode(self, net):\n",
    "        self._reset() #reset the environment\n",
    "        steps = 0\n",
    "        reward = None\n",
    "        while reward is None:\n",
    "            reward = self.play_step(net, epsilon=0.0)#mae a step in the env with the best action\n",
    "            steps += 1\n",
    "        return reward, steps\n",
    "\n",
    "\n",
    "#Calculate loos on a given mini-batch for net and tgt_net\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    Tracer()()\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    \n",
    "    #Create tensors for states, next_states, actions, rewards and done flags\n",
    "    states_v = torch.FloatTensor(states) .to(device)\n",
    "    #print('states_v.shape=',states_v.shape,' states_v=', states_v)\n",
    "    \n",
    "    next_states_v = torch.FloatTensor(next_states).to(device)\n",
    "    #print('next_states_v.shape=',next_states_v.shape,' next_states_v=', next_states_v)\n",
    "    actions_v = torch.tensor(actions) .to(device)\n",
    "    rewards_v = torch.tensor(rewards) .to(device)\n",
    "    done_mask = torch.ByteTensor(dones) .to(device)\n",
    "\n",
    "    #state_action_values = sm(net(states_v)).gather(1, actions_v.unsqueeze(-1)).squeeze(-1) # Applying the net on sates as input and then extract the Q-values based on the action taken\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1) # Applying the net on sates as input and then extract the Q-values based on the action taken\n",
    "    #print('net(states_v)', net(states_v), 'actions_v=', actions_v, 'actions_v.unsqueeze(-1)=', actions_v.unsqueeze(-1),\n",
    "    #      'net(states_v).gather(1, actions_v.unsqueeze(-1))=', net(states_v).gather(1, actions_v.unsqueeze(-1)),\n",
    "    #      'net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)=', net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1))\n",
    "    \n",
    "    #next_state_values = sm(tgt_net(next_states_v)).max(1)[0] #apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1. Function max() returns both maximum values and indices of those values (so it calculates both max and argmax), which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result.\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0] #apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1. Function max() returns both maximum values and indices of those values (so it calculates both max and argmax), which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result.\n",
    "    next_state_values[done_mask] = 0.0 #Zero next state value in case current state is the last 1\n",
    "    next_state_values = next_state_values.detach() #detach the value from its computation graph to prevent gradients from flowing into the neural network\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v #Calculating the expected state action values based on Bellman equation \n",
    "    #print('next_state_values:', next_state_values, 'expected_state_action_values:', expected_state_action_values)\n",
    "    \n",
    "    \n",
    "    loss = nn.MSELoss()(state_action_values, expected_state_action_values) #calculate the loss and return it\n",
    "    #print('loss:', loss)\n",
    "    #print('====================================================================')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#  Setup for GPU\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "#     parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
    "#                         help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "#     parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
    "#                         help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
    "#     args = parser.parse_args()\n",
    "#     device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#     env = wrappers.make_env(args.env)\n",
    "    device = 'cpu'\n",
    "\n",
    "#    env = wrappers.make_env(args.env)\n",
    "\n",
    "#    env = DiscreteOneHotWrapper(gym.make(DEFAULT_ENV_NAME)) #creating a new environment \n",
    "    env = DiscreteOneHotWrapper(gym.make(DEFAULT_ENV_NAME)) #creating a new environment \n",
    "    \n",
    "#    env = gym.make(\"CartPole-v0\") #using CartPole as environment\n",
    "    \n",
    "    obs_size = env.observation_space.shape[0] #4 in the case of CartPole\n",
    "    n_actions = env.action_space.n  # 2 actions (left, right) in the case of CartPole\n",
    "\n",
    "    net = DQN(obs_size, HIDDEN_SIZE, n_actions) .to(device)\n",
    "    tgt_net = DQN(obs_size, HIDDEN_SIZE, n_actions).to(device)\n",
    "\n",
    "    writer = SummaryWriter(comment=\"-\") #(comment=\"-\" + args.env)\n",
    "    print(net)\n",
    "\n",
    "    exp_buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env,exp_buffer) #Creating a new instance of an agent\n",
    "    epsilon = EPSILON_START #Init the greed factor - higer value means more explor and less exploit\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    iter_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None\n",
    "    \n",
    "    #sm = nn.Softmax(dim=1) #creating a softmax function \n",
    "    prev_idx = 0\n",
    "    prev_games = 0\n",
    "\n",
    "    while True:\n",
    "        iter_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - iter_idx / EPSILON_DECAY_LAST_FRAME) #Update epslion (greed factor) by reducing it baed on the iter  \n",
    "\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (iter_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = iter_idx\n",
    "            ts = time.time()\n",
    "            mean_reward = np.mean(total_rewards[-100:])/200\n",
    "            writer.add_scalar(\"epsilon\", epsilon, iter_idx)\n",
    "            writer.add_scalar(\"speed\", speed, iter_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, iter_idx)\n",
    "            writer.add_scalar(\"reward\", reward, iter_idx)\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                #torch.save(net.state_dict(), \"-best.dat\") #Save (serialize the model parameters) - not sure why needed\n",
    "                #print('net state_dict saved >>>>>>')\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"total_iter=%d, delta_iteration=%d, games=%d, mean reward=%.3f, eps=%.2f\" \n",
    "                          % (iter_idx, iter_idx - prev_idx, len(total_rewards) - prev_games, mean_reward, epsilon))\n",
    "                best_mean_reward = mean_reward\n",
    "                prev_idx = iter_idx\n",
    "                prev_games = len(total_rewards)\n",
    "\n",
    "            if mean_reward >= 1: # args.reward:\n",
    "                #print(\"Solved in %d frames!\" % iter_idx)\n",
    "                break\n",
    "\n",
    "        if len(exp_buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        if iter_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict()) #sync the net model parameters to the target net\n",
    "            #print('total_iter=', iter_idx, 'mean_reward=', mean_reward, ' best_mean_reward=', best_mean_reward)\n",
    "            #print('net state_dict loaded <<<<<')\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = exp_buffer.sample(BATCH_SIZE) #sample from the experiance buffer \n",
    "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "        #print('loss=',loss_t.abs())\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47.0, 15.0, 89.0, 35.0, 76.0, 118.0, 38.0, 25.0, 26.0, 19.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward= 54.51  Av. steps per episode =  54.51\n"
     ]
    }
   ],
   "source": [
    "TEST_EPISODES = 100\n",
    "#env = DiscreteOneHotWrapper(gym.make(DEFAULT_ENV_NAME)) #creating a new environment\n",
    "total_steps = 0\n",
    "total_rewards = 0\n",
    "for i in range(TEST_EPISODES):\n",
    "    reward, steps = agent.play_episode(net)\n",
    "    #print('Episode: ',i,' Reward=', reward, ' Steps=', steps)\n",
    "    total_steps += steps\n",
    "    total_rewards += reward\n",
    "print('Average reward=', total_rewards/TEST_EPISODES, ' Av. steps per episode = ', total_steps/TEST_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart shows the change in Epsilon and mean rewards through the iterations.\n",
    "![](img/fig6-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
